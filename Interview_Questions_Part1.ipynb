{"cells":[{"cell_type":"code","source":["print(\"krishnareddy\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"915c7a9b-37d8-4adf-b50e-0b24a79faad2"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"krishnareddy\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["krishnareddy\n"]}}],"execution_count":0},{"cell_type":"code","source":["Read CSV file from file store"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"ca7a2f42-f73f-47ef-b00c-6887cc201ca4"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["%fs\nhead dbfs:/FileStore/shared_uploads/krishnareddyesuru@gmail.com/delimit_input-4.txt"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"7c311ace-5431-4a3a-9924-18490072f1de"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">Azar|BE|8|BigData|9273541255|Ramesh|Btech|3|pyspark|912354885|Krishna|MBA|2|sql|8106186796\n</div>","removedWidgets":[],"addedWidgets":{},"metadata":{"isDbfsCommandResult":false},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Azar|BE|8|BigData|9273541255|Ramesh|Btech|3|pyspark|912354885|Krishna|MBA|2|sql|8106186796\n</div>"]}}],"execution_count":0},{"cell_type":"code","source":["df = spark.read.csv(\"dbfs:/FileStore/shared_uploads/krishnareddyesuru@gmail.com/delimit_input-4.txt\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"6545ab09-2764-4480-a92c-f30b2be2e21e"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["df.show(truncate = False)\ndf.display()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"532dbe4e-7512-4454-9f28-d984e18b9cf7"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"+------------------------------------------------------------------------------------------+\n|_c0                                                                                       |\n+------------------------------------------------------------------------------------------+\n|Azar|BE|8|BigData|9273541255|Ramesh|Btech|3|pyspark|912354885|Krishna|MBA|2|sql|8106186796|\n+------------------------------------------------------------------------------------------+\n\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["+------------------------------------------------------------------------------------------+\n|_c0                                                                                       |\n+------------------------------------------------------------------------------------------+\n|Azar|BE|8|BigData|9273541255|Ramesh|Btech|3|pyspark|912354885|Krishna|MBA|2|sql|8106186796|\n+------------------------------------------------------------------------------------------+\n\n"]}},{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"overflow":false,"datasetInfos":[],"data":[["Azar|BE|8|BigData|9273541255|Ramesh|Btech|3|pyspark|912354885|Krishna|MBA|2|sql|8106186796"]],"plotOptions":{"displayType":"table","customPlotOptions":{},"pivotColumns":null,"pivotAggregation":null,"xColumns":null,"yColumns":null},"columnCustomDisplayInfos":{},"aggType":"","isJsonSchema":true,"removedWidgets":[],"aggSchema":[],"schema":[{"name":"_c0","type":"\"string\"","metadata":"{}"}],"aggError":"","aggData":[],"addedWidgets":{},"metadata":{},"dbfsResultPath":null,"type":"table","aggOverflow":false,"aggSeriesLimitReached":false,"arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>_c0</th></tr></thead><tbody><tr><td>Azar|BE|8|BigData|9273541255|Ramesh|Btech|3|pyspark|912354885|Krishna|MBA|2|sql|8106186796</td></tr></tbody></table></div>"]}}],"execution_count":0},{"cell_type":"code","source":["#Apply regular expression Replace pipesymbol of every 5th occurance\n#PySpark function explode(e: Column) is used to explode or create array or map columns to rows.\n#When an array is passed to this function, it creates a new default column “col1” and it contains all array elements. When a map is passed, it creates two new columns one for key and one for value and each element in map split into the rows.\nimport pyspark\nfrom pyspark.sql.functions import regexp_replace,split,explode\ndf1  = df.withColumn(\"check\",regexp_replace(\"_c0\",\"(.*?\\\\|){5}\",\"$0-\"))\ndf1.display()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"30e6956c-100c-47c7-b468-5094bb0b7fbb"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"overflow":false,"datasetInfos":[],"data":[["Azar|BE|8|BigData|9273541255|Ramesh|Btech|3|pyspark|912354885|Krishna|MBA|2|sql|8106186796","Azar|BE|8|BigData|9273541255|-Ramesh|Btech|3|pyspark|912354885|-Krishna|MBA|2|sql|8106186796"]],"plotOptions":{"displayType":"table","customPlotOptions":{},"pivotColumns":null,"pivotAggregation":null,"xColumns":null,"yColumns":null},"columnCustomDisplayInfos":{},"aggType":"","isJsonSchema":true,"removedWidgets":[],"aggSchema":[],"schema":[{"name":"_c0","type":"\"string\"","metadata":"{}"},{"name":"check","type":"\"string\"","metadata":"{}"}],"aggError":"","aggData":[],"addedWidgets":{},"metadata":{},"dbfsResultPath":null,"type":"table","aggOverflow":false,"aggSeriesLimitReached":false,"arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>_c0</th><th>check</th></tr></thead><tbody><tr><td>Azar|BE|8|BigData|9273541255|Ramesh|Btech|3|pyspark|912354885|Krishna|MBA|2|sql|8106186796</td><td>Azar|BE|8|BigData|9273541255|-Ramesh|Btech|3|pyspark|912354885|-Krishna|MBA|2|sql|8106186796</td></tr></tbody></table></div>"]}}],"execution_count":0},{"cell_type":"code","source":["df2=df1.withColumn(\"explode_colum\",explode(split(\"check\",\"\\|-\"))).select(\"explode_colum\")\n\ndf2.display()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"a5d6680a-f1b2-4115-b6e2-725f5e6c2560"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"overflow":false,"datasetInfos":[],"data":[["Azar|BE|8|BigData|9273541255"],["Ramesh|Btech|3|pyspark|912354885"],["Krishna|MBA|2|sql|8106186796"]],"plotOptions":{"displayType":"table","customPlotOptions":{},"pivotColumns":null,"pivotAggregation":null,"xColumns":null,"yColumns":null},"columnCustomDisplayInfos":{},"aggType":"","isJsonSchema":true,"removedWidgets":[],"aggSchema":[],"schema":[{"name":"explode_colum","type":"\"string\"","metadata":"{}"}],"aggError":"","aggData":[],"addedWidgets":{},"metadata":{},"dbfsResultPath":null,"type":"table","aggOverflow":false,"aggSeriesLimitReached":false,"arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>explode_colum</th></tr></thead><tbody><tr><td>Azar|BE|8|BigData|9273541255</td></tr><tr><td>Ramesh|Btech|3|pyspark|912354885</td></tr><tr><td>Krishna|MBA|2|sql|8106186796</td></tr></tbody></table></div>"]}}],"execution_count":0},{"cell_type":"code","source":["df2.select(split(\"explode_colum\",\"\\|\")[3]).display()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"d736dcd6-31ba-41da-954d-8c603dd61475"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"overflow":false,"datasetInfos":[],"data":[["BigData"],["pyspark"],["sql"]],"plotOptions":{"displayType":"table","customPlotOptions":{},"pivotColumns":null,"pivotAggregation":null,"xColumns":null,"yColumns":null},"columnCustomDisplayInfos":{},"aggType":"","isJsonSchema":true,"removedWidgets":[],"aggSchema":[],"schema":[{"name":"split(explode_colum, \\|, -1)[3]","type":"\"string\"","metadata":"{\"__autoGeneratedAlias\":\"true\"}"}],"aggError":"","aggData":[],"addedWidgets":{},"metadata":{},"dbfsResultPath":null,"type":"table","aggOverflow":false,"aggSeriesLimitReached":false,"arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>split(explode_colum, \\|, -1)[3]</th></tr></thead><tbody><tr><td>BigData</td></tr><tr><td>pyspark</td></tr><tr><td>sql</td></tr></tbody></table></div>"]}}],"execution_count":0},{"cell_type":"code","source":["#Convert DF to RDD and split by\nfrom pyspark.sql import SparkSession\nspark = SparkSession.builder.appName(\"convert DF to RDD\").getOrCreate()\nrddf2 = spark.sparkContext.parallelize([])\ndf2.select(\"explode_colum\").rdd.map(lambda x:x[0].split(\"|\")).collect()\nrddf2 = df2.select(\"explode_colum\").rdd.map(lambda x:x[0].split(\"|\"))\ndf3=rddf2.toDF([\"name\",\"edu\",\"yearofexp\",\"tech\",\"mobile\"])\ndf3.display()\n\n"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"ac0d0388-bc53-444b-a113-b2f9abbad9cf"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"overflow":false,"datasetInfos":[],"data":[["Azar","BE","8","BigData","9273541255"],["Ramesh","Btech","3","pyspark","912354885"],["Krishna","MBA","2","sql","8106186796"]],"plotOptions":{"displayType":"table","customPlotOptions":{},"pivotColumns":null,"pivotAggregation":null,"xColumns":null,"yColumns":null},"columnCustomDisplayInfos":{},"aggType":"","isJsonSchema":true,"removedWidgets":[],"aggSchema":[],"schema":[{"name":"name","type":"\"string\"","metadata":"{}"},{"name":"edu","type":"\"string\"","metadata":"{}"},{"name":"yearofexp","type":"\"string\"","metadata":"{}"},{"name":"tech","type":"\"string\"","metadata":"{}"},{"name":"mobile","type":"\"string\"","metadata":"{}"}],"aggError":"","aggData":[],"addedWidgets":{},"metadata":{},"dbfsResultPath":null,"type":"table","aggOverflow":false,"aggSeriesLimitReached":false,"arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>name</th><th>edu</th><th>yearofexp</th><th>tech</th><th>mobile</th></tr></thead><tbody><tr><td>Azar</td><td>BE</td><td>8</td><td>BigData</td><td>9273541255</td></tr><tr><td>Ramesh</td><td>Btech</td><td>3</td><td>pyspark</td><td>912354885</td></tr><tr><td>Krishna</td><td>MBA</td><td>2</td><td>sql</td><td>8106186796</td></tr></tbody></table></div>"]}}],"execution_count":0},{"cell_type":"code","source":["%fs\nhead dbfs:/FileStore/shared_uploads/krishnareddyesuru@gmail.com/delimit_input___Copy.txt"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"0fea6305-120f-4338-8d4b-6aabace7d3ce"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">ABCD|BE|8|BigData|9273541255|EFGH|Btech|3|pyspark|912354885|IJKL|MBA|2|sql|8106186796\n</div>","removedWidgets":[],"addedWidgets":{},"metadata":{"isDbfsCommandResult":false},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">ABCD|BE|8|BigData|9273541255|EFGH|Btech|3|pyspark|912354885|IJKL|MBA|2|sql|8106186796\n</div>"]}}],"execution_count":0},{"cell_type":"code","source":["df10 = spark.read.csv(\"dbfs:/FileStore/shared_uploads/krishnareddyesuru@gmail.com/delimit_input___Copy.txt\")\ndf11=df10.withColumn(\"column\",regexp_replace(\"_c0\",\"(.*?\\\\|){5}\",\"$0-\"))\ndf22=df11.withColumn(\"exp_column\",explode(split(\"column\",\"\\|-\"))).select(\"exp_column\")\ndf22.select(split(\"exp_column\",\"\\|\")).display()\n\n"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"b1c5c944-b03b-43e0-9a84-b68f3cfa149d"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"overflow":false,"datasetInfos":[],"data":[[["ABCD","BE","8","BigData","9273541255"]],[["EFGH","Btech","3","pyspark","912354885"]],[["IJKL","MBA","2","sql","8106186796"]]],"plotOptions":{"displayType":"table","customPlotOptions":{},"pivotColumns":null,"pivotAggregation":null,"xColumns":null,"yColumns":null},"columnCustomDisplayInfos":{},"aggType":"","isJsonSchema":true,"removedWidgets":[],"aggSchema":[],"schema":[{"name":"split(exp_column, \\|, -1)","type":"{\"type\":\"array\",\"elementType\":\"string\",\"containsNull\":false}","metadata":"{}"}],"aggError":"","aggData":[],"addedWidgets":{},"metadata":{},"dbfsResultPath":null,"type":"table","aggOverflow":false,"aggSeriesLimitReached":false,"arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>split(exp_column, \\|, -1)</th></tr></thead><tbody><tr><td>List(ABCD, BE, 8, BigData, 9273541255)</td></tr><tr><td>List(EFGH, Btech, 3, pyspark, 912354885)</td></tr><tr><td>List(IJKL, MBA, 2, sql, 8106186796)</td></tr></tbody></table></div>"]}}],"execution_count":0},{"cell_type":"code","source":["spark1 = SparkSession.builder.appName(\"testing\").getOrCreate()\n#rdd11 = spark1.sparkContext.parallelize([])\nrdd11=spark1.sparkContext.emptyRDD()\ndf22.select(\"exp_column\").rdd.map(lambda x:x[0].split(\"|\")).collect()\ndf22.display()\nrdd11 = df22.select(\"exp_column\").rdd.map(lambda x:x[0].split(\"|\"))\ndf33 = rdd11.toDF([\"name\",\"edu\",\"yearofexp\",\"tech\",\"mobile\"])\ndf33.printSchema()\ndf33.display()\n"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"990b98da-a3d9-4f67-8569-38643a582bb9"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"overflow":false,"datasetInfos":[],"data":[["ABCD|BE|8|BigData|9273541255"],["EFGH|Btech|3|pyspark|912354885"],["IJKL|MBA|2|sql|8106186796"]],"plotOptions":{"displayType":"table","customPlotOptions":{},"pivotColumns":null,"pivotAggregation":null,"xColumns":null,"yColumns":null},"columnCustomDisplayInfos":{},"aggType":"","isJsonSchema":true,"removedWidgets":[],"aggSchema":[],"schema":[{"name":"exp_column","type":"\"string\"","metadata":"{}"}],"aggError":"","aggData":[],"addedWidgets":{},"metadata":{},"dbfsResultPath":null,"type":"table","aggOverflow":false,"aggSeriesLimitReached":false,"arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>exp_column</th></tr></thead><tbody><tr><td>ABCD|BE|8|BigData|9273541255</td></tr><tr><td>EFGH|Btech|3|pyspark|912354885</td></tr><tr><td>IJKL|MBA|2|sql|8106186796</td></tr></tbody></table></div>"]}},{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"root\n |-- name: string (nullable = true)\n |-- edu: string (nullable = true)\n |-- yearofexp: string (nullable = true)\n |-- tech: string (nullable = true)\n |-- mobile: string (nullable = true)\n\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["root\n |-- name: string (nullable = true)\n |-- edu: string (nullable = true)\n |-- yearofexp: string (nullable = true)\n |-- tech: string (nullable = true)\n |-- mobile: string (nullable = true)\n\n"]}},{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"overflow":false,"datasetInfos":[],"data":[["ABCD","BE","8","BigData","9273541255"],["EFGH","Btech","3","pyspark","912354885"],["IJKL","MBA","2","sql","8106186796"]],"plotOptions":{"displayType":"table","customPlotOptions":{},"pivotColumns":null,"pivotAggregation":null,"xColumns":null,"yColumns":null},"columnCustomDisplayInfos":{},"aggType":"","isJsonSchema":true,"removedWidgets":[],"aggSchema":[],"schema":[{"name":"name","type":"\"string\"","metadata":"{}"},{"name":"edu","type":"\"string\"","metadata":"{}"},{"name":"yearofexp","type":"\"string\"","metadata":"{}"},{"name":"tech","type":"\"string\"","metadata":"{}"},{"name":"mobile","type":"\"string\"","metadata":"{}"}],"aggError":"","aggData":[],"addedWidgets":{},"metadata":{},"dbfsResultPath":null,"type":"table","aggOverflow":false,"aggSeriesLimitReached":false,"arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>name</th><th>edu</th><th>yearofexp</th><th>tech</th><th>mobile</th></tr></thead><tbody><tr><td>ABCD</td><td>BE</td><td>8</td><td>BigData</td><td>9273541255</td></tr><tr><td>EFGH</td><td>Btech</td><td>3</td><td>pyspark</td><td>912354885</td></tr><tr><td>IJKL</td><td>MBA</td><td>2</td><td>sql</td><td>8106186796</td></tr></tbody></table></div>"]}}],"execution_count":0},{"cell_type":"code","source":["Q2: consider a file with corrupt/bad data as show in the file how will you load and handle the bad data into spark dataframe?\ndbfs:/FileStore/shared_uploads/krishnareddyesuru@gmail.com/Question_2.txt\n\nspark.read.option(\"mode\",\"FAILFAST\").csv()\n\nmodes are 3 types \n1. PERMISSIVE --> DEFAULT MODE\n2.FAILFAST -->stop processing the bad file further.\n3. DROPMALFORMED -->remove the bad data and process the file\n\n"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"1149a58a-daec-4c11-a278-651f1f4d3c7d"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["%fs\nhead\ndbfs:/FileStore/shared_uploads/krishnareddyesuru@gmail.com/Question_2.txt"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"84f557f5-03f6-4bdb-930c-cbc2c20a7c0f"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">emp_no,emp_name,department\r\n101,reddy,healthcare\r\ninvalid entry,decription:bad record entry\r\n102,Krishna,IT\r\nconnection lost,description:poor connection\r\n104,pavan,HR\r\nBad Record,description:corrupt record\n</div>","removedWidgets":[],"addedWidgets":{},"metadata":{"isDbfsCommandResult":false},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">emp_no,emp_name,department\r\n101,reddy,healthcare\r\ninvalid entry,decription:bad record entry\r\n102,Krishna,IT\r\nconnection lost,description:poor connection\r\n104,pavan,HR\r\nBad Record,description:corrupt record\n</div>"]}}],"execution_count":0},{"cell_type":"code","source":["import pyspark\nfrom pyspark.sql import SparkSession\nspark = SparkSession.builder.appName(\"baddata\").getOrCreate()\nrdd = spark.sparkContext.parallelize([])\ndf99 = spark.read.option(\"mode\",\"FAILFAST\").csv(\"dbfs:/FileStore/shared_uploads/krishnareddyesuru@gmail.com/Question_2.txt\",header = True)\ndf99.display()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"e8dfb57b-dfd2-4caf-8fb9-786e588d6c47"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 7.0 failed 1 times, most recent failure: Lost task 0.0 in stage 7.0 (TID 7) (ip-10-172-205-67.us-west-2.compute.internal executor driver): com.databricks.sql.io.FileReadException: Error while reading file dbfs:REDACTED_LOCAL_PART@gmail.com/Question_2.txt.\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1$$anon$2.logFileNameAndThrow(FileScanRDD.scala:521)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1$$anon$2.getNext(FileScanRDD.scala:494)\n\tat org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:73)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.$anonfun$hasNext$1(FileScanRDD.scala:356)\n\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:351)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.sql.execution.collect.UnsafeRowBatchUtils$.encodeUnsafeRows(UnsafeRowBatchUtils.scala:80)\n\tat org.apache.spark.sql.execution.collect.Collector.$anonfun$processFunc$1(Collector.scala:155)\n\tat org.apache.spark.scheduler.ResultTask.$anonfun$runTask$3(ResultTask.scala:75)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.ResultTask.$anonfun$runTask$1(ResultTask.scala:75)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:55)\n\tat org.apache.spark.scheduler.Task.doRunTask(Task.scala:156)\n\tat org.apache.spark.scheduler.Task.$anonfun$run$1(Task.scala:125)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:95)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$13(Executor.scala:832)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1681)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:835)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:690)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.spark.SparkException: Malformed records are detected in record parsing. Parse Mode: FAILFAST. To process malformed records as null result, try setting the option 'mode' as 'PERMISSIVE'.\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.malformedRecordsDetectedInRecordParsingError(QueryExecutionErrors.scala:1384)\n\tat org.apache.spark.sql.catalyst.util.FailureSafeParser.parse(FailureSafeParser.scala:103)\n\tat org.apache.spark.sql.catalyst.csv.UnivocityParser$.$anonfun$parseIterator$2(UnivocityParser.scala:458)\n\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n\tat org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:31)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1$$anon$2.getNext(FileScanRDD.scala:459)\n\t... 26 more\nCaused by: org.apache.spark.sql.catalyst.util.BadRecordException: org.apache.spark.sql.catalyst.csv.MalformedCSVException: Malformed CSV record\nCaused by: org.apache.spark.sql.catalyst.csv.MalformedCSVException: Malformed CSV record\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:3029)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2976)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2970)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2970)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1390)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1390)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1390)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3238)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3179)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3167)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:1152)\n\tat org.apache.spark.SparkContext.runJobInternal(SparkContext.scala:2651)\n\tat org.apache.spark.sql.execution.collect.Collector.runSparkJobs(Collector.scala:266)\n\tat org.apache.spark.sql.execution.collect.Collector.collect(Collector.scala:276)\n\tat org.apache.spark.sql.execution.collect.Collector$.collect(Collector.scala:81)\n\tat org.apache.spark.sql.execution.collect.Collector$.collect(Collector.scala:87)\n\tat org.apache.spark.sql.execution.collect.InternalRowFormat$.collect(cachedSparkResults.scala:75)\n\tat org.apache.spark.sql.execution.collect.InternalRowFormat$.collect(cachedSparkResults.scala:62)\n\tat org.apache.spark.sql.execution.ResultCacheManager.collectResult$1(ResultCacheManager.scala:573)\n\tat org.apache.spark.sql.execution.ResultCacheManager.computeResult(ResultCacheManager.scala:582)\n\tat org.apache.spark.sql.execution.ResultCacheManager.$anonfun$getOrComputeResultInternal$1(ResultCacheManager.scala:528)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.sql.execution.ResultCacheManager.getOrComputeResultInternal(ResultCacheManager.scala:527)\n\tat org.apache.spark.sql.execution.ResultCacheManager.getOrComputeResult(ResultCacheManager.scala:424)\n\tat org.apache.spark.sql.execution.ResultCacheManager.getOrComputeResult(ResultCacheManager.scala:403)\n\tat org.apache.spark.sql.execution.SparkPlan.executeCollectResult(SparkPlan.scala:422)\n\tat org.apache.spark.sql.Dataset.collectResult(Dataset.scala:3153)\n\tat org.apache.spark.sql.Dataset.$anonfun$collectResult$1(Dataset.scala:3144)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3951)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withCustomExecutionEnv$8(SQLExecution.scala:239)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:386)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withCustomExecutionEnv$1(SQLExecution.scala:186)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:968)\n\tat org.apache.spark.sql.execution.SQLExecution$.withCustomExecutionEnv(SQLExecution.scala:141)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:336)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3949)\n\tat org.apache.spark.sql.Dataset.collectResult(Dataset.scala:3143)\n\tat com.databricks.backend.daemon.driver.OutputAggregator$.withOutputAggregation0(OutputAggregator.scala:266)\n\tat com.databricks.backend.daemon.driver.OutputAggregator$.withOutputAggregation(OutputAggregator.scala:100)\n\tat com.databricks.backend.daemon.driver.PythonDriverLocalBase.generateTableResult(PythonDriverLocalBase.scala:587)\n\tat com.databricks.backend.daemon.driver.JupyterDriverLocal.computeListResultsItem(JupyterDriverLocal.scala:1106)\n\tat com.databricks.backend.daemon.driver.JupyterDriverLocal$JupyterEntryPoint.addCustomDisplayData(JupyterDriverLocal.scala:370)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:380)\n\tat py4j.Gateway.invoke(Gateway.java:295)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:251)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: com.databricks.sql.io.FileReadException: Error while reading file dbfs:REDACTED_LOCAL_PART@gmail.com/Question_2.txt.\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1$$anon$2.logFileNameAndThrow(FileScanRDD.scala:521)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1$$anon$2.getNext(FileScanRDD.scala:494)\n\tat org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:73)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.$anonfun$hasNext$1(FileScanRDD.scala:356)\n\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:351)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.sql.execution.collect.UnsafeRowBatchUtils$.encodeUnsafeRows(UnsafeRowBatchUtils.scala:80)\n\tat org.apache.spark.sql.execution.collect.Collector.$anonfun$processFunc$1(Collector.scala:155)\n\tat org.apache.spark.scheduler.ResultTask.$anonfun$runTask$3(ResultTask.scala:75)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.ResultTask.$anonfun$runTask$1(ResultTask.scala:75)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:55)\n\tat org.apache.spark.scheduler.Task.doRunTask(Task.scala:156)\n\tat org.apache.spark.scheduler.Task.$anonfun$run$1(Task.scala:125)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:95)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$13(Executor.scala:832)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1681)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:835)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:690)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\nCaused by: org.apache.spark.SparkException: Malformed records are detected in record parsing. Parse Mode: FAILFAST. To process malformed records as null result, try setting the option 'mode' as 'PERMISSIVE'.\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.malformedRecordsDetectedInRecordParsingError(QueryExecutionErrors.scala:1384)\n\tat org.apache.spark.sql.catalyst.util.FailureSafeParser.parse(FailureSafeParser.scala:103)\n\tat org.apache.spark.sql.catalyst.csv.UnivocityParser$.$anonfun$parseIterator$2(UnivocityParser.scala:458)\n\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n\tat org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:31)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1$$anon$2.getNext(FileScanRDD.scala:459)\n\t... 26 more\nCaused by: org.apache.spark.sql.catalyst.util.BadRecordException: org.apache.spark.sql.catalyst.csv.MalformedCSVException: Malformed CSV record\nCaused by: org.apache.spark.sql.catalyst.csv.MalformedCSVException: Malformed CSV record\n","errorSummary":"FileReadException: Error while reading file dbfs:REDACTED_LOCAL_PART@gmail.com/Question_2.txt.\nCaused by: SparkException: Malformed records are detected in record parsing. Parse Mode: FAILFAST. To process malformed records as null result, try setting the option 'mode' as 'PERMISSIVE'.\nCaused by: BadRecordException: org.apache.spark.sql.catalyst.csv.MalformedCSVException: Malformed CSV record\nCaused by: MalformedCSVException: Malformed CSV record","metadata":{},"errorTraceType":"html","type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\norg.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 7.0 failed 1 times, most recent failure: Lost task 0.0 in stage 7.0 (TID 7) (ip-10-172-205-67.us-west-2.compute.internal executor driver): com.databricks.sql.io.FileReadException: Error while reading file dbfs:REDACTED_LOCAL_PART@gmail.com/Question_2.txt.\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1$$anon$2.logFileNameAndThrow(FileScanRDD.scala:521)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1$$anon$2.getNext(FileScanRDD.scala:494)\n\tat org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:73)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.$anonfun$hasNext$1(FileScanRDD.scala:356)\n\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:351)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.sql.execution.collect.UnsafeRowBatchUtils$.encodeUnsafeRows(UnsafeRowBatchUtils.scala:80)\n\tat org.apache.spark.sql.execution.collect.Collector.$anonfun$processFunc$1(Collector.scala:155)\n\tat org.apache.spark.scheduler.ResultTask.$anonfun$runTask$3(ResultTask.scala:75)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.ResultTask.$anonfun$runTask$1(ResultTask.scala:75)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:55)\n\tat org.apache.spark.scheduler.Task.doRunTask(Task.scala:156)\n\tat org.apache.spark.scheduler.Task.$anonfun$run$1(Task.scala:125)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:95)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$13(Executor.scala:832)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1681)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:835)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:690)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.spark.SparkException: Malformed records are detected in record parsing. Parse Mode: FAILFAST. To process malformed records as null result, try setting the option 'mode' as 'PERMISSIVE'.\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.malformedRecordsDetectedInRecordParsingError(QueryExecutionErrors.scala:1384)\n\tat org.apache.spark.sql.catalyst.util.FailureSafeParser.parse(FailureSafeParser.scala:103)\n\tat org.apache.spark.sql.catalyst.csv.UnivocityParser$.$anonfun$parseIterator$2(UnivocityParser.scala:458)\n\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n\tat org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:31)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1$$anon$2.getNext(FileScanRDD.scala:459)\n\t... 26 more\nCaused by: org.apache.spark.sql.catalyst.util.BadRecordException: org.apache.spark.sql.catalyst.csv.MalformedCSVException: Malformed CSV record\nCaused by: org.apache.spark.sql.catalyst.csv.MalformedCSVException: Malformed CSV record\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:3029)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2976)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2970)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2970)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1390)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1390)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1390)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3238)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3179)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3167)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:1152)\n\tat org.apache.spark.SparkContext.runJobInternal(SparkContext.scala:2651)\n\tat org.apache.spark.sql.execution.collect.Collector.runSparkJobs(Collector.scala:266)\n\tat org.apache.spark.sql.execution.collect.Collector.collect(Collector.scala:276)\n\tat org.apache.spark.sql.execution.collect.Collector$.collect(Collector.scala:81)\n\tat org.apache.spark.sql.execution.collect.Collector$.collect(Collector.scala:87)\n\tat org.apache.spark.sql.execution.collect.InternalRowFormat$.collect(cachedSparkResults.scala:75)\n\tat org.apache.spark.sql.execution.collect.InternalRowFormat$.collect(cachedSparkResults.scala:62)\n\tat org.apache.spark.sql.execution.ResultCacheManager.collectResult$1(ResultCacheManager.scala:573)\n\tat org.apache.spark.sql.execution.ResultCacheManager.computeResult(ResultCacheManager.scala:582)\n\tat org.apache.spark.sql.execution.ResultCacheManager.$anonfun$getOrComputeResultInternal$1(ResultCacheManager.scala:528)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.sql.execution.ResultCacheManager.getOrComputeResultInternal(ResultCacheManager.scala:527)\n\tat org.apache.spark.sql.execution.ResultCacheManager.getOrComputeResult(ResultCacheManager.scala:424)\n\tat org.apache.spark.sql.execution.ResultCacheManager.getOrComputeResult(ResultCacheManager.scala:403)\n\tat org.apache.spark.sql.execution.SparkPlan.executeCollectResult(SparkPlan.scala:422)\n\tat org.apache.spark.sql.Dataset.collectResult(Dataset.scala:3153)\n\tat org.apache.spark.sql.Dataset.$anonfun$collectResult$1(Dataset.scala:3144)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3951)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withCustomExecutionEnv$8(SQLExecution.scala:239)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:386)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withCustomExecutionEnv$1(SQLExecution.scala:186)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:968)\n\tat org.apache.spark.sql.execution.SQLExecution$.withCustomExecutionEnv(SQLExecution.scala:141)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:336)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3949)\n\tat org.apache.spark.sql.Dataset.collectResult(Dataset.scala:3143)\n\tat com.databricks.backend.daemon.driver.OutputAggregator$.withOutputAggregation0(OutputAggregator.scala:266)\n\tat com.databricks.backend.daemon.driver.OutputAggregator$.withOutputAggregation(OutputAggregator.scala:100)\n\tat com.databricks.backend.daemon.driver.PythonDriverLocalBase.generateTableResult(PythonDriverLocalBase.scala:587)\n\tat com.databricks.backend.daemon.driver.JupyterDriverLocal.computeListResultsItem(JupyterDriverLocal.scala:1106)\n\tat com.databricks.backend.daemon.driver.JupyterDriverLocal$JupyterEntryPoint.addCustomDisplayData(JupyterDriverLocal.scala:370)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:380)\n\tat py4j.Gateway.invoke(Gateway.java:295)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:251)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: com.databricks.sql.io.FileReadException: Error while reading file dbfs:REDACTED_LOCAL_PART@gmail.com/Question_2.txt.\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1$$anon$2.logFileNameAndThrow(FileScanRDD.scala:521)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1$$anon$2.getNext(FileScanRDD.scala:494)\n\tat org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:73)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.$anonfun$hasNext$1(FileScanRDD.scala:356)\n\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:351)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.sql.execution.collect.UnsafeRowBatchUtils$.encodeUnsafeRows(UnsafeRowBatchUtils.scala:80)\n\tat org.apache.spark.sql.execution.collect.Collector.$anonfun$processFunc$1(Collector.scala:155)\n\tat org.apache.spark.scheduler.ResultTask.$anonfun$runTask$3(ResultTask.scala:75)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.ResultTask.$anonfun$runTask$1(ResultTask.scala:75)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:55)\n\tat org.apache.spark.scheduler.Task.doRunTask(Task.scala:156)\n\tat org.apache.spark.scheduler.Task.$anonfun$run$1(Task.scala:125)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:95)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$13(Executor.scala:832)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1681)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:835)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:690)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\nCaused by: org.apache.spark.SparkException: Malformed records are detected in record parsing. Parse Mode: FAILFAST. To process malformed records as null result, try setting the option 'mode' as 'PERMISSIVE'.\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.malformedRecordsDetectedInRecordParsingError(QueryExecutionErrors.scala:1384)\n\tat org.apache.spark.sql.catalyst.util.FailureSafeParser.parse(FailureSafeParser.scala:103)\n\tat org.apache.spark.sql.catalyst.csv.UnivocityParser$.$anonfun$parseIterator$2(UnivocityParser.scala:458)\n\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n\tat org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:31)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1$$anon$2.getNext(FileScanRDD.scala:459)\n\t... 26 more\nCaused by: org.apache.spark.sql.catalyst.util.BadRecordException: org.apache.spark.sql.catalyst.csv.MalformedCSVException: Malformed CSV record\nCaused by: org.apache.spark.sql.catalyst.csv.MalformedCSVException: Malformed CSV record"]}}],"execution_count":0},{"cell_type":"code","source":["df98 = spark.read.option(\"mode\",\"DROPMALFORMED\").csv(\"dbfs:/FileStore/shared_uploads/krishnareddyesuru@gmail.com/Question_2.txt\",header = True)\ndf98.display()\ndf97 = spark.read.option(\"mode\",\"PERMISSIVE\").csv(\"dbfs:/FileStore/shared_uploads/krishnareddyesuru@gmail.com/Question_2.txt\",header = True)\ndf97.display()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"51a28183-8cea-4c15-a18c-d2450e11637f"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"overflow":false,"datasetInfos":[],"data":[["101","reddy","healthcare"],["102","Krishna","IT"],["104","pavan","HR"]],"plotOptions":{"displayType":"table","customPlotOptions":{},"pivotColumns":null,"pivotAggregation":null,"xColumns":null,"yColumns":null},"columnCustomDisplayInfos":{},"aggType":"","isJsonSchema":true,"removedWidgets":[],"aggSchema":[],"schema":[{"name":"emp_no","type":"\"string\"","metadata":"{}"},{"name":"emp_name","type":"\"string\"","metadata":"{}"},{"name":"department","type":"\"string\"","metadata":"{}"}],"aggError":"","aggData":[],"addedWidgets":{},"metadata":{},"dbfsResultPath":null,"type":"table","aggOverflow":false,"aggSeriesLimitReached":false,"arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>emp_no</th><th>emp_name</th><th>department</th></tr></thead><tbody><tr><td>101</td><td>reddy</td><td>healthcare</td></tr><tr><td>102</td><td>Krishna</td><td>IT</td></tr><tr><td>104</td><td>pavan</td><td>HR</td></tr></tbody></table></div>"]}},{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"overflow":false,"datasetInfos":[],"data":[["101","reddy","healthcare"],["invalid entry","decription:bad record entry",null],["102","Krishna","IT"],["connection lost","description:poor connection",null],["104","pavan","HR"],["Bad Record","description:corrupt record",null]],"plotOptions":{"displayType":"table","customPlotOptions":{},"pivotColumns":null,"pivotAggregation":null,"xColumns":null,"yColumns":null},"columnCustomDisplayInfos":{},"aggType":"","isJsonSchema":true,"removedWidgets":[],"aggSchema":[],"schema":[{"name":"emp_no","type":"\"string\"","metadata":"{}"},{"name":"emp_name","type":"\"string\"","metadata":"{}"},{"name":"department","type":"\"string\"","metadata":"{}"}],"aggError":"","aggData":[],"addedWidgets":{},"metadata":{},"dbfsResultPath":null,"type":"table","aggOverflow":false,"aggSeriesLimitReached":false,"arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>emp_no</th><th>emp_name</th><th>department</th></tr></thead><tbody><tr><td>101</td><td>reddy</td><td>healthcare</td></tr><tr><td>invalid entry</td><td>decription:bad record entry</td><td>null</td></tr><tr><td>102</td><td>Krishna</td><td>IT</td></tr><tr><td>connection lost</td><td>description:poor connection</td><td>null</td></tr><tr><td>104</td><td>pavan</td><td>HR</td></tr><tr><td>Bad Record</td><td>description:corrupt record</td><td>null</td></tr></tbody></table></div>"]}}],"execution_count":0},{"cell_type":"code","source":["Q3:consider a file with delimted ~| how will you load it in the spark data frame?\n"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"5517aca0-03aa-452f-ac37-8f81115afe79"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["%fs\nhead\ndbfs:/FileStore/shared_uploads/krishnareddyesuru@gmail.com/Question_3.txt"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"6cc5fe34-7742-4627-be39-051421607481"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">Name~|age\r\nKrishna,Reddy~|38\r\nGali,Mani~|26\r\nMR,Esuru~|25\n</div>","removedWidgets":[],"addedWidgets":{},"metadata":{"isDbfsCommandResult":false},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Name~|age\r\nKrishna,Reddy~|38\r\nGali,Mani~|26\r\nMR,Esuru~|25\n</div>"]}}],"execution_count":0},{"cell_type":"code","source":["#first method\ndf33 = spark.read.option(\"mode\",\"permissive\").option(\"delimiter\",\"~|\").csv(\"dbfs:/FileStore/shared_uploads/krishnareddyesuru@gmail.com/Question_3.txt\",header = True)\ndf33.display()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"42ea1861-ddc0-459c-90d1-49cc49d26003"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"overflow":false,"datasetInfos":[],"data":[["Krishna,Reddy","38"],["Gali,Mani","26"],["MR,Esuru","25"]],"plotOptions":{"displayType":"table","customPlotOptions":{},"pivotColumns":null,"pivotAggregation":null,"xColumns":null,"yColumns":null},"columnCustomDisplayInfos":{},"aggType":"","isJsonSchema":true,"removedWidgets":[],"aggSchema":[],"schema":[{"name":"Name","type":"\"string\"","metadata":"{}"},{"name":"age","type":"\"string\"","metadata":"{}"}],"aggError":"","aggData":[],"addedWidgets":{},"metadata":{},"dbfsResultPath":null,"type":"table","aggOverflow":false,"aggSeriesLimitReached":false,"arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>Name</th><th>age</th></tr></thead><tbody><tr><td>Krishna,Reddy</td><td>38</td></tr><tr><td>Gali,Mani</td><td>26</td></tr><tr><td>MR,Esuru</td><td>25</td></tr></tbody></table></div>"]}}],"execution_count":0},{"cell_type":"code","source":["#second method\ndf44= spark.read.text(\"dbfs:/FileStore/shared_uploads/krishnareddyesuru@gmail.com/Question_3.txt\")\nheader = df44.first()[0]\nheader\nschema = header.split(\"~|\")\nschema\n#df44.filter(df44[\"value\"] != header).show()\ndf44.filter(df44[\"value\"] != header).rdd.map(lambda x:x[0].split(\"~|\")).collect()\ndf55= df44.filter(df44[\"value\"] != header).rdd.map(lambda x:x[0].split(\"~|\"))\ndf66 = df55.toDF(schema)\ndf66.show(truncate = False)\n#df44.show(truncate = False)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"6b39cb7c-5379-4ce2-a92c-92bfee03a9c9"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"+-----------------+\n|            value|\n+-----------------+\n|Krishna,Reddy~|38|\n|    Gali,Mani~|26|\n|     MR,Esuru~|25|\n+-----------------+\n\n+-------------+---+\n|Name         |age|\n+-------------+---+\n|Krishna,Reddy|38 |\n|Gali,Mani    |26 |\n|MR,Esuru     |25 |\n+-------------+---+\n\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["+-----------------+\n|            value|\n+-----------------+\n|Krishna,Reddy~|38|\n|    Gali,Mani~|26|\n|     MR,Esuru~|25|\n+-----------------+\n\n+-------------+---+\n|Name         |age|\n+-------------+---+\n|Krishna,Reddy|38 |\n|Gali,Mani    |26 |\n|MR,Esuru     |25 |\n+-------------+---+\n\n"]}}],"execution_count":0},{"cell_type":"code","source":["Q4:merge two dataframe in pyspark\ndbfs:/FileStore/shared_uploads/krishnareddyesuru@gmail.com/Question_4.txt\ndbfs:/FileStore/shared_uploads/krishnareddyesuru@gmail.com/Question_4A.txt"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"f72e82eb-fe8c-4305-8652-d94c124c5ffb"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["%fs\nhead\ndbfs:/FileStore/shared_uploads/krishnareddyesuru@gmail.com/Question_4.txt"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"2b3a7ffa-b9c5-4497-a127-256d7eefbb46"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">Name|Age\nKrishna,Reddt|35\nmani,gali|26\n</div>","removedWidgets":[],"addedWidgets":{},"metadata":{"isDbfsCommandResult":false},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Name|Age\nKrishna,Reddt|35\nmani,gali|26\n</div>"]}}],"execution_count":0},{"cell_type":"code","source":["%fs\nhead\ndbfs:/FileStore/shared_uploads/krishnareddyesuru@gmail.com/Question_4A.txt"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"e72c2cdf-3079-4873-a8d0-96997eaf7c4e"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">Name|Age|Gender\nEsuru,Kurmarao|32|M\nmohini,gali|26|F\nPotti,mani|25|F\n</div>","removedWidgets":[],"addedWidgets":{},"metadata":{"isDbfsCommandResult":false},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Name|Age|Gender\nEsuru,Kurmarao|32|M\nmohini,gali|26|F\nPotti,mani|25|F\n</div>"]}}],"execution_count":0},{"cell_type":"code","source":["df33 = spark.read.option(\"delimiter\",\"|\").csv(\"dbfs:/FileStore/shared_uploads/krishnareddyesuru@gmail.com/Question_4.txt\",header = True)\ndf33.display()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"4a7f78a7-d6da-438e-94c2-ec285d23ae67"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)\n\u001B[0;32m<command-3001292341343909>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[0;32m----> 1\u001B[0;31m \u001B[0mdf33\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mspark\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mread\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0moption\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"delimiter\"\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\"|\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mcsv\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"dbfs:/FileStore/shared_uploads/krishnareddyesuru@gmail.com/Question_4.txt\"\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0mheader\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;32mTrue\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0mschema\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mschema\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      2\u001B[0m \u001B[0mdf33\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mdisplay\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;31mNameError\u001B[0m: name 'schema' is not defined","errorSummary":"<span class='ansi-red-fg'>NameError</span>: name 'schema' is not defined","metadata":{},"errorTraceType":"ansi","type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/plain":["\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)\n\u001B[0;32m<command-3001292341343909>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[0;32m----> 1\u001B[0;31m \u001B[0mdf33\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mspark\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mread\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0moption\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"delimiter\"\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\"|\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mcsv\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"dbfs:/FileStore/shared_uploads/krishnareddyesuru@gmail.com/Question_4.txt\"\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0mheader\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;32mTrue\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0mschema\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mschema\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      2\u001B[0m \u001B[0mdf33\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mdisplay\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;31mNameError\u001B[0m: name 'schema' is not defined"]}}],"execution_count":0},{"cell_type":"code","source":["df31 = spark.read.option(\"delimiter\",\"|\").csv(\"dbfs:/FileStore/shared_uploads/krishnareddyesuru@gmail.com/Question_4A.txt\", header = True)\ndf31.display()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"580c9da9-421a-4f0f-be5e-9e970ec1188b"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"overflow":false,"datasetInfos":[],"data":[["Esuru,Kurmarao","32","M"],["mohini,gali","26","F"],["Potti,mani","25","F"]],"plotOptions":{"displayType":"table","customPlotOptions":{},"pivotColumns":null,"pivotAggregation":null,"xColumns":null,"yColumns":null},"columnCustomDisplayInfos":{},"aggType":"","isJsonSchema":true,"removedWidgets":[],"aggSchema":[],"schema":[{"name":"Name","type":"\"string\"","metadata":"{}"},{"name":"Age","type":"\"string\"","metadata":"{}"},{"name":"Gender","type":"\"string\"","metadata":"{}"}],"aggError":"","aggData":[],"addedWidgets":{},"metadata":{},"dbfsResultPath":null,"type":"table","aggOverflow":false,"aggSeriesLimitReached":false,"arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>Name</th><th>Age</th><th>Gender</th></tr></thead><tbody><tr><td>Esuru,Kurmarao</td><td>32</td><td>M</td></tr><tr><td>mohini,gali</td><td>26</td><td>F</td></tr><tr><td>Potti,mani</td><td>25</td><td>F</td></tr></tbody></table></div>"]}}],"execution_count":0},{"cell_type":"code","source":["import pyspark\nfrom pyspark.sql.functions import *\nfrom pyspark.sql.types import *\n#df33.union(df31)\n#First method withcolumn and union\ndf34 =df33.withColumn(\"Gender\",lit(\"null\"))\ndf34.show()\ndf34.union(df31).show()\n"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"8881a3df-a472-433d-9c40-6c7c0013e70d"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"+-------------+---+------+\n|         Name|Age|Gender|\n+-------------+---+------+\n|Krishna,Reddt| 35|  null|\n|    mani,gali| 26|  null|\n+-------------+---+------+\n\n+--------------+---+------+\n|          Name|Age|Gender|\n+--------------+---+------+\n| Krishna,Reddt| 35|  null|\n|     mani,gali| 26|  null|\n|Esuru,Kurmarao| 32|     M|\n|   mohini,gali| 26|     F|\n|    Potti,mani| 25|     F|\n+--------------+---+------+\n\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["+-------------+---+------+\n|         Name|Age|Gender|\n+-------------+---+------+\n|Krishna,Reddt| 35|  null|\n|    mani,gali| 26|  null|\n+-------------+---+------+\n\n+--------------+---+------+\n|          Name|Age|Gender|\n+--------------+---+------+\n| Krishna,Reddt| 35|  null|\n|     mani,gali| 26|  null|\n|Esuru,Kurmarao| 32|     M|\n|   mohini,gali| 26|     F|\n|    Potti,mani| 25|     F|\n+--------------+---+------+\n\n"]}}],"execution_count":0},{"cell_type":"code","source":["from pyspark.sql.functions import *\nfrom pyspark.sql.types import *\n#second method Schema and union\nschema = StructType([\n            StructField(\"name\",StringType(),True),\n            StructField(\"age\",IntegerType(),True),\n            StructField(\"gender\",StringType(),True)\n])\n\nschema1 = StructType([\n            StructField(\"name\",StringType(),True),\n            StructField(\"age\",IntegerType(),True),\n            StructField(\"gender\",StringType(),True)\n])\n\ndf44 = spark.read.option(\"delimiter\",\"|\").csv(\"dbfs:/FileStore/shared_uploads/krishnareddyesuru@gmail.com/Question_4.txt\",header = True,schema=schema)\ndf44.display()\ndf55 = spark.read.option(\"delimiter\",\"|\").csv(\"dbfs:/FileStore/shared_uploads/krishnareddyesuru@gmail.com/Question_4A.txt\", header = True,schema=schema1)\ndf55.display()\ndf44.union(df55).show()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"7cf08db3-660a-405c-bae8-1040c944812d"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"overflow":false,"datasetInfos":[],"data":[["Krishna,Reddt",35,null],["mani,gali",26,null]],"plotOptions":{"displayType":"table","customPlotOptions":{},"pivotColumns":null,"pivotAggregation":null,"xColumns":null,"yColumns":null},"columnCustomDisplayInfos":{},"aggType":"","isJsonSchema":true,"removedWidgets":[],"aggSchema":[],"schema":[{"name":"name","type":"\"string\"","metadata":"{}"},{"name":"age","type":"\"integer\"","metadata":"{}"},{"name":"gender","type":"\"string\"","metadata":"{}"}],"aggError":"","aggData":[],"addedWidgets":{},"metadata":{},"dbfsResultPath":null,"type":"table","aggOverflow":false,"aggSeriesLimitReached":false,"arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>name</th><th>age</th><th>gender</th></tr></thead><tbody><tr><td>Krishna,Reddt</td><td>35</td><td>null</td></tr><tr><td>mani,gali</td><td>26</td><td>null</td></tr></tbody></table></div>"]}},{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"overflow":false,"datasetInfos":[],"data":[["Esuru,Kurmarao","32","M"],["mohini,gali","26","F"],["Potti,mani","25","F"]],"plotOptions":{"displayType":"table","customPlotOptions":{},"pivotColumns":null,"pivotAggregation":null,"xColumns":null,"yColumns":null},"columnCustomDisplayInfos":{},"aggType":"","isJsonSchema":true,"removedWidgets":[],"aggSchema":[],"schema":[{"name":"Name","type":"\"string\"","metadata":"{}"},{"name":"Age","type":"\"string\"","metadata":"{}"},{"name":"Gender","type":"\"string\"","metadata":"{}"}],"aggError":"","aggData":[],"addedWidgets":{},"metadata":{},"dbfsResultPath":null,"type":"table","aggOverflow":false,"aggSeriesLimitReached":false,"arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>Name</th><th>Age</th><th>Gender</th></tr></thead><tbody><tr><td>Esuru,Kurmarao</td><td>32</td><td>M</td></tr><tr><td>mohini,gali</td><td>26</td><td>F</td></tr><tr><td>Potti,mani</td><td>25</td><td>F</td></tr></tbody></table></div>"]}},{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"+--------------+---+------+\n|          name|age|gender|\n+--------------+---+------+\n| Krishna,Reddt| 35|  null|\n|     mani,gali| 26|  null|\n|Esuru,Kurmarao| 32|     M|\n|   mohini,gali| 26|     F|\n|    Potti,mani| 25|     F|\n+--------------+---+------+\n\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["+--------------+---+------+\n|          name|age|gender|\n+--------------+---+------+\n| Krishna,Reddt| 35|  null|\n|     mani,gali| 26|  null|\n|Esuru,Kurmarao| 32|     M|\n|   mohini,gali| 26|     F|\n|    Potti,mani| 25|     F|\n+--------------+---+------+\n\n"]}}],"execution_count":0},{"cell_type":"code","source":["#third method outer join\ndf66=df44.join(df55,df44.name==df55.name,\"outer\")\ndf66=df44.join(df55,on=[\"name\",\"age\",\"gender\"],how =\"outer\")\ndf66.show(truncate = False)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"7787902e-41bc-4209-8072-ee7c3df85a03"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"+--------------+---+------+\n|name          |age|gender|\n+--------------+---+------+\n|Krishna,Reddt |35 |null  |\n|mani,gali     |26 |null  |\n|Esuru,Kurmarao|32 |M     |\n|Potti,mani    |25 |F     |\n|mohini,gali   |26 |F     |\n+--------------+---+------+\n\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["+--------------+---+------+\n|name          |age|gender|\n+--------------+---+------+\n|Krishna,Reddt |35 |null  |\n|mani,gali     |26 |null  |\n|Esuru,Kurmarao|32 |M     |\n|Potti,mani    |25 |F     |\n|mohini,gali   |26 |F     |\n+--------------+---+------+\n\n"]}}],"execution_count":0},{"cell_type":"code","source":["from pyspark.sql.functions import *\n#fourth step automated approach\ndf33.columns\ndf31.columns\ndf66 = spark.read.option(\"delimiter\",\"|\").csv(\"dbfs:/FileStore/shared_uploads/krishnareddyesuru@gmail.com/Question_4.txt\",header = True)\n\ndf77 = spark.read.option(\"delimiter\",\"|\").csv(\"dbfs:/FileStore/shared_uploads/krishnareddyesuru@gmail.com/Question_4A.txt\", header = True)\n\n\nlistA = list(set(df66.columns)-set(df77.columns))\nlistB = list(set(df77.columns)-set(df66.columns))\nfor i in listA:\n    df77=df77.withColumn(i,lit(\"null\"))\n    \nfor i in listB:\n    df66=df66.withColumn(i,lit(\"null\"))\n    \ndf77.union(df66).show()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"0806fc32-b470-49b2-8554-f6ffc550cfe4"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"+--------------+---+------+\n|          Name|Age|Gender|\n+--------------+---+------+\n|Esuru,Kurmarao| 32|     M|\n|   mohini,gali| 26|     F|\n|    Potti,mani| 25|     F|\n| Krishna,Reddt| 35|  null|\n|     mani,gali| 26|  null|\n+--------------+---+------+\n\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["+--------------+---+------+\n|          Name|Age|Gender|\n+--------------+---+------+\n|Esuru,Kurmarao| 32|     M|\n|   mohini,gali| 26|     F|\n|    Potti,mani| 25|     F|\n| Krishna,Reddt| 35|  null|\n|     mani,gali| 26|  null|\n+--------------+---+------+\n\n"]}}],"execution_count":0},{"cell_type":"code","source":["calculate expiry data by remainig date and expiry date\ndbfs:/FileStore/shared_uploads/krishnareddyesuru@gmail.com/Question_6.txt"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"77e1c948-f125-46d0-83e5-371c350c1153"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["%fs\nhead\ndbfs:/FileStore/shared_uploads/krishnareddyesuru@gmail.com/Question_6.txt"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"4618889e-c5b6-430a-a5bf-0e4787aca602"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">rechargeid|rechargedate|remainingdays|validity\r\nR201623|20200511|1|online\r\nR201624|20200119|110|online\r\nR201625|20200105|35|online\r\nR201626|20191105|215|online\n</div>","removedWidgets":[],"addedWidgets":{},"metadata":{"isDbfsCommandResult":false},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">rechargeid|rechargedate|remainingdays|validity\r\nR201623|20200511|1|online\r\nR201624|20200119|110|online\r\nR201625|20200105|35|online\r\nR201626|20191105|215|online\n</div>"]}}],"execution_count":0},{"cell_type":"code","source":["import pyspark\nfrom pyspark.sql.types import StringType\nfrom pyspark.sql.functions import *\ndf66 = spark.read.option(\"mode\",\"PERMISSIVE\")\\\n                .option(\"delimiter\",\"|\")\\\n                .option(\"inferschema\",True)\\\n                .csv(\"dbfs:/FileStore/shared_uploads/krishnareddyesuru@gmail.com/Question_6.txt\",header=True)\n#df66.display()\n#df66.printSchema()\n#df66.withColumn(\"expiry_date\",date_add(to_date(col(\"rechargedate\"),\"yyyy-mm-dd\"),\"remainingdays\"))\ndf61=df66.withColumn(\"rechargedate\",to_date(col(\"rechargedate\").cast(StringType()),\"yyyyMMdd\"))\ndf61.show()\ndf61.printSchema()\n#df61=df66.select(col(\"rechargedate\").cast(StringType()).alias(\"to_string\"))\n#df61=df66.withColumn(\"rechargedate1\",col(\"rechargedate\").cast(StringType()))\n#df62=df61.withColumn(\"rechargedate\",to_date(col(\"rechargedate1\"),\"yyyyMMdd\"))\n#df62.show()\n#df62.printSchema()\n#df66.show()\n#df66.printSchema()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"ae6956ed-94c0-415d-8674-57588547ef46"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"+----------+------------+-------------+--------+\n|rechargeid|rechargedate|remainingdays|validity|\n+----------+------------+-------------+--------+\n|   R201623|  2020-05-11|            1|  online|\n|   R201624|  2020-01-19|          110|  online|\n|   R201625|  2020-01-05|           35|  online|\n|   R201626|  2019-11-05|          215|  online|\n+----------+------------+-------------+--------+\n\nroot\n |-- rechargeid: string (nullable = true)\n |-- rechargedate: date (nullable = true)\n |-- remainingdays: integer (nullable = true)\n |-- validity: string (nullable = true)\n\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["+----------+------------+-------------+--------+\n|rechargeid|rechargedate|remainingdays|validity|\n+----------+------------+-------------+--------+\n|   R201623|  2020-05-11|            1|  online|\n|   R201624|  2020-01-19|          110|  online|\n|   R201625|  2020-01-05|           35|  online|\n|   R201626|  2019-11-05|          215|  online|\n+----------+------------+-------------+--------+\n\nroot\n |-- rechargeid: string (nullable = true)\n |-- rechargedate: date (nullable = true)\n |-- remainingdays: integer (nullable = true)\n |-- validity: string (nullable = true)\n\n"]}}],"execution_count":0},{"cell_type":"code","source":["Q10:Convert RDD to Dataframe & Dataframe to RDD"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"dbbd979b-3322-4444-92f1-164487165816"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["import pyspark\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.functions import *\nspark=SparkSession.builder.master(\"local\").appName(\"example\").getOrCreate()\ndata = [(\"krishna\",\"reddy\",\"esuru\")]\nrdd = spark.sparkContext.parallelize(data)\n#rdd1=spark.SparkContext.emptyRDD([])\nrdd.collect()\n\n#type(rdd)\ndf=rdd.toDF([\"first\",\"middle\",\"last\"])\n#type(df)\ndf.show()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"6e07a7dd-0bce-4966-b61c-2b25a49c1634"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"+-------+------+-----+\n|  first|middle| last|\n+-------+------+-----+\n|krishna| reddy|esuru|\n+-------+------+-----+\n\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["+-------+------+-----+\n|  first|middle| last|\n+-------+------+-----+\n|krishna| reddy|esuru|\n+-------+------+-----+\n\n"]}}],"execution_count":0},{"cell_type":"code","source":[""],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"3948e5db-2cca-4e83-a368-ff063419ef40"}},"outputs":[],"execution_count":0}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"Interview_Questions_Part1","dashboards":[],"notebookMetadata":{"pythonIndentUnit":4},"language":"python","widgets":{},"notebookOrigID":1441739153835677}},"nbformat":4,"nbformat_minor":0}
