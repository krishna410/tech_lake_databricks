{"cells":[{"cell_type":"code","source":["PySpark Groupby Explained with Example\nSimilar to SQL GROUP BY clause, PySpark groupBy() function is used to collect the identical data into groups on DataFrame and perform count, sum, avg, min, max functions on the grouped data. In this article, I will explain several groupBy() examples using PySpark (Spark with Python).\nsimpleData = [(\"James\",\"Sales\",\"NY\",90000,34,10000),\n    (\"Michael\",\"Sales\",\"NY\",86000,56,20000),\n    (\"Robert\",\"Sales\",\"CA\",81000,30,23000),\n    (\"Maria\",\"Finance\",\"CA\",90000,24,23000),\n    (\"Raman\",\"Finance\",\"CA\",99000,40,24000),\n    (\"Scott\",\"Finance\",\"NY\",83000,36,19000),\n    (\"Jen\",\"Finance\",\"NY\",79000,53,15000),\n    (\"Jeff\",\"Marketing\",\"CA\",80000,25,18000),\n    (\"Kumar\",\"Marketing\",\"NY\",91000,50,21000)\n  ]\n\nschema = [\"employee_name\",\"department\",\"state\",\"salary\",\"age\",\"bonus\"]\ndf = spark.createDataFrame(data=simpleData, schema = schema)\ndf.printSchema()\ndf.show(truncate=False)\n2. PySpark groupBy on DataFrame Columns\nLet’s do the groupBy() on department column of DataFrame and then find the sum of salary for each department using sum() function.\ndf.groupBy(\"department\").sum(\"salary\").show(truncate=False)\nSimilarly, we can calculate the number of employees in each department using.\ndf.groupBy(\"department\").count()\nCalculate the minimum salary of each department using min()\ndf.groupBy(\"department\").min(\"salary\")\nCalculate the maximin salary of each department using max()\ndf.groupBy(\"department\").max(\"salary\")\nCalculate the average salary of each department using avg()\ndf.groupBy(\"department\").avg( \"salary\")\nCalculate the mean salary of each department using mean()\ndf.groupBy(\"department\").mean( \"salary\") "],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"6427001f-9617-4cb9-948c-ccc799866a06"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["import pyspark\nsimpleData = [(\"James\",\"Sales\",\"NY\",90000,34,10000),\n    (\"Michael\",\"Sales\",\"NY\",86000,56,20000),\n    (\"Robert\",\"Sales\",\"CA\",81000,30,23000),\n    (\"Maria\",\"Finance\",\"CA\",90000,24,23000),\n    (\"Raman\",\"Finance\",\"CA\",99000,40,24000),\n    (\"Scott\",\"Finance\",\"NY\",83000,36,19000),\n    (\"Jen\",\"Finance\",\"NY\",79000,53,15000),\n    (\"Jeff\",\"Marketing\",\"CA\",80000,25,18000),\n    (\"Kumar\",\"Marketing\",\"NY\",91000,50,21000)\n  ]\nschema = [\"employee_name\",\"department\",\"state\",\"salary\",\"age\",\"bonus\"]\ndf200 = spark.createDataFrame(data = simpleData,schema = schema)\ndf200.printSchema()\ndf200.show()\ndf200.groupBy(\"department\").sum(\"salary\").show()\ndf200.groupBy(\"department\").count().show()\ndf200.groupBy(\"department\").min(\"salary\").show()\ndf200.groupBy(\"department\").max(\"salary\").show()\ndf200.groupBy(\"department\").avg(\"salary\").show()\ndf200.groupBy(\"department\").mean(\"salary\").show()\n"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"2b2e4588-c87a-48ae-8dba-6bc588f3b105"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"root\n |-- employee_name: string (nullable = true)\n |-- department: string (nullable = true)\n |-- state: string (nullable = true)\n |-- salary: long (nullable = true)\n |-- age: long (nullable = true)\n |-- bonus: long (nullable = true)\n\n+-------------+----------+-----+------+---+-----+\n|employee_name|department|state|salary|age|bonus|\n+-------------+----------+-----+------+---+-----+\n|        James|     Sales|   NY| 90000| 34|10000|\n|      Michael|     Sales|   NY| 86000| 56|20000|\n|       Robert|     Sales|   CA| 81000| 30|23000|\n|        Maria|   Finance|   CA| 90000| 24|23000|\n|        Raman|   Finance|   CA| 99000| 40|24000|\n|        Scott|   Finance|   NY| 83000| 36|19000|\n|          Jen|   Finance|   NY| 79000| 53|15000|\n|         Jeff| Marketing|   CA| 80000| 25|18000|\n|        Kumar| Marketing|   NY| 91000| 50|21000|\n+-------------+----------+-----+------+---+-----+\n\n+----------+-----------+\n|department|sum(salary)|\n+----------+-----------+\n|     Sales|     257000|\n|   Finance|     351000|\n| Marketing|     171000|\n+----------+-----------+\n\n+----------+-----+\n|department|count|\n+----------+-----+\n|     Sales|    3|\n|   Finance|    4|\n| Marketing|    2|\n+----------+-----+\n\n+----------+-----------+\n|department|min(salary)|\n+----------+-----------+\n|     Sales|      81000|\n|   Finance|      79000|\n| Marketing|      80000|\n+----------+-----------+\n\n+----------+-----------+\n|department|max(salary)|\n+----------+-----------+\n|     Sales|      90000|\n|   Finance|      99000|\n| Marketing|      91000|\n+----------+-----------+\n\n+----------+-----------------+\n|department|      avg(salary)|\n+----------+-----------------+\n|     Sales|85666.66666666667|\n|   Finance|          87750.0|\n| Marketing|          85500.0|\n+----------+-----------------+\n\n+----------+-----------------+\n|department|      avg(salary)|\n+----------+-----------------+\n|     Sales|85666.66666666667|\n|   Finance|          87750.0|\n| Marketing|          85500.0|\n+----------+-----------------+\n\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["root\n |-- employee_name: string (nullable = true)\n |-- department: string (nullable = true)\n |-- state: string (nullable = true)\n |-- salary: long (nullable = true)\n |-- age: long (nullable = true)\n |-- bonus: long (nullable = true)\n\n+-------------+----------+-----+------+---+-----+\n|employee_name|department|state|salary|age|bonus|\n+-------------+----------+-----+------+---+-----+\n|        James|     Sales|   NY| 90000| 34|10000|\n|      Michael|     Sales|   NY| 86000| 56|20000|\n|       Robert|     Sales|   CA| 81000| 30|23000|\n|        Maria|   Finance|   CA| 90000| 24|23000|\n|        Raman|   Finance|   CA| 99000| 40|24000|\n|        Scott|   Finance|   NY| 83000| 36|19000|\n|          Jen|   Finance|   NY| 79000| 53|15000|\n|         Jeff| Marketing|   CA| 80000| 25|18000|\n|        Kumar| Marketing|   NY| 91000| 50|21000|\n+-------------+----------+-----+------+---+-----+\n\n+----------+-----------+\n|department|sum(salary)|\n+----------+-----------+\n|     Sales|     257000|\n|   Finance|     351000|\n| Marketing|     171000|\n+----------+-----------+\n\n+----------+-----+\n|department|count|\n+----------+-----+\n|     Sales|    3|\n|   Finance|    4|\n| Marketing|    2|\n+----------+-----+\n\n+----------+-----------+\n|department|min(salary)|\n+----------+-----------+\n|     Sales|      81000|\n|   Finance|      79000|\n| Marketing|      80000|\n+----------+-----------+\n\n+----------+-----------+\n|department|max(salary)|\n+----------+-----------+\n|     Sales|      90000|\n|   Finance|      99000|\n| Marketing|      91000|\n+----------+-----------+\n\n+----------+-----------------+\n|department|      avg(salary)|\n+----------+-----------------+\n|     Sales|85666.66666666667|\n|   Finance|          87750.0|\n| Marketing|          85500.0|\n+----------+-----------------+\n\n+----------+-----------------+\n|department|      avg(salary)|\n+----------+-----------------+\n|     Sales|85666.66666666667|\n|   Finance|          87750.0|\n| Marketing|          85500.0|\n+----------+-----------------+\n\n"]}}],"execution_count":0},{"cell_type":"code","source":["3. Using Multiple columns\nSimilarly, we can also run groupBy and aggregate on two or more DataFrame columns, below example does group by on department,state and does sum() on salary and bonus columns.\n//GroupBy on multiple columns\ndf.groupBy(\"department\",\"state\") \\\n    .sum(\"salary\",\"bonus\") \\\n    .show(false)\n\n4. Running more aggregates at a time\nUsing agg() aggregate function we can calculate many aggregations at a time on a single statement using SQL functions sum(), avg(), min(), max() mean() e.t.c. In order to use these, we should import \"from pyspark.sql.functions import sum,avg,max,min,mean,count\"\nfrom pyspark.sql.functions import sum,avg,max\ndf.groupBy(\"department\") \\\n    .agg(sum(\"salary\").alias(\"sum_salary\"), \\\n         avg(\"salary\").alias(\"avg_salary\"), \\\n         sum(\"bonus\").alias(\"sum_bonus\"), \\\n         max(\"bonus\").alias(\"max_bonus\") \\\n     ) \\\n    .show(truncate=False)\nThis example does group on department column and calculates sum() and avg() of salary for each department and calculates sum() and max() of bonus for each department.\n5. Using filter on aggregate data\nSimilar to SQL “HAVING” clause, On PySpark DataFrame we can use either where() or filter() function to filter the rows of aggregated data.\nfrom pyspark.sql.functions import sum,avg,max\ndf.groupBy(\"department\") \\\n    .agg(sum(\"salary\").alias(\"sum_salary\"), \\\n      avg(\"salary\").alias(\"avg_salary\"), \\\n      sum(\"bonus\").alias(\"sum_bonus\"), \\\n      max(\"bonus\").alias(\"max_bonus\")) \\\n    .where(col(\"sum_bonus\") >= 50000) \\\n    .show(truncate=False)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"c1fe2186-ce09-4b9e-8987-fffc309b38b2"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["from pyspark.sql.functions import sum,min,max,avg,col\ndf200.groupBy(\"department\",\"state\").sum(\"salary\",\"bonus\").show(truncate = False)\ndf200.groupBy(\"department\").agg(sum(\"salary\").alias(\"sum\"),\\\n                               avg(\"salary\").alias(\"salary\"),\\\n                                min(\"salary\").alias(\"min\"),\\\n                                max(\"salary\").alias(\"max\")\n                               ).show()\ndf200.groupBy(\"department\").agg(sum(\"salary\").alias(\"sum_salary\"),avg(\"salary\").alias(\"avg_salary\"),\n                                sum(\"bonus\").alias(\"bonus_sum\"),max(\"bonus\").alias(\"max_bonus\")).where(col(\"bonus_sum\")>=50000).show()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"55c7b460-1ee5-476c-8aa6-3c1806b5a997"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)\n\u001B[0;32m<command-478151557241616>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[0;32mfrom\u001B[0m \u001B[0mpyspark\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msql\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mfunctions\u001B[0m \u001B[0;32mimport\u001B[0m \u001B[0msum\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0mmin\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0mmax\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0mavg\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0mcol\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 2\u001B[0;31m \u001B[0mdf200\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mgroupBy\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"department\"\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\"state\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msum\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"salary\"\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\"bonus\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mshow\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mtruncate\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;32mFalse\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      3\u001B[0m df200.groupBy(\"department\").agg(sum(\"salary\").alias(\"sum\"),\\\n\u001B[1;32m      4\u001B[0m                                \u001B[0mavg\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"salary\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0malias\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"salary\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;31m\\\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      5\u001B[0m                                 \u001B[0mmin\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"salary\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0malias\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"min\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;31m\\\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;31mNameError\u001B[0m: name 'df200' is not defined","errorSummary":"<span class='ansi-red-fg'>NameError</span>: name 'df200' is not defined","metadata":{},"errorTraceType":"ansi","type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/plain":["\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)\n\u001B[0;32m<command-478151557241616>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[0;32mfrom\u001B[0m \u001B[0mpyspark\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msql\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mfunctions\u001B[0m \u001B[0;32mimport\u001B[0m \u001B[0msum\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0mmin\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0mmax\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0mavg\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0mcol\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 2\u001B[0;31m \u001B[0mdf200\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mgroupBy\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"department\"\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\"state\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msum\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"salary\"\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\"bonus\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mshow\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mtruncate\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;32mFalse\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      3\u001B[0m df200.groupBy(\"department\").agg(sum(\"salary\").alias(\"sum\"),\\\n\u001B[1;32m      4\u001B[0m                                \u001B[0mavg\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"salary\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0malias\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"salary\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;31m\\\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      5\u001B[0m                                 \u001B[0mmin\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"salary\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0malias\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"min\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;31m\\\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;31mNameError\u001B[0m: name 'df200' is not defined"]}}],"execution_count":0},{"cell_type":"code","source":["1. PySpark Join Syntax\nPySpark SQL join has a below syntax and it can be accessed directly from DataFrame.\njoin(self, other, on=None, how=None)\njoin() operation takes parameters as below and returns DataFrame.\n\nparam other: Right side of the join\nparam on: a string for the join column name\nparam how: default inner. Must be one of inner, cross, outer,full, full_outer, left, left_outer, right, right_outer,left_semi, and left_anti.\nYou can also write Join expression by adding where() and filter() methods on DataFrame and can have Join on multiple columns.\n\n2. PySpark Join Types\nBelow are the different Join Types PySpark supports.\nJoin String\tEquivalent SQL Join\ninner\tINNER JOIN\nouter, full, fullouter, full_outer\tFULL OUTER JOIN\nleft, leftouter, left_outer\tLEFT JOIN\nright, rightouter, right_outer\tRIGHT JOIN\ncross\t\nanti, leftanti, left_anti\t\nsemi, leftsemi, left_semi\t\n\n\nemp = [(1,\"Smith\",-1,\"2018\",\"10\",\"M\",3000), \\\n    (2,\"Rose\",1,\"2010\",\"20\",\"M\",4000), \\\n    (3,\"Williams\",1,\"2010\",\"10\",\"M\",1000), \\\n    (4,\"Jones\",2,\"2005\",\"10\",\"F\",2000), \\\n    (5,\"Brown\",2,\"2010\",\"40\",\"\",-1), \\\n      (6,\"Brown\",2,\"2010\",\"50\",\"\",-1) \\\n  ]\nempColumns = [\"emp_id\",\"name\",\"superior_emp_id\",\"year_joined\", \\\n       \"emp_dept_id\",\"gender\",\"salary\"]\n\nempDF = spark.createDataFrame(data=emp, schema = empColumns)\nempDF.printSchema()\nempDF.show(truncate=False)\n\ndept = [(\"Finance\",10), \\\n    (\"Marketing\",20), \\\n    (\"Sales\",30), \\\n    (\"IT\",40) \\\n  ]\ndeptColumns = [\"dept_name\",\"dept_id\"]\ndeptDF = spark.createDataFrame(data=dept, schema = deptColumns)\ndeptDF.printSchema()\ndeptDF.show(truncate=False)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"7f2d6eac-857b-41d8-8570-2e741b64bdd5"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["import pyspark \nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.functions import col\nemp=[(1,\"smith\",-1,\"2018\",\"10\",\"M\",3000),\\\n     (2,\"Rose\",1,\"2010\",\"20\",\"M\",4000),\\\n     (3,\"Williams\",1,\"2010\",\"10\",\"M\",1000),\\\n     (4,\"Jones\",2,\"2005\",\"10\",\"F\",2000), \\\n    (5,\"Brown\",2,\"2010\",\"40\",\"\",-1), \\\n      (6,\"Brown\",2,\"2010\",\"50\",\"\",-1) \\\n    ]\n\nempcolumns = [\"emp_id\",\"name\",\"superior_emp_id\",\"year_joined\",\"emp_dept_id\",\"gender\",\"salary\"]\nempdf = spark.createDataFrame(data = emp, schema = empcolumns)\n#empdf.printSchema()\nempdf.show()\n\ndept = [(\"Finance\",10), \\\n    (\"Marketing\",20), \\\n    (\"Sales\",30), \\\n    (\"IT\",40)   \n]\n\ndeptcolumns=[\"dept_name\",\"dept_id\"]\ndeptdf = spark.createDataFrame(data = dept,schema=deptcolumns)\ndeptdf.show()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"09afdcbb-6a02-42f0-b3f6-12d046731416"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"+------+--------+---------------+-----------+-----------+------+------+\n|emp_id|    name|superior_emp_id|year_joined|emp_dept_id|gender|salary|\n+------+--------+---------------+-----------+-----------+------+------+\n|     1|   smith|             -1|       2018|         10|     M|  3000|\n|     2|    Rose|              1|       2010|         20|     M|  4000|\n|     3|Williams|              1|       2010|         10|     M|  1000|\n|     4|   Jones|              2|       2005|         10|     F|  2000|\n|     5|   Brown|              2|       2010|         40|      |    -1|\n|     6|   Brown|              2|       2010|         50|      |    -1|\n+------+--------+---------------+-----------+-----------+------+------+\n\n+---------+-------+\n|dept_name|dept_id|\n+---------+-------+\n|  Finance|     10|\n|Marketing|     20|\n|    Sales|     30|\n|       IT|     40|\n+---------+-------+\n\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["+------+--------+---------------+-----------+-----------+------+------+\n|emp_id|    name|superior_emp_id|year_joined|emp_dept_id|gender|salary|\n+------+--------+---------------+-----------+-----------+------+------+\n|     1|   smith|             -1|       2018|         10|     M|  3000|\n|     2|    Rose|              1|       2010|         20|     M|  4000|\n|     3|Williams|              1|       2010|         10|     M|  1000|\n|     4|   Jones|              2|       2005|         10|     F|  2000|\n|     5|   Brown|              2|       2010|         40|      |    -1|\n|     6|   Brown|              2|       2010|         50|      |    -1|\n+------+--------+---------------+-----------+-----------+------+------+\n\n+---------+-------+\n|dept_name|dept_id|\n+---------+-------+\n|  Finance|     10|\n|Marketing|     20|\n|    Sales|     30|\n|       IT|     40|\n+---------+-------+\n\n"]}}],"execution_count":0},{"cell_type":"code","source":["#innerjoin\n#syntax \nempdf.join(deptdf,empdf.emp_dept_id==deptdf.dept_id,\"inner\")\\\n.show()\nempdf.join(deptdf,empdf.emp_dept_id == deptdf.dept_id,\"outer\").show(truncate = False)\nempdf.join(deptdf,empdf.emp_dept_id == deptdf.dept_id,\"full\").show(2)\nempdf.join(deptdf,empdf.emp_dept_id == deptdf.dept_id,\"fullouter\").show()\nempdf.join(deptdf,empdf.emp_dept_id == deptdf.dept_id,\"left\").show()\nempdf.join(deptdf,empdf.emp_dept_id == deptdf.dept_id,\"leftouter\").show()\nempdf.join(deptdf,empdf.emp_dept_id == deptdf.dept_id,\"right\").show()\nempdf.join(deptdf,empdf.emp_dept_id == deptdf.dept_id,\"rightouter\").show()\nempdf.join(deptdf,empdf.emp_dept_id == deptdf.dept_id,\"leftsemi\").show()\nempdf.join(deptdf,empdf.emp_dept_id == deptdf.dept_id,\"leftanti\").select(\"emp_id\",\"name\").show()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"4189fc3a-23a6-4647-9391-4d77b2f72af9"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n|emp_id|    name|superior_emp_id|year_joined|emp_dept_id|gender|salary|dept_name|dept_id|\n+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n|     1|   smith|             -1|       2018|         10|     M|  3000|  Finance|     10|\n|     3|Williams|              1|       2010|         10|     M|  1000|  Finance|     10|\n|     4|   Jones|              2|       2005|         10|     F|  2000|  Finance|     10|\n|     2|    Rose|              1|       2010|         20|     M|  4000|Marketing|     20|\n|     5|   Brown|              2|       2010|         40|      |    -1|       IT|     40|\n+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n\n+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n|emp_id|name    |superior_emp_id|year_joined|emp_dept_id|gender|salary|dept_name|dept_id|\n+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n|1     |smith   |-1             |2018       |10         |M     |3000  |Finance  |10     |\n|3     |Williams|1              |2010       |10         |M     |1000  |Finance  |10     |\n|4     |Jones   |2              |2005       |10         |F     |2000  |Finance  |10     |\n|2     |Rose    |1              |2010       |20         |M     |4000  |Marketing|20     |\n|null  |null    |null           |null       |null       |null  |null  |Sales    |30     |\n|5     |Brown   |2              |2010       |40         |      |-1    |IT       |40     |\n|6     |Brown   |2              |2010       |50         |      |-1    |null     |null   |\n+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n\n+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n|emp_id|    name|superior_emp_id|year_joined|emp_dept_id|gender|salary|dept_name|dept_id|\n+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n|     1|   smith|             -1|       2018|         10|     M|  3000|  Finance|     10|\n|     3|Williams|              1|       2010|         10|     M|  1000|  Finance|     10|\n+------+--------+---------------+-----------+-----------+------+------+---------+-------+\nonly showing top 2 rows\n\n+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n|emp_id|    name|superior_emp_id|year_joined|emp_dept_id|gender|salary|dept_name|dept_id|\n+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n|     1|   smith|             -1|       2018|         10|     M|  3000|  Finance|     10|\n|     3|Williams|              1|       2010|         10|     M|  1000|  Finance|     10|\n|     4|   Jones|              2|       2005|         10|     F|  2000|  Finance|     10|\n|     2|    Rose|              1|       2010|         20|     M|  4000|Marketing|     20|\n|  null|    null|           null|       null|       null|  null|  null|    Sales|     30|\n|     5|   Brown|              2|       2010|         40|      |    -1|       IT|     40|\n|     6|   Brown|              2|       2010|         50|      |    -1|     null|   null|\n+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n\n+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n|emp_id|    name|superior_emp_id|year_joined|emp_dept_id|gender|salary|dept_name|dept_id|\n+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n|     1|   smith|             -1|       2018|         10|     M|  3000|  Finance|     10|\n|     3|Williams|              1|       2010|         10|     M|  1000|  Finance|     10|\n|     4|   Jones|              2|       2005|         10|     F|  2000|  Finance|     10|\n|     2|    Rose|              1|       2010|         20|     M|  4000|Marketing|     20|\n|     5|   Brown|              2|       2010|         40|      |    -1|       IT|     40|\n|     6|   Brown|              2|       2010|         50|      |    -1|     null|   null|\n+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n\n+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n|emp_id|    name|superior_emp_id|year_joined|emp_dept_id|gender|salary|dept_name|dept_id|\n+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n|     1|   smith|             -1|       2018|         10|     M|  3000|  Finance|     10|\n|     3|Williams|              1|       2010|         10|     M|  1000|  Finance|     10|\n|     4|   Jones|              2|       2005|         10|     F|  2000|  Finance|     10|\n|     2|    Rose|              1|       2010|         20|     M|  4000|Marketing|     20|\n|     5|   Brown|              2|       2010|         40|      |    -1|       IT|     40|\n|     6|   Brown|              2|       2010|         50|      |    -1|     null|   null|\n+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n\n+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n|emp_id|    name|superior_emp_id|year_joined|emp_dept_id|gender|salary|dept_name|dept_id|\n+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n|     1|   smith|             -1|       2018|         10|     M|  3000|  Finance|     10|\n|     3|Williams|              1|       2010|         10|     M|  1000|  Finance|     10|\n|     4|   Jones|              2|       2005|         10|     F|  2000|  Finance|     10|\n|     2|    Rose|              1|       2010|         20|     M|  4000|Marketing|     20|\n|  null|    null|           null|       null|       null|  null|  null|    Sales|     30|\n|     5|   Brown|              2|       2010|         40|      |    -1|       IT|     40|\n+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n\n+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n|emp_id|    name|superior_emp_id|year_joined|emp_dept_id|gender|salary|dept_name|dept_id|\n+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n|     1|   smith|             -1|       2018|         10|     M|  3000|  Finance|     10|\n|     3|Williams|              1|       2010|         10|     M|  1000|  Finance|     10|\n|     4|   Jones|              2|       2005|         10|     F|  2000|  Finance|     10|\n|     2|    Rose|              1|       2010|         20|     M|  4000|Marketing|     20|\n|  null|    null|           null|       null|       null|  null|  null|    Sales|     30|\n|     5|   Brown|              2|       2010|         40|      |    -1|       IT|     40|\n+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n\n+------+--------+---------------+-----------+-----------+------+------+\n|emp_id|    name|superior_emp_id|year_joined|emp_dept_id|gender|salary|\n+------+--------+---------------+-----------+-----------+------+------+\n|     1|   smith|             -1|       2018|         10|     M|  3000|\n|     3|Williams|              1|       2010|         10|     M|  1000|\n|     4|   Jones|              2|       2005|         10|     F|  2000|\n|     2|    Rose|              1|       2010|         20|     M|  4000|\n|     5|   Brown|              2|       2010|         40|      |    -1|\n+------+--------+---------------+-----------+-----------+------+------+\n\n+------+-----+\n|emp_id| name|\n+------+-----+\n|     6|Brown|\n+------+-----+\n\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n|emp_id|    name|superior_emp_id|year_joined|emp_dept_id|gender|salary|dept_name|dept_id|\n+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n|     1|   smith|             -1|       2018|         10|     M|  3000|  Finance|     10|\n|     3|Williams|              1|       2010|         10|     M|  1000|  Finance|     10|\n|     4|   Jones|              2|       2005|         10|     F|  2000|  Finance|     10|\n|     2|    Rose|              1|       2010|         20|     M|  4000|Marketing|     20|\n|     5|   Brown|              2|       2010|         40|      |    -1|       IT|     40|\n+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n\n+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n|emp_id|name    |superior_emp_id|year_joined|emp_dept_id|gender|salary|dept_name|dept_id|\n+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n|1     |smith   |-1             |2018       |10         |M     |3000  |Finance  |10     |\n|3     |Williams|1              |2010       |10         |M     |1000  |Finance  |10     |\n|4     |Jones   |2              |2005       |10         |F     |2000  |Finance  |10     |\n|2     |Rose    |1              |2010       |20         |M     |4000  |Marketing|20     |\n|null  |null    |null           |null       |null       |null  |null  |Sales    |30     |\n|5     |Brown   |2              |2010       |40         |      |-1    |IT       |40     |\n|6     |Brown   |2              |2010       |50         |      |-1    |null     |null   |\n+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n\n+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n|emp_id|    name|superior_emp_id|year_joined|emp_dept_id|gender|salary|dept_name|dept_id|\n+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n|     1|   smith|             -1|       2018|         10|     M|  3000|  Finance|     10|\n|     3|Williams|              1|       2010|         10|     M|  1000|  Finance|     10|\n+------+--------+---------------+-----------+-----------+------+------+---------+-------+\nonly showing top 2 rows\n\n+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n|emp_id|    name|superior_emp_id|year_joined|emp_dept_id|gender|salary|dept_name|dept_id|\n+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n|     1|   smith|             -1|       2018|         10|     M|  3000|  Finance|     10|\n|     3|Williams|              1|       2010|         10|     M|  1000|  Finance|     10|\n|     4|   Jones|              2|       2005|         10|     F|  2000|  Finance|     10|\n|     2|    Rose|              1|       2010|         20|     M|  4000|Marketing|     20|\n|  null|    null|           null|       null|       null|  null|  null|    Sales|     30|\n|     5|   Brown|              2|       2010|         40|      |    -1|       IT|     40|\n|     6|   Brown|              2|       2010|         50|      |    -1|     null|   null|\n+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n\n+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n|emp_id|    name|superior_emp_id|year_joined|emp_dept_id|gender|salary|dept_name|dept_id|\n+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n|     1|   smith|             -1|       2018|         10|     M|  3000|  Finance|     10|\n|     3|Williams|              1|       2010|         10|     M|  1000|  Finance|     10|\n|     4|   Jones|              2|       2005|         10|     F|  2000|  Finance|     10|\n|     2|    Rose|              1|       2010|         20|     M|  4000|Marketing|     20|\n|     5|   Brown|              2|       2010|         40|      |    -1|       IT|     40|\n|     6|   Brown|              2|       2010|         50|      |    -1|     null|   null|\n+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n\n+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n|emp_id|    name|superior_emp_id|year_joined|emp_dept_id|gender|salary|dept_name|dept_id|\n+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n|     1|   smith|             -1|       2018|         10|     M|  3000|  Finance|     10|\n|     3|Williams|              1|       2010|         10|     M|  1000|  Finance|     10|\n|     4|   Jones|              2|       2005|         10|     F|  2000|  Finance|     10|\n|     2|    Rose|              1|       2010|         20|     M|  4000|Marketing|     20|\n|     5|   Brown|              2|       2010|         40|      |    -1|       IT|     40|\n|     6|   Brown|              2|       2010|         50|      |    -1|     null|   null|\n+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n\n+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n|emp_id|    name|superior_emp_id|year_joined|emp_dept_id|gender|salary|dept_name|dept_id|\n+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n|     1|   smith|             -1|       2018|         10|     M|  3000|  Finance|     10|\n|     3|Williams|              1|       2010|         10|     M|  1000|  Finance|     10|\n|     4|   Jones|              2|       2005|         10|     F|  2000|  Finance|     10|\n|     2|    Rose|              1|       2010|         20|     M|  4000|Marketing|     20|\n|  null|    null|           null|       null|       null|  null|  null|    Sales|     30|\n|     5|   Brown|              2|       2010|         40|      |    -1|       IT|     40|\n+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n\n+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n|emp_id|    name|superior_emp_id|year_joined|emp_dept_id|gender|salary|dept_name|dept_id|\n+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n|     1|   smith|             -1|       2018|         10|     M|  3000|  Finance|     10|\n|     3|Williams|              1|       2010|         10|     M|  1000|  Finance|     10|\n|     4|   Jones|              2|       2005|         10|     F|  2000|  Finance|     10|\n|     2|    Rose|              1|       2010|         20|     M|  4000|Marketing|     20|\n|  null|    null|           null|       null|       null|  null|  null|    Sales|     30|\n|     5|   Brown|              2|       2010|         40|      |    -1|       IT|     40|\n+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n\n+------+--------+---------------+-----------+-----------+------+------+\n|emp_id|    name|superior_emp_id|year_joined|emp_dept_id|gender|salary|\n+------+--------+---------------+-----------+-----------+------+------+\n|     1|   smith|             -1|       2018|         10|     M|  3000|\n|     3|Williams|              1|       2010|         10|     M|  1000|\n|     4|   Jones|              2|       2005|         10|     F|  2000|\n|     2|    Rose|              1|       2010|         20|     M|  4000|\n|     5|   Brown|              2|       2010|         40|      |    -1|\n+------+--------+---------------+-----------+-----------+------+------+\n\n+------+-----+\n|emp_id| name|\n+------+-----+\n|     6|Brown|\n+------+-----+\n\n"]}}],"execution_count":0},{"cell_type":"code","source":["empdf.join(deptdf,empdf.emp_dept_id == deptdf.dept_id,\"leftanti\").select(\"emp_id\",\"name\").show()\nempdf.join(deptdf,empdf.emp_dept_id == deptdf.dept_id,\"inner\").select(col(\"emp_id\"),col(\"dept_id\"),col(\"dept_name\")).where(col(\"salary\") >1000).show()\nempdf.join(deptdf,empdf.emp_dept_id == deptdf.dept_id,\"inner\").select(empdf.emp_id,deptdf.dept_id).where((deptdf.dept_id == 10)& (empdf.salary>\"1000\")).show()\nempdf.join(deptdf,empdf.emp_dept_id == deptdf.dept_id,\"inner\").select(empdf.emp_id,deptdf.dept_id).where((deptdf.dept_id == 10)& (empdf.salary.cast(\"int\")>\"1000\")).show()\n\ndf = empdf.join(deptdf,empdf.emp_dept_id == deptdf.dept_id,\"inner\").select(empdf.emp_id,deptdf.dept_id).where((deptdf.dept_id == 10)& (empdf.salary.cast(\"int\")>\"1000\"))\ndf.show()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"c02303ed-3b43-4454-8e67-edb4dce155ac"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"+------+-----+\n|emp_id| name|\n+------+-----+\n|     6|Brown|\n+------+-----+\n\n+------+-------+---------+\n|emp_id|dept_id|dept_name|\n+------+-------+---------+\n|     1|     10|  Finance|\n|     4|     10|  Finance|\n|     2|     20|Marketing|\n+------+-------+---------+\n\n+------+-------+\n|emp_id|dept_id|\n+------+-------+\n|     1|     10|\n|     4|     10|\n+------+-------+\n\n+------+-------+\n|emp_id|dept_id|\n+------+-------+\n|     1|     10|\n|     4|     10|\n+------+-------+\n\n+------+-------+\n|emp_id|dept_id|\n+------+-------+\n|     1|     10|\n|     4|     10|\n+------+-------+\n\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["+------+-----+\n|emp_id| name|\n+------+-----+\n|     6|Brown|\n+------+-----+\n\n+------+-------+---------+\n|emp_id|dept_id|dept_name|\n+------+-------+---------+\n|     1|     10|  Finance|\n|     4|     10|  Finance|\n|     2|     20|Marketing|\n+------+-------+---------+\n\n+------+-------+\n|emp_id|dept_id|\n+------+-------+\n|     1|     10|\n|     4|     10|\n+------+-------+\n\n+------+-------+\n|emp_id|dept_id|\n+------+-------+\n|     1|     10|\n|     4|     10|\n+------+-------+\n\n+------+-------+\n|emp_id|dept_id|\n+------+-------+\n|     1|     10|\n|     4|     10|\n+------+-------+\n\n"]}}],"execution_count":0},{"cell_type":"code","source":["#selef join\nempdf.show()\nempdf.alias(\"emp1\").join(empdf.alias(\"emp2\"),col(\"emp1.emp_id\")==col(\"emp2.superior_emp_id\"),\"inner\").show()\ndeptdf.show()\nempdf.alias(\"emp11\").join(empdf.alias(\"emp22\"),col(\"emp11.emp_id\")==col(\"emp22.superior_emp_id\"),\"inner\").select(\"emp11.emp_id\",\"emp11.name\",\"emp22.salary\").show()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"26858375-2bff-4fd8-879d-129d4073c4aa"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"+------+--------+---------------+-----------+-----------+------+------+\n|emp_id|    name|superior_emp_id|year_joined|emp_dept_id|gender|salary|\n+------+--------+---------------+-----------+-----------+------+------+\n|     1|   smith|             -1|       2018|         10|     M|  3000|\n|     2|    Rose|              1|       2010|         20|     M|  4000|\n|     3|Williams|              1|       2010|         10|     M|  1000|\n|     4|   Jones|              2|       2005|         10|     F|  2000|\n|     5|   Brown|              2|       2010|         40|      |    -1|\n|     6|   Brown|              2|       2010|         50|      |    -1|\n+------+--------+---------------+-----------+-----------+------+------+\n\n+------+-----+---------------+-----------+-----------+------+------+------+--------+---------------+-----------+-----------+------+------+\n|emp_id| name|superior_emp_id|year_joined|emp_dept_id|gender|salary|emp_id|    name|superior_emp_id|year_joined|emp_dept_id|gender|salary|\n+------+-----+---------------+-----------+-----------+------+------+------+--------+---------------+-----------+-----------+------+------+\n|     1|smith|             -1|       2018|         10|     M|  3000|     2|    Rose|              1|       2010|         20|     M|  4000|\n|     1|smith|             -1|       2018|         10|     M|  3000|     3|Williams|              1|       2010|         10|     M|  1000|\n|     2| Rose|              1|       2010|         20|     M|  4000|     4|   Jones|              2|       2005|         10|     F|  2000|\n|     2| Rose|              1|       2010|         20|     M|  4000|     5|   Brown|              2|       2010|         40|      |    -1|\n|     2| Rose|              1|       2010|         20|     M|  4000|     6|   Brown|              2|       2010|         50|      |    -1|\n+------+-----+---------------+-----------+-----------+------+------+------+--------+---------------+-----------+-----------+------+------+\n\n+---------+-------+\n|dept_name|dept_id|\n+---------+-------+\n|  Finance|     10|\n|Marketing|     20|\n|    Sales|     30|\n|       IT|     40|\n+---------+-------+\n\n+------+-----+------+\n|emp_id| name|salary|\n+------+-----+------+\n|     1|smith|  4000|\n|     1|smith|  1000|\n|     2| Rose|  2000|\n|     2| Rose|    -1|\n|     2| Rose|    -1|\n+------+-----+------+\n\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["+------+--------+---------------+-----------+-----------+------+------+\n|emp_id|    name|superior_emp_id|year_joined|emp_dept_id|gender|salary|\n+------+--------+---------------+-----------+-----------+------+------+\n|     1|   smith|             -1|       2018|         10|     M|  3000|\n|     2|    Rose|              1|       2010|         20|     M|  4000|\n|     3|Williams|              1|       2010|         10|     M|  1000|\n|     4|   Jones|              2|       2005|         10|     F|  2000|\n|     5|   Brown|              2|       2010|         40|      |    -1|\n|     6|   Brown|              2|       2010|         50|      |    -1|\n+------+--------+---------------+-----------+-----------+------+------+\n\n+------+-----+---------------+-----------+-----------+------+------+------+--------+---------------+-----------+-----------+------+------+\n|emp_id| name|superior_emp_id|year_joined|emp_dept_id|gender|salary|emp_id|    name|superior_emp_id|year_joined|emp_dept_id|gender|salary|\n+------+-----+---------------+-----------+-----------+------+------+------+--------+---------------+-----------+-----------+------+------+\n|     1|smith|             -1|       2018|         10|     M|  3000|     2|    Rose|              1|       2010|         20|     M|  4000|\n|     1|smith|             -1|       2018|         10|     M|  3000|     3|Williams|              1|       2010|         10|     M|  1000|\n|     2| Rose|              1|       2010|         20|     M|  4000|     4|   Jones|              2|       2005|         10|     F|  2000|\n|     2| Rose|              1|       2010|         20|     M|  4000|     5|   Brown|              2|       2010|         40|      |    -1|\n|     2| Rose|              1|       2010|         20|     M|  4000|     6|   Brown|              2|       2010|         50|      |    -1|\n+------+-----+---------------+-----------+-----------+------+------+------+--------+---------------+-----------+-----------+------+------+\n\n+---------+-------+\n|dept_name|dept_id|\n+---------+-------+\n|  Finance|     10|\n|Marketing|     20|\n|    Sales|     30|\n|       IT|     40|\n+---------+-------+\n\n+------+-----+------+\n|emp_id| name|salary|\n+------+-----+------+\n|     1|smith|  4000|\n|     1|smith|  1000|\n|     2| Rose|  2000|\n|     2| Rose|    -1|\n|     2| Rose|    -1|\n+------+-----+------+\n\n"]}}],"execution_count":0},{"cell_type":"code","source":["4. Using SQL Expression\nSince PySpark SQL support native SQL syntax, we can also write join operations after creating temporary tables on DataFrames and use these tables on spark.sql().\n\n\nempDF.createOrReplaceTempView(\"EMP\")\ndeptDF.createOrReplaceTempView(\"DEPT\")\n\njoinDF = spark.sql(\"select * from EMP e, DEPT d where e.emp_dept_id == d.dept_id\") \\\n  .show(truncate=False)\n\njoinDF2 = spark.sql(\"select * from EMP e INNER JOIN DEPT d ON e.emp_dept_id == d.dept_id\") \\\n  .show(truncate=False)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"4da69d0e-a974-469b-bb66-b9351f997dc2"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["empdf.createOrReplaceTempView(\"EMP\")\ndeptdf.createOrReplaceTempView(\"DEP\")\njoinDF = spark.sql(\"select * from EMP e,DEP d where e.emp_dept_id==d.dept_id\").show()\njoinDF2 = spark.sql(\"select * from EMP inner join DEP on EMP.emp_dept_id == DEP.dept_id\").show()\njoinDF2 = spark.sql(\"select * from EMP left join DEP on EMP.emp_dept_id == DEP.dept_id\").show()\njoinDF2 = spark.sql(\"select * from EMP right join DEP on EMP.emp_dept_id == DEP.dept_id\").show()\njoinDF2 = spark.sql(\"select * from EMP left outer join DEP on EMP.emp_dept_id == DEP.dept_id\").show()\njoinDF2 = spark.sql(\"select * from EMP right outer join DEP on EMP.emp_dept_id == DEP.dept_id\").show()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"a1c92e1a-a4e3-49d7-b53f-c10f30026e70"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n|emp_id|    name|superior_emp_id|year_joined|emp_dept_id|gender|salary|dept_name|dept_id|\n+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n|     1|   smith|             -1|       2018|         10|     M|  3000|  Finance|     10|\n|     3|Williams|              1|       2010|         10|     M|  1000|  Finance|     10|\n|     4|   Jones|              2|       2005|         10|     F|  2000|  Finance|     10|\n|     2|    Rose|              1|       2010|         20|     M|  4000|Marketing|     20|\n|     5|   Brown|              2|       2010|         40|      |    -1|       IT|     40|\n+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n\n+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n|emp_id|    name|superior_emp_id|year_joined|emp_dept_id|gender|salary|dept_name|dept_id|\n+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n|     1|   smith|             -1|       2018|         10|     M|  3000|  Finance|     10|\n|     3|Williams|              1|       2010|         10|     M|  1000|  Finance|     10|\n|     4|   Jones|              2|       2005|         10|     F|  2000|  Finance|     10|\n|     2|    Rose|              1|       2010|         20|     M|  4000|Marketing|     20|\n|     5|   Brown|              2|       2010|         40|      |    -1|       IT|     40|\n+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n\n+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n|emp_id|    name|superior_emp_id|year_joined|emp_dept_id|gender|salary|dept_name|dept_id|\n+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n|     1|   smith|             -1|       2018|         10|     M|  3000|  Finance|     10|\n|     3|Williams|              1|       2010|         10|     M|  1000|  Finance|     10|\n|     4|   Jones|              2|       2005|         10|     F|  2000|  Finance|     10|\n|     2|    Rose|              1|       2010|         20|     M|  4000|Marketing|     20|\n|     5|   Brown|              2|       2010|         40|      |    -1|       IT|     40|\n|     6|   Brown|              2|       2010|         50|      |    -1|     null|   null|\n+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n\n+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n|emp_id|    name|superior_emp_id|year_joined|emp_dept_id|gender|salary|dept_name|dept_id|\n+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n|     1|   smith|             -1|       2018|         10|     M|  3000|  Finance|     10|\n|     3|Williams|              1|       2010|         10|     M|  1000|  Finance|     10|\n|     4|   Jones|              2|       2005|         10|     F|  2000|  Finance|     10|\n|     2|    Rose|              1|       2010|         20|     M|  4000|Marketing|     20|\n|  null|    null|           null|       null|       null|  null|  null|    Sales|     30|\n|     5|   Brown|              2|       2010|         40|      |    -1|       IT|     40|\n+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n\n+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n|emp_id|    name|superior_emp_id|year_joined|emp_dept_id|gender|salary|dept_name|dept_id|\n+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n|     1|   smith|             -1|       2018|         10|     M|  3000|  Finance|     10|\n|     3|Williams|              1|       2010|         10|     M|  1000|  Finance|     10|\n|     4|   Jones|              2|       2005|         10|     F|  2000|  Finance|     10|\n|     2|    Rose|              1|       2010|         20|     M|  4000|Marketing|     20|\n|     5|   Brown|              2|       2010|         40|      |    -1|       IT|     40|\n|     6|   Brown|              2|       2010|         50|      |    -1|     null|   null|\n+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n\n+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n|emp_id|    name|superior_emp_id|year_joined|emp_dept_id|gender|salary|dept_name|dept_id|\n+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n|     1|   smith|             -1|       2018|         10|     M|  3000|  Finance|     10|\n|     3|Williams|              1|       2010|         10|     M|  1000|  Finance|     10|\n|     4|   Jones|              2|       2005|         10|     F|  2000|  Finance|     10|\n|     2|    Rose|              1|       2010|         20|     M|  4000|Marketing|     20|\n|  null|    null|           null|       null|       null|  null|  null|    Sales|     30|\n|     5|   Brown|              2|       2010|         40|      |    -1|       IT|     40|\n+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n|emp_id|    name|superior_emp_id|year_joined|emp_dept_id|gender|salary|dept_name|dept_id|\n+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n|     1|   smith|             -1|       2018|         10|     M|  3000|  Finance|     10|\n|     3|Williams|              1|       2010|         10|     M|  1000|  Finance|     10|\n|     4|   Jones|              2|       2005|         10|     F|  2000|  Finance|     10|\n|     2|    Rose|              1|       2010|         20|     M|  4000|Marketing|     20|\n|     5|   Brown|              2|       2010|         40|      |    -1|       IT|     40|\n+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n\n+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n|emp_id|    name|superior_emp_id|year_joined|emp_dept_id|gender|salary|dept_name|dept_id|\n+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n|     1|   smith|             -1|       2018|         10|     M|  3000|  Finance|     10|\n|     3|Williams|              1|       2010|         10|     M|  1000|  Finance|     10|\n|     4|   Jones|              2|       2005|         10|     F|  2000|  Finance|     10|\n|     2|    Rose|              1|       2010|         20|     M|  4000|Marketing|     20|\n|     5|   Brown|              2|       2010|         40|      |    -1|       IT|     40|\n+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n\n+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n|emp_id|    name|superior_emp_id|year_joined|emp_dept_id|gender|salary|dept_name|dept_id|\n+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n|     1|   smith|             -1|       2018|         10|     M|  3000|  Finance|     10|\n|     3|Williams|              1|       2010|         10|     M|  1000|  Finance|     10|\n|     4|   Jones|              2|       2005|         10|     F|  2000|  Finance|     10|\n|     2|    Rose|              1|       2010|         20|     M|  4000|Marketing|     20|\n|     5|   Brown|              2|       2010|         40|      |    -1|       IT|     40|\n|     6|   Brown|              2|       2010|         50|      |    -1|     null|   null|\n+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n\n+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n|emp_id|    name|superior_emp_id|year_joined|emp_dept_id|gender|salary|dept_name|dept_id|\n+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n|     1|   smith|             -1|       2018|         10|     M|  3000|  Finance|     10|\n|     3|Williams|              1|       2010|         10|     M|  1000|  Finance|     10|\n|     4|   Jones|              2|       2005|         10|     F|  2000|  Finance|     10|\n|     2|    Rose|              1|       2010|         20|     M|  4000|Marketing|     20|\n|  null|    null|           null|       null|       null|  null|  null|    Sales|     30|\n|     5|   Brown|              2|       2010|         40|      |    -1|       IT|     40|\n+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n\n+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n|emp_id|    name|superior_emp_id|year_joined|emp_dept_id|gender|salary|dept_name|dept_id|\n+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n|     1|   smith|             -1|       2018|         10|     M|  3000|  Finance|     10|\n|     3|Williams|              1|       2010|         10|     M|  1000|  Finance|     10|\n|     4|   Jones|              2|       2005|         10|     F|  2000|  Finance|     10|\n|     2|    Rose|              1|       2010|         20|     M|  4000|Marketing|     20|\n|     5|   Brown|              2|       2010|         40|      |    -1|       IT|     40|\n|     6|   Brown|              2|       2010|         50|      |    -1|     null|   null|\n+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n\n+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n|emp_id|    name|superior_emp_id|year_joined|emp_dept_id|gender|salary|dept_name|dept_id|\n+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n|     1|   smith|             -1|       2018|         10|     M|  3000|  Finance|     10|\n|     3|Williams|              1|       2010|         10|     M|  1000|  Finance|     10|\n|     4|   Jones|              2|       2005|         10|     F|  2000|  Finance|     10|\n|     2|    Rose|              1|       2010|         20|     M|  4000|Marketing|     20|\n|  null|    null|           null|       null|       null|  null|  null|    Sales|     30|\n|     5|   Brown|              2|       2010|         40|      |    -1|       IT|     40|\n+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n\n"]}}],"execution_count":0},{"cell_type":"code","source":["5. PySpark SQL Join on multiple DataFrames\nWhen you need to join more than two tables, you either use SQL expression after creating a temporary view on the DataFrame or use the result of join operation to join with another DataFrame like chaining them. for example\n\n\ndf1.join(df2,df1.id1 == df2.id2,\"inner\") \\\n   .join(df3,df1.id1 == df3.id3,\"inner\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"6adb5820-bb4b-40b6-87f1-c3b17f20dd6f"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["df1.join(df2,df1.id == df2.id,\"inner\").join(df3,df1.id==df2.id,\"inner\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"65b63b52-21e6-479e-a3ca-369710d98931"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["PySpark Union and UnionAll Explained\nDataframe union() – union() method of the DataFrame is used to merge two DataFrame’s of the same structure/schema. If schemas are not the same it returns an error.\nDataFrame unionAll() – unionAll() is deprecated since Spark “2.0.0” version and replaced with union().\n"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"c20b259b-6155-47e0-8e04-cb247e9c3915"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["import pyspark\nfrom pyspark.sql import SparkSession\nspark = SparkSession.builder.appName(\"SparkByexample.com\").getOrCreate()\nsimpledata = [(\"james\",\"sales\",\"NY\",90000,34,10000),\\\n              (\"Michael\",\"Sales\",\"NY\",86000,56,20000), \\\n    (\"Robert\",\"Sales\",\"CA\",81000,30,23000), \\\n    (\"Maria\",\"Finance\",\"CA\",90000,24,23000)    \n]\ncolumns = [\"employee_name\",\"department\",\"state\",\"salary\",\"age\",\"bonus\"]\ndf300 = spark.createDataFrame(data = simpledata, schema = columns)\n\nsimpleData2 = [(\"James\",\"Sales\",\"NY\",90000,34,10000), \\\n    (\"Maria\",\"Finance\",\"CA\",90000,24,23000), \\\n    (\"Jen\",\"Finance\",\"NY\",79000,53,15000), \\\n    (\"Jeff\",\"Marketing\",\"CA\",80000,25,18000), \\\n    (\"Kumar\",\"Marketing\",\"NY\",91000,50,21000) \\\n  ]\ncolumn2= [\"employee_name\",\"department\",\"state\",\"salary\",\"age\",\"bonus\"]\ndf301 = spark.createDataFrame(data = simpleData2 , schema = column2)\nunionDF = df300.union(df301)\nunionDF.show()\n\nunionDF2 = df300.unionAll(df301)\nunionDF.show()\nunionDF2.count().cast('str').show()\n\n\n\n\n"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"8b23ac24-6107-458b-a22b-23af3c909ff9"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"+-------------+----------+-----+------+---+-----+\n|employee_name|department|state|salary|age|bonus|\n+-------------+----------+-----+------+---+-----+\n|        james|     sales|   NY| 90000| 34|10000|\n|      Michael|     Sales|   NY| 86000| 56|20000|\n|       Robert|     Sales|   CA| 81000| 30|23000|\n|        Maria|   Finance|   CA| 90000| 24|23000|\n|        James|     Sales|   NY| 90000| 34|10000|\n|        Maria|   Finance|   CA| 90000| 24|23000|\n|          Jen|   Finance|   NY| 79000| 53|15000|\n|         Jeff| Marketing|   CA| 80000| 25|18000|\n|        Kumar| Marketing|   NY| 91000| 50|21000|\n+-------------+----------+-----+------+---+-----+\n\n+-------------+----------+-----+------+---+-----+\n|employee_name|department|state|salary|age|bonus|\n+-------------+----------+-----+------+---+-----+\n|        james|     sales|   NY| 90000| 34|10000|\n|      Michael|     Sales|   NY| 86000| 56|20000|\n|       Robert|     Sales|   CA| 81000| 30|23000|\n|        Maria|   Finance|   CA| 90000| 24|23000|\n|        James|     Sales|   NY| 90000| 34|10000|\n|        Maria|   Finance|   CA| 90000| 24|23000|\n|          Jen|   Finance|   NY| 79000| 53|15000|\n|         Jeff| Marketing|   CA| 80000| 25|18000|\n|        Kumar| Marketing|   NY| 91000| 50|21000|\n+-------------+----------+-----+------+---+-----+\n\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["+-------------+----------+-----+------+---+-----+\n|employee_name|department|state|salary|age|bonus|\n+-------------+----------+-----+------+---+-----+\n|        james|     sales|   NY| 90000| 34|10000|\n|      Michael|     Sales|   NY| 86000| 56|20000|\n|       Robert|     Sales|   CA| 81000| 30|23000|\n|        Maria|   Finance|   CA| 90000| 24|23000|\n|        James|     Sales|   NY| 90000| 34|10000|\n|        Maria|   Finance|   CA| 90000| 24|23000|\n|          Jen|   Finance|   NY| 79000| 53|15000|\n|         Jeff| Marketing|   CA| 80000| 25|18000|\n|        Kumar| Marketing|   NY| 91000| 50|21000|\n+-------------+----------+-----+------+---+-----+\n\n+-------------+----------+-----+------+---+-----+\n|employee_name|department|state|salary|age|bonus|\n+-------------+----------+-----+------+---+-----+\n|        james|     sales|   NY| 90000| 34|10000|\n|      Michael|     Sales|   NY| 86000| 56|20000|\n|       Robert|     Sales|   CA| 81000| 30|23000|\n|        Maria|   Finance|   CA| 90000| 24|23000|\n|        James|     Sales|   NY| 90000| 34|10000|\n|        Maria|   Finance|   CA| 90000| 24|23000|\n|          Jen|   Finance|   NY| 79000| 53|15000|\n|         Jeff| Marketing|   CA| 80000| 25|18000|\n|        Kumar| Marketing|   NY| 91000| 50|21000|\n+-------------+----------+-----+------+---+-----+\n\n"]}},{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n\u001B[0;31mAttributeError\u001B[0m                            Traceback (most recent call last)\n\u001B[0;32m<command-821191946760139>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m     23\u001B[0m \u001B[0munionDF2\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mdf300\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0munionAll\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mdf301\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     24\u001B[0m \u001B[0munionDF\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mshow\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 25\u001B[0;31m \u001B[0munionDF2\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mcount\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mcast\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m'str'\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mshow\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     26\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     27\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;31mAttributeError\u001B[0m: 'int' object has no attribute 'cast'","errorSummary":"<span class='ansi-red-fg'>AttributeError</span>: 'int' object has no attribute 'cast'","metadata":{},"errorTraceType":"ansi","type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/plain":["\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n\u001B[0;31mAttributeError\u001B[0m                            Traceback (most recent call last)\n\u001B[0;32m<command-821191946760139>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m     23\u001B[0m \u001B[0munionDF2\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mdf300\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0munionAll\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mdf301\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     24\u001B[0m \u001B[0munionDF\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mshow\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 25\u001B[0;31m \u001B[0munionDF2\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mcount\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mcast\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m'str'\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mshow\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     26\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     27\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;31mAttributeError\u001B[0m: 'int' object has no attribute 'cast'"]}}],"execution_count":0},{"cell_type":"code","source":["PySpark Merge Two DataFrames with Different Columns\nIn PySpark to merge two DataFrames with different columns, will use the similar approach explain above and uses unionByName() transformation. First let’s create DataFrame’s with different number of columns.\n#Create DataFrame df1 with columns name,dept & age\ndata = [(\"James\",\"Sales\",34), (\"Michael\",\"Sales\",56), \\\n    (\"Robert\",\"Sales\",30), (\"Maria\",\"Finance\",24) ]\ncolumns= [\"name\",\"dept\",\"age\"]\ndf1 = spark.createDataFrame(data = data, schema = columns)\ndf1.printSchema()\n\n#Create DataFrame df1 with columns name,dep,state & salary\ndata2=[(\"James\",\"Sales\",\"NY\",9000),(\"Maria\",\"Finance\",\"CA\",9000), \\\n    (\"Jen\",\"Finance\",\"NY\",7900),(\"Jeff\",\"Marketing\",\"CA\",8000)]\ncolumns2= [\"name\",\"dept\",\"state\",\"salary\"]\ndf2 = spark.createDataFrame(data = data2, schema = columns2)\ndf2.printSchema()\n\n\n#Add missing columns 'state' & 'salary' to df1\nfrom pyspark.sql.functions import lit\nfor column in [column for column in df2.columns if column not in df1.columns]:\n    df1 = df1.withColumn(column, lit(None))\n\n#Add missing column 'age' to df2\nfor column in [column for column in df1.columns if column not in df2.columns]:\n    df2 = df2.withColumn(column, lit(None))\n    \n#Finally join two dataframe's df1 & df2 by name\nmerged_df=df1.unionByName(df2)\nmerged_df.show()\n"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"5e01fd0d-18ce-41ee-b671-01fe4454e0fd"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["from pyspark.sql.functions import lit\ndata1 = [(\"James\",\"Sales\",34), (\"Michael\",\"Sales\",56), \\\n    (\"Robert\",\"Sales\",30), (\"Maria\",\"Finance\",24) ]\ncolumns1= [\"name\",\"dept\",\"age\"]\ndf1 = spark.createDataFrame(data = data1, schema = columns1)\ndf1.show()\n\n#Create DataFrame df1 with columns name,dep,state & salary\ndata2=[(\"James\",\"Sales\",\"NY\",9000),(\"Maria\",\"Finance\",\"CA\",9000), \\\n    (\"Jen\",\"Finance\",\"NY\",7900),(\"Jeff\",\"Marketing\",\"CA\",8000)]\ncolumns2= [\"name\",\"dept\",\"state\",\"salary\"]\ndf2 = spark.createDataFrame(data = data2, schema = columns2)\ndf2.show()\n\nfor column in [column for column in df2.columns if column not in df1.columns]:\n    df1=df1.withColumn(column,lit(None))\nfor column in [column for column in df1.columns if column not in df2.columns]:\n    df2=df2.withColumn(column,lit(None))\n    \nmerged_df = df1.unionByName(df2)\nmerged_df.show()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"d7b54468-a765-4575-87e7-b73b118b80b7"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"+-------+-------+---+\n|   name|   dept|age|\n+-------+-------+---+\n|  James|  Sales| 34|\n|Michael|  Sales| 56|\n| Robert|  Sales| 30|\n|  Maria|Finance| 24|\n+-------+-------+---+\n\n+-----+---------+-----+------+\n| name|     dept|state|salary|\n+-----+---------+-----+------+\n|James|    Sales|   NY|  9000|\n|Maria|  Finance|   CA|  9000|\n|  Jen|  Finance|   NY|  7900|\n| Jeff|Marketing|   CA|  8000|\n+-----+---------+-----+------+\n\n+-------+---------+----+-----+------+\n|   name|     dept| age|state|salary|\n+-------+---------+----+-----+------+\n|  James|    Sales|  34| null|  null|\n|Michael|    Sales|  56| null|  null|\n| Robert|    Sales|  30| null|  null|\n|  Maria|  Finance|  24| null|  null|\n|  James|    Sales|null|   NY|  9000|\n|  Maria|  Finance|null|   CA|  9000|\n|    Jen|  Finance|null|   NY|  7900|\n|   Jeff|Marketing|null|   CA|  8000|\n+-------+---------+----+-----+------+\n\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["+-------+-------+---+\n|   name|   dept|age|\n+-------+-------+---+\n|  James|  Sales| 34|\n|Michael|  Sales| 56|\n| Robert|  Sales| 30|\n|  Maria|Finance| 24|\n+-------+-------+---+\n\n+-----+---------+-----+------+\n| name|     dept|state|salary|\n+-----+---------+-----+------+\n|James|    Sales|   NY|  9000|\n|Maria|  Finance|   CA|  9000|\n|  Jen|  Finance|   NY|  7900|\n| Jeff|Marketing|   CA|  8000|\n+-----+---------+-----+------+\n\n+-------+---------+----+-----+------+\n|   name|     dept| age|state|salary|\n+-------+---------+----+-----+------+\n|  James|    Sales|  34| null|  null|\n|Michael|    Sales|  56| null|  null|\n| Robert|    Sales|  30| null|  null|\n|  Maria|  Finance|  24| null|  null|\n|  James|    Sales|null|   NY|  9000|\n|  Maria|  Finance|null|   CA|  9000|\n|    Jen|  Finance|null|   NY|  7900|\n|   Jeff|Marketing|null|   CA|  8000|\n+-------+---------+----+-----+------+\n\n"]}}],"execution_count":0},{"cell_type":"code","source":["PySpark map() Transformation\nPySpark map (map()) is an RDD transformation that is used to apply the transformation function (lambda) on every element of RDD/DataFrame and returns a new RDD. In this article, you will learn the syntax and usage of the RDD map() transformation with an example and how to use it with DataFrame.\n\nRDD map() transformation is used to apply any complex operations like adding a column, updating a column, transforming the data e.t.c, the output of map transformations would always have the same number of records as input.\nNote1: DataFrame doesn’t have map() transformation to use with DataFrame hence you need to DataFrame to RDD first.\nNote2: If you have a heavy initialization use PySpark mapPartitions() transformation instead of map(), as with mapPartitions() heavy initialization executes only once for each partition instead of every record.\nFirst, let’s create an RDD from the list.\n\nfrom pyspark.sql import SparkSession\nspark = SparkSession.builder.master(\"local[1]\") \\\n    .appName(\"SparkByExamples.com\").getOrCreate()\n\ndata = [\"Project\",\"Gutenberg’s\",\"Alice’s\",\"Adventures\",\n\"in\",\"Wonderland\",\"Project\",\"Gutenberg’s\",\"Adventures\",\n\"in\",\"Wonderland\",\"Project\",\"Gutenberg’s\"]\n\nrdd=spark.sparkContext.parallelize(data)\nmap() Syntax\n\nmap(f, preservesPartitioning=False)\nPySpark map() Example with RDD\nIn this PySpark map() example, we are adding a new element with value 1 for each element, the result of the RDD is PairRDDFunctions which contains key-value pairs, word of type String as Key and 1 of type Int as value.\n\nrdd2=rdd.map(lambda x: (x,1))\nfor element in rdd2.collect():\n    print(element)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"c2cc5c73-c8fc-4e80-9334-ea57294aded8"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["import pyspark\nfrom pyspark.sql import SparkSession\nspark = SparkSession.builder.master(\"local[1]\").appName(\"fhdf\").getOrCreate()\ndata = [\"Project\",\"Gutenberg\",\"Alice\",\"Adventures\",\n\"in\",\"Wonderland\",\"Project\",\"Gutenberg’s\",\"Adventures\",\n\"in\",\"Wonderland\",\"Project\",\"Gutenberg’s\"]\nrdd = spark.sparkContext.parallelize(data)\nrdd2 = rdd.map(lambda x: (x,2))\nfor element in rdd2.collect():print(element)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"17c982fc-8c1b-45b5-9879-3af1e002bd70"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"('Project', 2)\n('Gutenberg', 2)\n('Alice', 2)\n('Adventures', 2)\n('in', 2)\n('Wonderland', 2)\n('Project', 2)\n('Gutenberg’s', 2)\n('Adventures', 2)\n('in', 2)\n('Wonderland', 2)\n('Project', 2)\n('Gutenberg’s', 2)\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["('Project', 2)\n('Gutenberg', 2)\n('Alice', 2)\n('Adventures', 2)\n('in', 2)\n('Wonderland', 2)\n('Project', 2)\n('Gutenberg’s', 2)\n('Adventures', 2)\n('in', 2)\n('Wonderland', 2)\n('Project', 2)\n('Gutenberg’s', 2)\n"]}}],"execution_count":0},{"cell_type":"code","source":["PySpark map() Example with DataFrame\nPySpark DataFrame doesn’t have map() transformation to apply the lambda function, when you wanted to apply the custom transformation, you need to convert the DataFrame to RDD and apply the map() transformation. Let’s use another dataset to explain this.\ndata301 = [('James','Smith','M',30),\n  ('Anna','Rose','F',41),\n  ('Robert','Williams','M',62), \n]\ncolumns301 = [\"firstname\",\"lastname\",\"gender\",\"salary\"]\n\ndf302 = spark.createDataFrame(data = data301,schema = columns301)\ndf302.show()\n# Refering columns by index.\nrdd2=df302.rdd.map(lambda x: \n    (x[0]+\",\"+x[1],x[2],x[3]*2)\n    )  \ndf2=rdd2.toDF([\"name\",\"gender\",\"new_salary\"]   )\ndf2.show()\n#refering columns names\nrdd2=df302.rdd.map(lambda x:(x.firstname+\",\"+x.lastname,x.gender,x.salary*2))\n\nYou can also create a custom function to perform an operation. Below func1() function executes for every DataFrame row from the lambda function.\n\n# By Calling function\ndef func1(x):\n    firstName=x.firstname\n    lastName=x.lastname\n    name=firstName+\",\"+lastName\n    gender=x.gender.lower()\n    salary=x.salary*2\n    return (name,gender,salary)\n\nrdd2=df.rdd.map(lambda x: func1(x))\n\n"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"5aee9ba5-db1f-4d8a-8f4c-2f4a3fe1e21e"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"\u001B[0;36m  File \u001B[0;32m\"<command-3118337423178868>\"\u001B[0;36m, line \u001B[0;32m1\u001B[0m\n\u001B[0;31m    PySpark map() Example with DataFrame\u001B[0m\n\u001B[0m            ^\u001B[0m\n\u001B[0;31mSyntaxError\u001B[0m\u001B[0;31m:\u001B[0m invalid syntax\n","errorSummary":"<span class='ansi-red-fg'>SyntaxError</span>: invalid syntax (<command-3118337423178868>, line 1)","metadata":{},"errorTraceType":"ansi","type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/plain":["\u001B[0;36m  File \u001B[0;32m\"<command-3118337423178868>\"\u001B[0;36m, line \u001B[0;32m1\u001B[0m\n\u001B[0;31m    PySpark map() Example with DataFrame\u001B[0m\n\u001B[0m            ^\u001B[0m\n\u001B[0;31mSyntaxError\u001B[0m\u001B[0;31m:\u001B[0m invalid syntax\n"]}}],"execution_count":0},{"cell_type":"code","source":["data301 = [('James','Smith','M',30),\n  ('Anna','Rose','F',41),\n  ('Robert','Williams','M',62), \n]\n\ncolumns301 = [\"firstname\",\"lastname\",\"gender\",\"salary\"]\n\ndf302 = spark.createDataFrame(data = data301,schema = columns301)\ndf302.show()\nrdd = spark.sparkContext.parallelize(data301)\nrdd3=df302.rdd.map(lambda x: (x[0]+\",\"+x[1],x[2],x[3]*2))\ndf303 = rdd3.toDF([\"name\",\"gender\",\"salary\"])\ndf303.show()\nrdd4=df302.rdd.map(lambda x:(x.firstname+\",\"+x.lastname,x.gender,x.salary*2))\ndf304 = rdd4.toDF([\"name\",\"gender\",\"salary\"])\ndf304.show()\nrdd5 = df302.rdd.map(lambda x:(x[\"firstname\"]+\",\"+x[\"lastname\"],x[\"gender\"],x[\"salary\"]*2))\ndf305 = rdd5.toDF([\"name\",\"gender\",\"salary\"])\ndf305.show()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"e29cce80-be06-4898-92db-830a70862b1c"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"+---------+--------+------+------+\n|firstname|lastname|gender|salary|\n+---------+--------+------+------+\n|    James|   Smith|     M|    30|\n|     Anna|    Rose|     F|    41|\n|   Robert|Williams|     M|    62|\n+---------+--------+------+------+\n\n+---------------+------+------+\n|           name|gender|salary|\n+---------------+------+------+\n|    James,Smith|     M|    60|\n|      Anna,Rose|     F|    82|\n|Robert,Williams|     M|   124|\n+---------------+------+------+\n\n+---------------+------+------+\n|           name|gender|salary|\n+---------------+------+------+\n|    James,Smith|     M|    60|\n|      Anna,Rose|     F|    82|\n|Robert,Williams|     M|   124|\n+---------------+------+------+\n\n+---------------+------+------+\n|           name|gender|salary|\n+---------------+------+------+\n|    James,Smith|     M|    60|\n|      Anna,Rose|     F|    82|\n|Robert,Williams|     M|   124|\n+---------------+------+------+\n\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["+---------+--------+------+------+\n|firstname|lastname|gender|salary|\n+---------+--------+------+------+\n|    James|   Smith|     M|    30|\n|     Anna|    Rose|     F|    41|\n|   Robert|Williams|     M|    62|\n+---------+--------+------+------+\n\n+---------------+------+------+\n|           name|gender|salary|\n+---------------+------+------+\n|    James,Smith|     M|    60|\n|      Anna,Rose|     F|    82|\n|Robert,Williams|     M|   124|\n+---------------+------+------+\n\n+---------------+------+------+\n|           name|gender|salary|\n+---------------+------+------+\n|    James,Smith|     M|    60|\n|      Anna,Rose|     F|    82|\n|Robert,Williams|     M|   124|\n+---------------+------+------+\n\n+---------------+------+------+\n|           name|gender|salary|\n+---------------+------+------+\n|    James,Smith|     M|    60|\n|      Anna,Rose|     F|    82|\n|Robert,Williams|     M|   124|\n+---------------+------+------+\n\n"]}}],"execution_count":0},{"cell_type":"code","source":["def func1(x):\n    firstName = x.firstname\n    lastName = x.lastname\n    name = firstName+\",\"+lastName\n    gender = x.gender.lower()\n    salary = x.salary*2\n    return(name,gender,salary)\nrdd6 = df302.rdd.map(lambda x: func1(x))\ndf306 = rdd6.toDF([\"name\",\"gender\",\"salary\"])\ndf306.show()\n\n"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"2a7a6fd3-c170-4b78-987e-778b843d2170"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"+---------------+------+------+\n|           name|gender|salary|\n+---------------+------+------+\n|    James,Smith|     m|    60|\n|      Anna,Rose|     f|    82|\n|Robert,Williams|     m|   124|\n+---------------+------+------+\n\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["+---------------+------+------+\n|           name|gender|salary|\n+---------------+------+------+\n|    James,Smith|     m|    60|\n|      Anna,Rose|     f|    82|\n|Robert,Williams|     m|   124|\n+---------------+------+------+\n\n"]}}],"execution_count":0},{"cell_type":"code","source":["PySpark flatMap() Transformation\nPySpark flatMap() is a transformation operation that flattens the RDD/DataFrame (array/map DataFrame columns) after applying the function on every element and returns a new PySpark RDD/DataFrame. In this article, you will learn the syntax and usage of the PySpark flatMap() with an example.\ndata = [\"Project Gutenberg’s\",\n        \"Alice’s Adventures in Wonderland\",\n        \"Project Gutenberg’s\",\n        \"Adventures in Wonderland\",\n        \"Project Gutenberg’s\"]\nrdd=spark.sparkContext.parallelize(data)\nfor element in rdd.collect():\n    print(element)\n    \n    flatMap() Example\nNow, let’s see with an example of how to apply a flatMap() transformation on RDD. In the below example, first, it splits each record by space in an RDD and finally flattens it. Resulting RDD consists of a single word on each record.\nrdd2=rdd.flatMap(lambda x: x.split(\" \"))\nfor element in rdd2.collect():\n    print(element)\n\n"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"8f339569-568c-436d-ad5e-a9833868088a"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["data20 = [\"Project Gutenberg’s\",\n        \"Alice’s Adventures in Wonderland\",\n        \"Project Gutenberg’s\",\n        \"Adventures in Wonderland\",\n        \"Project Gutenberg’s\"]\nrdd10=spark.sparkContext.parallelize(data20)\nfor i in rdd10.collect():print(i)\n\nrdd11 =rdd10.flatMap(lambda x:x.split(\" \"))\nfor j in rdd11.collect():print(j)\n\n"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"a6b67884-d51e-4a5f-bf4a-2ab1d54fc853"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"Project Gutenberg’s\nAlice’s Adventures in Wonderland\nProject Gutenberg’s\nAdventures in Wonderland\nProject Gutenberg’s\nProject\nGutenberg’s\nAlice’s\nAdventures\nin\nWonderland\nProject\nGutenberg’s\nAdventures\nin\nWonderland\nProject\nGutenberg’s\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["Project Gutenberg’s\nAlice’s Adventures in Wonderland\nProject Gutenberg’s\nAdventures in Wonderland\nProject Gutenberg’s\nProject\nGutenberg’s\nAlice’s\nAdventures\nin\nWonderland\nProject\nGutenberg’s\nAdventures\nin\nWonderland\nProject\nGutenberg’s\n"]}}],"execution_count":0},{"cell_type":"code","source":["Using flatMap() transformation on DataFrame\nUnfortunately, PySpark DataFame doesn’t have flatMap() transformation however, DataFrame has explode() SQL function that is used to flatten the column. Below is a complete example.\n\nimport pyspark\nfrom pyspark.sql import SparkSession\nspark = SparkSession.builder.appName('pyspark-by-examples').getOrCreate()\n\narrayData = [\n        ('James',['Java','Scala'],{'hair':'black','eye':'brown'}),\n        ('Michael',['Spark','Java',None],{'hair':'brown','eye':None}),\n        ('Robert',['CSharp',''],{'hair':'red','eye':''}),\n        ('Washington',None,None),\n        ('Jefferson',['1','2'],{})]\ndf = spark.createDataFrame(data=arrayData, schema = ['name','knownLanguages','properties'])\n\nfrom pyspark.sql.functions import explode\ndf2 = df.select(df.name,explode(df.knownLanguages))\ndf2.printSchema()\ndf2.show()\n"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"c3bd8d5b-f929-43cc-9593-e8a37758bd6a"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["import pyspark\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.functions import explode\nspark = SparkSession.builder.master(\"krishnareddy\").appName(\"pyspark.com\").getOrCreate()\narraydata = [('james',['java','scala'],{'hair':'black','eye':'brown'}),\n        ('Michael',['Spark','Java',None],{'hair':'brown','eye':None}),\n        ('Robert',['CSharp',''],{'hair':'red','eye':''}),\n        ('Washington',None,None),\n        ('Jefferson',['1','2'],{})]\ndf22 = spark.createDataFrame(data= arraydata ,schema = ['name','knownlanguage','properties'])\ndf21 = df22.select(df22.name,explode(df22.properties))\ndf21.printSchema()\ndf21.show()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"c294138c-807e-4a1f-bec8-33c04bd57546"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"root\n |-- name: string (nullable = true)\n |-- key: string (nullable = false)\n |-- value: string (nullable = true)\n\n+-------+----+-----+\n|   name| key|value|\n+-------+----+-----+\n|  james| eye|brown|\n|  james|hair|black|\n|Michael| eye| null|\n|Michael|hair|brown|\n| Robert| eye|     |\n| Robert|hair|  red|\n+-------+----+-----+\n\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["root\n |-- name: string (nullable = true)\n |-- key: string (nullable = false)\n |-- value: string (nullable = true)\n\n+-------+----+-----+\n|   name| key|value|\n+-------+----+-----+\n|  james| eye|brown|\n|  james|hair|black|\n|Michael| eye| null|\n|Michael|hair|brown|\n| Robert| eye|     |\n| Robert|hair|  red|\n+-------+----+-----+\n\n"]}}],"execution_count":0},{"cell_type":"code","source":["Using foreach() to Loop Through Rows in DataFrame\nSimilar to map(), foreach() also applied to every row of DataFrame, the difference being foreach() is an action and it returns nothing. Below are some examples to iterate through DataFrame using for each.\n# Foreach example\ndef f(x): print(x)\ndf.foreach(f)\n\n# Another example\ndf.foreach(lambda x: \n    print(\"Data ==>\"+x[\"firstname\"]+\",\"+x[\"lastname\"]+\",\"+x[\"gender\"]+\",\"+str(x[\"salary\"]*2))\n    ) \nUsing pandas() to Iterate\nIf you have a small dataset, you can also Convert PySpark DataFrame to Pandas and use pandas to iterate through. Use spark.sql.execution.arrow.enabled config to enable Apache Arrow with Spark. Apache Spark uses Apache Arrow which is an in-memory columnar format to transfer the data between Python and JVM.\n\n\n# Using pandas\nimport pandas as pd\nspark.conf.set(\"spark.sql.execution.arrow.enabled\", \"true\")\npandasDF = df.toPandas()\nfor index, row in pandasDF.iterrows():\n    print(row['firstname'], row['gender'])\nCollect Data As List and Loop Through\nYou can also Collect the PySpark DataFrame to Driver and iterate through Python, you can also use toLocalIterator().\n\n\n# Collect the data to Python List\ndataCollect = df.collect()\nfor row in dataCollect:\n    print(row['firstname'] + \",\" +row['lastname'])\n\n#Using toLocalIterator()\ndataCollect=df.rdd.toLocalIterator()\nfor row in dataCollect:\n    print(row['firstname'] + \",\" +row['lastname'])"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"4c5378ab-e644-4c11-a6e2-146308361e1c"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["\nfrom pyspark.sql import SparkSession\nspark = SparkSession.builder.appName('SparkByExamples.com').getOrCreate()\n\ndata = [('James','Smith','M',30),('Anna','Rose','F',41),\n  ('Robert','Williams','M',62), \n]\ncolumns = [\"firstname\",\"lastname\",\"gender\",\"salary\"]\ndf = spark.createDataFrame(data=data, schema = columns)\ndef f(x): print(x)\ndf.foreach(f)\n\ndf.foreach(lambda x:\n          print(\"data==>\"+x[\"firstname\"]+\",\"+x[\"lastname\"]+\",\"+x[\"gender\"]+\",\"+str(x[\"salary\"]*2)\n          ))"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"a695c88c-cc39-4afe-a103-a7d8b34dd4ae"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["import pandas as pd\nspark.conf.set(\"spark.sql.execution.arrow.enabled\", \"true\")\npandasDF = df.toPandas()\nfor index,row in pandasDF.iterrows():print(row['firstname'],row['gender'],)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"8cc58461-1370-4a16-9366-ff718486ee36"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"James M\nAnna F\nRobert M\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["James M\nAnna F\nRobert M\n"]}}],"execution_count":0},{"cell_type":"code","source":["datacollect = df.collect()\nfor row in datacollect:print(row['firstname']+\",\"+row['gender'])\n\ndatacollect1 = df.rdd.toLocalIterator()\nfor row in datacollect1:print(row['firstname']+\",\"+str(row['salary']*2))"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"17a93b28-7d88-46cc-8103-f73678c8fc29"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"James,M\nAnna,F\nRobert,M\nJames,60\nAnna,82\nRobert,124\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["James,M\nAnna,F\nRobert,M\nJames,60\nAnna,82\nRobert,124\n"]}}],"execution_count":0},{"cell_type":"code","source":["PySpark fillna() & fill() – Replace NULL/None Values\nIn PySpark, DataFrame.fillna() or DataFrameNaFunctions.fill() is used to replace NULL/None values on all or selected multiple DataFrame columns with either zero(0), empty string, space, or any constant literal values.\n\nWhile working on PySpark DataFrame we often need to replace null values since certain operations on null value return error hence, we need to graciously handle nulls as the first step before processing. Also, while writing to a file, it’s always best practice to replace null values, not doing this result nulls on the output file.\nAs part of the cleanup, sometimes you may need to Drop Rows with NULL/None Values in PySpark DataFrame and Filter Rows by checking IS NULL/NOT NULL conditions.\nIn this article, I will use both fill() and fillna() to replace null/none values with an empty string, constant value, and zero(0) on Dataframe columns integer, string with Python examples.\nfrom pyspark.sql import SparkSession\nspark = SparkSession.builder \\\n    .master(\"local[1]\") \\\n    .appName(\"SparkByExamples.com\") \\\n    .getOrCreate()\n\nfilePath=\"resources/small_zipcode.csv\"\ndf = spark.read.options(header='true', inferSchema='true') \\\n          .csv(filePath)\n\ndf.printSchema()\ndf.show(truncate=False)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"0f6918fa-9763-4c1b-8400-ac595ccc6735"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["from pyspark.sql import SparkSession\n\nspark = SparkSession.builder \\\n    .master(\"local[8]\") \\\n    .appName(\"SparkByExamples.com\") \\\n    .getOrCreate()\n\n#filePath=\"D:\\\\Data\\\\small_zipcode.csv\"\n\n#df = spark.read.options(header='true', inferSchema='True').csv(filePath)\n#df = spark.read.csv(filePath)\n\n#df = spark.read.csv(\"D:\\\\Data\\\\small_zipcode.csv\")\n\n#df = spark.read.csv('D:\\Data\\small_zipcode.csv', inferSchema = True, header = True)\n#df.printSchema()\n#df.show(truncate=False)\nsimpleData9 = [(\"James\",\"Sales\",\"NY\",90000,34,10000),\n    (\"Michael\",\"Sales\",\"NY\",86000,56,20000),\n    (\"Robert\",\"Sales\",\"\",81000,30,23000),\n    (\"Maria\",\"Finance\",\"CA\",90000,24,23000),\n    (\"Raman\",\"Finance\",\"CA\",99000,40,24000),\n    (\"Scott\",\"Finance\",\"\",83000,36,19000),\n    (\"Jen\",\"Finance\",\"NY\",79000,53,15000),\n    (\"Jeff\",\"Marketing\",\"CA\",80000,25,14000),\n    (\"Kumar\",\"Marketing\",\"NY\",91000,50,21000)\n  ]\nschema9 = [\"employee_name\",\"department\",\"state\",\"salary\",\"age\",\"bonus\"]\ndf = spark.createDataFrame(data = simpleData9 , schema = schema9)\ndf.printSchema()\ndf.show()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"8e20ec08-2d95-4169-964f-9a487db2fdb2"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"root\n |-- employee_name: string (nullable = true)\n |-- department: string (nullable = true)\n |-- state: string (nullable = true)\n |-- salary: long (nullable = true)\n |-- age: long (nullable = true)\n |-- bonus: long (nullable = true)\n\n+-------------+----------+-----+------+---+-----+\n|employee_name|department|state|salary|age|bonus|\n+-------------+----------+-----+------+---+-----+\n|        James|     Sales|   NY| 90000| 34|10000|\n|      Michael|     Sales|   NY| 86000| 56|20000|\n|       Robert|     Sales|     | 81000| 30|23000|\n|        Maria|   Finance|   CA| 90000| 24|23000|\n|        Raman|   Finance|   CA| 99000| 40|24000|\n|        Scott|   Finance|     | 83000| 36|19000|\n|          Jen|   Finance|   NY| 79000| 53|15000|\n|         Jeff| Marketing|   CA| 80000| 25|14000|\n|        Kumar| Marketing|   NY| 91000| 50|21000|\n+-------------+----------+-----+------+---+-----+\n\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["root\n |-- employee_name: string (nullable = true)\n |-- department: string (nullable = true)\n |-- state: string (nullable = true)\n |-- salary: long (nullable = true)\n |-- age: long (nullable = true)\n |-- bonus: long (nullable = true)\n\n+-------------+----------+-----+------+---+-----+\n|employee_name|department|state|salary|age|bonus|\n+-------------+----------+-----+------+---+-----+\n|        James|     Sales|   NY| 90000| 34|10000|\n|      Michael|     Sales|   NY| 86000| 56|20000|\n|       Robert|     Sales|     | 81000| 30|23000|\n|        Maria|   Finance|   CA| 90000| 24|23000|\n|        Raman|   Finance|   CA| 99000| 40|24000|\n|        Scott|   Finance|     | 83000| 36|19000|\n|          Jen|   Finance|   NY| 79000| 53|15000|\n|         Jeff| Marketing|   CA| 80000| 25|14000|\n|        Kumar| Marketing|   NY| 91000| 50|21000|\n+-------------+----------+-----+------+---+-----+\n\n"]}}],"execution_count":0},{"cell_type":"code","source":["df.fillna(value =\"IN\",subset=[\"state\"]).show()\ndf.na.fill({\"state\" :\"unknow\"}).show()\ndf.na.fill(\"unknown\").show()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"12be301b-5591-4193-be86-cbfc48503e7f"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"+-------------+----------+-----+------+---+-----+\n|employee_name|department|state|salary|age|bonus|\n+-------------+----------+-----+------+---+-----+\n|        James|     Sales|   NY| 90000| 34|10000|\n|      Michael|     Sales|   NY| 86000| 56|20000|\n|       Robert|     Sales|     | 81000| 30|23000|\n|        Maria|   Finance|   CA| 90000| 24|23000|\n|        Raman|   Finance|   CA| 99000| 40|24000|\n|        Scott|   Finance|     | 83000| 36|19000|\n|          Jen|   Finance|   NY| 79000| 53|15000|\n|         Jeff| Marketing|   CA| 80000| 25|14000|\n|        Kumar| Marketing|   NY| 91000| 50|21000|\n+-------------+----------+-----+------+---+-----+\n\n+-------------+----------+-----+------+---+-----+\n|employee_name|department|state|salary|age|bonus|\n+-------------+----------+-----+------+---+-----+\n|        James|     Sales|   NY| 90000| 34|10000|\n|      Michael|     Sales|   NY| 86000| 56|20000|\n|       Robert|     Sales|     | 81000| 30|23000|\n|        Maria|   Finance|   CA| 90000| 24|23000|\n|        Raman|   Finance|   CA| 99000| 40|24000|\n|        Scott|   Finance|     | 83000| 36|19000|\n|          Jen|   Finance|   NY| 79000| 53|15000|\n|         Jeff| Marketing|   CA| 80000| 25|14000|\n|        Kumar| Marketing|   NY| 91000| 50|21000|\n+-------------+----------+-----+------+---+-----+\n\n+-------------+----------+-----+------+---+-----+\n|employee_name|department|state|salary|age|bonus|\n+-------------+----------+-----+------+---+-----+\n|        James|     Sales|   NY| 90000| 34|10000|\n|      Michael|     Sales|   NY| 86000| 56|20000|\n|       Robert|     Sales|     | 81000| 30|23000|\n|        Maria|   Finance|   CA| 90000| 24|23000|\n|        Raman|   Finance|   CA| 99000| 40|24000|\n|        Scott|   Finance|     | 83000| 36|19000|\n|          Jen|   Finance|   NY| 79000| 53|15000|\n|         Jeff| Marketing|   CA| 80000| 25|14000|\n|        Kumar| Marketing|   NY| 91000| 50|21000|\n+-------------+----------+-----+------+---+-----+\n\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["+-------------+----------+-----+------+---+-----+\n|employee_name|department|state|salary|age|bonus|\n+-------------+----------+-----+------+---+-----+\n|        James|     Sales|   NY| 90000| 34|10000|\n|      Michael|     Sales|   NY| 86000| 56|20000|\n|       Robert|     Sales|     | 81000| 30|23000|\n|        Maria|   Finance|   CA| 90000| 24|23000|\n|        Raman|   Finance|   CA| 99000| 40|24000|\n|        Scott|   Finance|     | 83000| 36|19000|\n|          Jen|   Finance|   NY| 79000| 53|15000|\n|         Jeff| Marketing|   CA| 80000| 25|14000|\n|        Kumar| Marketing|   NY| 91000| 50|21000|\n+-------------+----------+-----+------+---+-----+\n\n+-------------+----------+-----+------+---+-----+\n|employee_name|department|state|salary|age|bonus|\n+-------------+----------+-----+------+---+-----+\n|        James|     Sales|   NY| 90000| 34|10000|\n|      Michael|     Sales|   NY| 86000| 56|20000|\n|       Robert|     Sales|     | 81000| 30|23000|\n|        Maria|   Finance|   CA| 90000| 24|23000|\n|        Raman|   Finance|   CA| 99000| 40|24000|\n|        Scott|   Finance|     | 83000| 36|19000|\n|          Jen|   Finance|   NY| 79000| 53|15000|\n|         Jeff| Marketing|   CA| 80000| 25|14000|\n|        Kumar| Marketing|   NY| 91000| 50|21000|\n+-------------+----------+-----+------+---+-----+\n\n+-------------+----------+-----+------+---+-----+\n|employee_name|department|state|salary|age|bonus|\n+-------------+----------+-----+------+---+-----+\n|        James|     Sales|   NY| 90000| 34|10000|\n|      Michael|     Sales|   NY| 86000| 56|20000|\n|       Robert|     Sales|     | 81000| 30|23000|\n|        Maria|   Finance|   CA| 90000| 24|23000|\n|        Raman|   Finance|   CA| 99000| 40|24000|\n|        Scott|   Finance|     | 83000| 36|19000|\n|          Jen|   Finance|   NY| 79000| 53|15000|\n|         Jeff| Marketing|   CA| 80000| 25|14000|\n|        Kumar| Marketing|   NY| 91000| 50|21000|\n+-------------+----------+-----+------+---+-----+\n\n"]}}],"execution_count":0},{"cell_type":"code","source":["PySpark Pivot and Unpivot DataFrame\nPySpark pivot() function is used to rotate/transpose the data from one column into multiple Dataframe columns and back using unpivot(). Pivot() It is an aggregation where one of the grouping columns values is transposed into individual columns with distinct data.\n\nThis tutorial describes and provides a PySpark example on how to create a Pivot table on DataFrame and Unpivot back.\nimport pyspark\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.functions import expr\n#Create spark session\ndata = [(\"Banana\",1000,\"USA\"), (\"Carrots\",1500,\"USA\"), (\"Beans\",1600,\"USA\"), \\\n      (\"Orange\",2000,\"USA\"),(\"Orange\",2000,\"USA\"),(\"Banana\",400,\"China\"), \\\n      (\"Carrots\",1200,\"China\"),(\"Beans\",1500,\"China\"),(\"Orange\",4000,\"China\"), \\\n      (\"Banana\",2000,\"Canada\"),(\"Carrots\",2000,\"Canada\"),(\"Beans\",2000,\"Mexico\")]\n\ncolumns= [\"Product\",\"Amount\",\"Country\"]\ndf = spark.createDataFrame(data = data, schema = columns)\ndf.printSchema()\ndf.show(truncate=False)\nPivot PySpark DataFrame\nPySpark SQL provides pivot() function to rotate the data from one column into multiple columns. It is an aggregation where one of the grouping columns values is transposed into individual columns with distinct data. To get the total amount exported to each country of each product, will do group by Product, pivot by Country, and the sum of Amount.\n\npivotDF = df.groupBy(\"Product\").pivot(\"Country\").sum(\"Amount\")\npivotDF.printSchema()\npivotDF.show(truncate=False)\nThis will transpose the countries from DataFrame rows into columns and produces the below output. where ever data is not present, it represents as null by default.\nPivot Performance improvement in PySpark 2.0\nversion 2.0 on-wards performance has been improved on Pivot, however, if you are using the lower version; note that pivot is a very expensive operation hence, it is recommended to provide column data (if known) as an argument to function as shown below.\ncountries = [\"USA\",\"China\",\"Canada\",\"Mexico\"]\npivotDF = df.groupBy(\"Product\").pivot(\"Country\", countries).sum(\"Amount\")\npivotDF.show(truncate=False)\nAnother approach is to do two-phase aggregation. PySpark 2.0 uses this implementation in order to improve the performance Spark-13749\n\n\npivotDF = df.groupBy(\"Product\",\"Country\") \\\n      .sum(\"Amount\") \\\n      .groupBy(\"Product\") \\\n      .pivot(\"Country\") \\\n      .sum(\"sum(Amount)\") \\\npivotDF.show(truncate=False)\nThe above two examples return the same output but with better performance."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"0c6790dd-4716-43d6-bef9-d9b0627b26b7"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["from pyspark.sql import SparkSession\n#Create spark session\ndata = [(\"Banana\",1000,\"USA\"), (\"Carrots\",1500,\"USA\"), (\"Beans\",1600,\"USA\"), \\\n      (\"Orange\",2000,\"USA\"),(\"Orange\",2000,\"USA\"),(\"Banana\",400,\"China\"), \\\n      (\"Carrots\",1200,\"China\"),(\"Beans\",1500,\"China\"),(\"Orange\",4000,\"China\"), \\\n      (\"Banana\",2000,\"Canada\"),(\"Carrots\",2000,\"Canada\"),(\"Beans\",2000,\"Mexico\")]\n\ncolumns= [\"Product\",\"Amount\",\"Country\"]\ndf = spark.createDataFrame(data = data, schema = columns)\ndf.printSchema()\n#df.show()\npivotdf = df.groupBy(\"Product\").pivot(\"Country\").sum(\"Amount\").show()\npivotdf1 = df.groupBy(\"Country\").pivot(\"Product\").sum(\"Amount\").show()\ncountries=[\"China\",\"USA\",\"Mexico\",\"Canada\"]\npivotdf = df.groupBy(\"Product\").pivot(\"Country\",countries).sum(\"Amount\").show()\npivotdf3 = df.groupBy(\"Product\",\"Country\")\\\n            .sum(\"Amount\")\\\n            .groupBy(\"Product\")\\\n            .pivot(\"Country\")\\\n            .sum(\"sum(Amount)\")\\\n            .show()\n\n"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"bea2078d-5b2e-4195-9099-28571bf649f1"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"root\n |-- Product: string (nullable = true)\n |-- Amount: long (nullable = true)\n |-- Country: string (nullable = true)\n\n+-------+------+-----+------+----+\n|Product|Canada|China|Mexico| USA|\n+-------+------+-----+------+----+\n| Orange|  null| 4000|  null|4000|\n|  Beans|  null| 1500|  2000|1600|\n| Banana|  2000|  400|  null|1000|\n|Carrots|  2000| 1200|  null|1500|\n+-------+------+-----+------+----+\n\n+-------+------+-----+-------+------+\n|Country|Banana|Beans|Carrots|Orange|\n+-------+------+-----+-------+------+\n|  China|   400| 1500|   1200|  4000|\n|    USA|  1000| 1600|   1500|  4000|\n| Mexico|  null| 2000|   null|  null|\n| Canada|  2000| null|   2000|  null|\n+-------+------+-----+-------+------+\n\n+-------+-----+----+------+------+\n|Product|China| USA|Mexico|Canada|\n+-------+-----+----+------+------+\n| Orange| 4000|4000|  null|  null|\n|  Beans| 1500|1600|  2000|  null|\n| Banana|  400|1000|  null|  2000|\n|Carrots| 1200|1500|  null|  2000|\n+-------+-----+----+------+------+\n\n+-------+------+-----+------+----+\n|Product|Canada|China|Mexico| USA|\n+-------+------+-----+------+----+\n| Orange|  null| 4000|  null|4000|\n|  Beans|  null| 1500|  2000|1600|\n| Banana|  2000|  400|  null|1000|\n|Carrots|  2000| 1200|  null|1500|\n+-------+------+-----+------+----+\n\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["root\n |-- Product: string (nullable = true)\n |-- Amount: long (nullable = true)\n |-- Country: string (nullable = true)\n\n+-------+------+-----+------+----+\n|Product|Canada|China|Mexico| USA|\n+-------+------+-----+------+----+\n| Orange|  null| 4000|  null|4000|\n|  Beans|  null| 1500|  2000|1600|\n| Banana|  2000|  400|  null|1000|\n|Carrots|  2000| 1200|  null|1500|\n+-------+------+-----+------+----+\n\n+-------+------+-----+-------+------+\n|Country|Banana|Beans|Carrots|Orange|\n+-------+------+-----+-------+------+\n|  China|   400| 1500|   1200|  4000|\n|    USA|  1000| 1600|   1500|  4000|\n| Mexico|  null| 2000|   null|  null|\n| Canada|  2000| null|   2000|  null|\n+-------+------+-----+-------+------+\n\n+-------+-----+----+------+------+\n|Product|China| USA|Mexico|Canada|\n+-------+-----+----+------+------+\n| Orange| 4000|4000|  null|  null|\n|  Beans| 1500|1600|  2000|  null|\n| Banana|  400|1000|  null|  2000|\n|Carrots| 1200|1500|  null|  2000|\n+-------+-----+----+------+------+\n\n+-------+------+-----+------+----+\n|Product|Canada|China|Mexico| USA|\n+-------+------+-----+------+----+\n| Orange|  null| 4000|  null|4000|\n|  Beans|  null| 1500|  2000|1600|\n| Banana|  2000|  400|  null|1000|\n|Carrots|  2000| 1200|  null|1500|\n+-------+------+-----+------+----+\n\n"]}}],"execution_count":0},{"cell_type":"code","source":["Unpivot PySpark DataFrame\nUnpivot is a reverse operation, we can achieve by rotating column values into rows values. PySpark SQL doesn’t have unpivot function hence will use the stack() function. Below code converts column countries to row.\nfrom pyspark.sql.functions import expr\nunpivotExpr = \"stack(3, 'Canada', Canada, 'China', China, 'Mexico', Mexico) as (Country,Total)\"\nunPivotDF = pivotDF.select(\"Product\", expr(unpivotExpr)) \\\n    .where(\"Total is not null\")\nunPivotDF.show(truncate=False)\nunPivotDF.show()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"1029e184-01e6-4a1c-878f-cbb3e9b70239"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["from pyspark.sql.functions import expr\nunpivoexpr = \"stack(3,'Canada',Canada,'China',China,'Mexico',Mexico) as (Country,Total)\"\nunpivotdf = pivotdf.select(\"Product\",expr(unpivoexpr)).where(\"Total is not null\")\nunpivotdf.show(truncate = False)\nunpivotdf.show()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"a15c575b-d7f1-4153-82ad-bb12302432a9"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n\u001B[0;31mAttributeError\u001B[0m                            Traceback (most recent call last)\n\u001B[0;32m<command-3484641498263362>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[0;32mfrom\u001B[0m \u001B[0mpyspark\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msql\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mfunctions\u001B[0m \u001B[0;32mimport\u001B[0m \u001B[0mexpr\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      2\u001B[0m \u001B[0munpivoexpr\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;34m\"stack(3,'Canada',Canada,'China',China,'Mexico',Mexico) as (Country,Total)\"\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 3\u001B[0;31m \u001B[0munpivotdf\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mpivotdf\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mselect\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"Product\"\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0mexpr\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0munpivoexpr\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mwhere\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"Total is not null\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      4\u001B[0m \u001B[0munpivotdf\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mshow\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mtruncate\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;32mFalse\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      5\u001B[0m \u001B[0munpivotdf\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mshow\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;31mAttributeError\u001B[0m: 'NoneType' object has no attribute 'select'","errorSummary":"<span class='ansi-red-fg'>AttributeError</span>: 'NoneType' object has no attribute 'select'","metadata":{},"errorTraceType":"ansi","type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/plain":["\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n\u001B[0;31mAttributeError\u001B[0m                            Traceback (most recent call last)\n\u001B[0;32m<command-3484641498263362>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[0;32mfrom\u001B[0m \u001B[0mpyspark\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msql\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mfunctions\u001B[0m \u001B[0;32mimport\u001B[0m \u001B[0mexpr\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      2\u001B[0m \u001B[0munpivoexpr\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;34m\"stack(3,'Canada',Canada,'China',China,'Mexico',Mexico) as (Country,Total)\"\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 3\u001B[0;31m \u001B[0munpivotdf\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mpivotdf\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mselect\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"Product\"\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0mexpr\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0munpivoexpr\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mwhere\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"Total is not null\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      4\u001B[0m \u001B[0munpivotdf\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mshow\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mtruncate\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;32mFalse\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      5\u001B[0m \u001B[0munpivotdf\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mshow\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;31mAttributeError\u001B[0m: 'NoneType' object has no attribute 'select'"]}}],"execution_count":0},{"cell_type":"code","source":["PySpark ArrayType Column With Examples\nPySpark pyspark.sql.types.ArrayType (ArrayType extends DataType class) is used to define an array data type column on DataFrame that holds the same type of elements, In this article, I will explain how to create a DataFrame ArrayType column using org.apache.spark.sql.types.ArrayType class and applying some SQL functions on the array columns with examples.\n\nWhile working with structured files (Avro, Parquet e.t.c) or semi-structured (JSON) files, we often get data with complex structures like MapType, ArrayType, StructType e.t.c. I will try my best to cover some mostly used functions on ArrayType columns.\nWhat is PySpark ArrayType\nPySpark ArrayType is a collection data type that extends the DataType class which is a superclass of all types in PySpark. All elements of ArrayType should have the same type of elements.\nCreate PySpark ArrayType\nYou can create an instance of an ArrayType using ArraType() class, This takes arguments valueType and one optional argument valueContainsNull to specify if a value can accept null, by default it takes True. valueType should be a PySpark type that extends DataType class.\nfrom pyspark.sql.types import StringType, ArrayType\narrayCol = ArrayType(StringType(),False)\nAbove example creates string array and doesn’t not accept null values.\n\nCreate PySpark ArrayType Column Using StructType\nLet’s create a DataFrame with few array columns by using PySpark StructType & StructField classes.\n\n\ndata = [\n (\"James,,Smith\",[\"Java\",\"Scala\",\"C++\"],[\"Spark\",\"Java\"],\"OH\",\"CA\"),\n (\"Michael,Rose,\",[\"Spark\",\"Java\",\"C++\"],[\"Spark\",\"Java\"],\"NY\",\"NJ\"),\n (\"Robert,,Williams\",[\"CSharp\",\"VB\"],[\"Spark\",\"Python\"],\"UT\",\"NV\")\n]\n\nfrom pyspark.sql.types import StringType, ArrayType,StructType,StructField\nschema = StructType([ \n    StructField(\"name\",StringType(),True), \n    StructField(\"languagesAtSchool\",ArrayType(StringType()),True), \n    StructField(\"languagesAtWork\",ArrayType(StringType()),True), \n    StructField(\"currentState\", StringType(), True), \n    StructField(\"previousState\", StringType(), True)\n  ])\n\ndf = spark.createDataFrame(data=data,schema=schema)\ndf.printSchema()\ndf.show()\nThis snippet creates two Array columns languagesAtSchool and languagesAtWork which defines languages learned at School and languages using at work. For the rest of the article, I will use these array columns of DataFrame and provide examples of PySpark SQL array functions. printSchema() and show() from above snippet display below output.\n\nPySpark ArrayType (Array) Functions\nPySpark SQL provides several Array functions to work with the ArrayType column, In this section, we will see some of the most commonly used SQL functions.\n\nexplode()\nUse explode() function to create a new row for each element in the given array column. There are various PySpark SQL explode functions available to work with Array columns.\nfrom pyspark.sql.functions import explode\ndf.select(df.name,explode(df.languagesAtS\n\nSplit()\nsplit() sql function returns an array type after splitting the string column by delimiter. Below example split the name column by comma delimiter.\n from pyspark.sql.functions import split\ndf.select(split(df.name,\",\").alias(\"nameAsArray\")).show()\n\narray()\nUse array() function to create a new array column by merging the data from multiple columns. All input columns must have the same data type. The below example combines the data from currentState and previousState and creates a new column states.\n\n\nfrom pyspark.sql.functions import array\ndf.select(df.name,array(df.currentState,df.previousState).alias(\"States\")).show()\n"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"af850dfa-6ffe-4d7c-88f1-c40cafd56de5"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["import pyspark\nfrom pyspark.sql.types import StructType,StructField,StringType,IntegerType,ArrayType,MapType\nfrom pyspark.sql.functions import explode,split,array,array_contains\ndata = [((\"james\",\"\",\"smith\"),[\"java\",\"scala\",\"c++\"],[\"spark\",\"Java\"],\"oh,HO\",\"ca\"),\n         ((\"Michael\",\"\",\"Rose\"),[\"Spark\",\"Java\",\"C++\"],[\"Spark\",\"Java\"],\"NY,YN\",\"NJ\"),\n ((\"Robert\",\"\",\"Williams\"),[\"CSharp\",\"VB\"],[\"Spark\",\"Python\"],\"UT,AP\",\"NV\")    \n]\ncolumns = StructType([StructField(\"name\",StructType([StructField(\"fname\",StringType(),True),StructField(\"lname\",StringType(),True),StructField(\"mname\",StringType(),True)])),\n                      StructField(\"languageAtSchool\",ArrayType(StringType()),True),\n                      StructField(\"languageAtWork\",ArrayType(StringType()),True),\n                      StructField(\"currentstatus\",StringType(),True),\n                      StructField(\"previousstate\",StringType(),True)    \n])\nDF5 = spark.createDataFrame(data= data , schema= columns)\nDF5.show()\n#DF5.select(DF5.name,DF5.currentstatus,DF5.languageAtWork).show()\nDF5.select(DF5.name,DF5.currentstatus,explode(DF5.languageAtWork)).show()\nDF5.select(DF5.name,DF5.previousstate,explode(DF5.languageAtSchool)).show()\n#split()\nDF5.select(split(DF5.currentstatus,\",\").alias(\"arrayname\")).show()\n#array()\nDF5.select(DF5.name,array(DF5.currentstatus,DF5.previousstate).alias(\"states\")).show()\nDF5.select(array(DF5.previousstate,DF5.currentstatus).alias(\"states1\")).show()\nDF5.select(DF5.name,array_contains(DF5.languageAtSchool,\"Spark\")).show()\nDF5.select(DF5.languageAtSchool,array_contains(DF5.languageAtSchool,\"C++\")).show()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"3e4f5996-cdfa-4aaf-9de7-6d9537c8e4cc"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"+--------------------+------------------+---------------+-------------+-------------+\n|                name|  languageAtSchool| languageAtWork|currentstatus|previousstate|\n+--------------------+------------------+---------------+-------------+-------------+\n|    {james, , smith}|[java, scala, c++]|  [spark, Java]|        oh,HO|           ca|\n|   {Michael, , Rose}|[Spark, Java, C++]|  [Spark, Java]|        NY,YN|           NJ|\n|{Robert, , Williams}|      [CSharp, VB]|[Spark, Python]|        UT,AP|           NV|\n+--------------------+------------------+---------------+-------------+-------------+\n\n+--------------------+-------------+------+\n|                name|currentstatus|   col|\n+--------------------+-------------+------+\n|    {james, , smith}|        oh,HO| spark|\n|    {james, , smith}|        oh,HO|  Java|\n|   {Michael, , Rose}|        NY,YN| Spark|\n|   {Michael, , Rose}|        NY,YN|  Java|\n|{Robert, , Williams}|        UT,AP| Spark|\n|{Robert, , Williams}|        UT,AP|Python|\n+--------------------+-------------+------+\n\n+--------------------+-------------+------+\n|                name|previousstate|   col|\n+--------------------+-------------+------+\n|    {james, , smith}|           ca|  java|\n|    {james, , smith}|           ca| scala|\n|    {james, , smith}|           ca|   c++|\n|   {Michael, , Rose}|           NJ| Spark|\n|   {Michael, , Rose}|           NJ|  Java|\n|   {Michael, , Rose}|           NJ|   C++|\n|{Robert, , Williams}|           NV|CSharp|\n|{Robert, , Williams}|           NV|    VB|\n+--------------------+-------------+------+\n\n+---------+\n|arrayname|\n+---------+\n| [oh, HO]|\n| [NY, YN]|\n| [UT, AP]|\n+---------+\n\n+--------------------+-----------+\n|                name|     states|\n+--------------------+-----------+\n|    {james, , smith}|[oh,HO, ca]|\n|   {Michael, , Rose}|[NY,YN, NJ]|\n|{Robert, , Williams}|[UT,AP, NV]|\n+--------------------+-----------+\n\n+-----------+\n|    states1|\n+-----------+\n|[ca, oh,HO]|\n|[NJ, NY,YN]|\n|[NV, UT,AP]|\n+-----------+\n\n+--------------------+---------------------------------------+\n|                name|array_contains(languageAtSchool, Spark)|\n+--------------------+---------------------------------------+\n|    {james, , smith}|                                  false|\n|   {Michael, , Rose}|                                   true|\n|{Robert, , Williams}|                                  false|\n+--------------------+---------------------------------------+\n\n+------------------+-------------------------------------+\n|  languageAtSchool|array_contains(languageAtSchool, C++)|\n+------------------+-------------------------------------+\n|[java, scala, c++]|                                false|\n|[Spark, Java, C++]|                                 true|\n|      [CSharp, VB]|                                false|\n+------------------+-------------------------------------+\n\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["+--------------------+------------------+---------------+-------------+-------------+\n|                name|  languageAtSchool| languageAtWork|currentstatus|previousstate|\n+--------------------+------------------+---------------+-------------+-------------+\n|    {james, , smith}|[java, scala, c++]|  [spark, Java]|        oh,HO|           ca|\n|   {Michael, , Rose}|[Spark, Java, C++]|  [Spark, Java]|        NY,YN|           NJ|\n|{Robert, , Williams}|      [CSharp, VB]|[Spark, Python]|        UT,AP|           NV|\n+--------------------+------------------+---------------+-------------+-------------+\n\n+--------------------+-------------+------+\n|                name|currentstatus|   col|\n+--------------------+-------------+------+\n|    {james, , smith}|        oh,HO| spark|\n|    {james, , smith}|        oh,HO|  Java|\n|   {Michael, , Rose}|        NY,YN| Spark|\n|   {Michael, , Rose}|        NY,YN|  Java|\n|{Robert, , Williams}|        UT,AP| Spark|\n|{Robert, , Williams}|        UT,AP|Python|\n+--------------------+-------------+------+\n\n+--------------------+-------------+------+\n|                name|previousstate|   col|\n+--------------------+-------------+------+\n|    {james, , smith}|           ca|  java|\n|    {james, , smith}|           ca| scala|\n|    {james, , smith}|           ca|   c++|\n|   {Michael, , Rose}|           NJ| Spark|\n|   {Michael, , Rose}|           NJ|  Java|\n|   {Michael, , Rose}|           NJ|   C++|\n|{Robert, , Williams}|           NV|CSharp|\n|{Robert, , Williams}|           NV|    VB|\n+--------------------+-------------+------+\n\n+---------+\n|arrayname|\n+---------+\n| [oh, HO]|\n| [NY, YN]|\n| [UT, AP]|\n+---------+\n\n+--------------------+-----------+\n|                name|     states|\n+--------------------+-----------+\n|    {james, , smith}|[oh,HO, ca]|\n|   {Michael, , Rose}|[NY,YN, NJ]|\n|{Robert, , Williams}|[UT,AP, NV]|\n+--------------------+-----------+\n\n+-----------+\n|    states1|\n+-----------+\n|[ca, oh,HO]|\n|[NJ, NY,YN]|\n|[NV, UT,AP]|\n+-----------+\n\n+--------------------+---------------------------------------+\n|                name|array_contains(languageAtSchool, Spark)|\n+--------------------+---------------------------------------+\n|    {james, , smith}|                                  false|\n|   {Michael, , Rose}|                                   true|\n|{Robert, , Williams}|                                  false|\n+--------------------+---------------------------------------+\n\n+------------------+-------------------------------------+\n|  languageAtSchool|array_contains(languageAtSchool, C++)|\n+------------------+-------------------------------------+\n|[java, scala, c++]|                                false|\n|[Spark, Java, C++]|                                 true|\n|      [CSharp, VB]|                                false|\n+------------------+-------------------------------------+\n\n"]}}],"execution_count":0},{"cell_type":"code","source":["PySpark MapType (Dict) Usage with Examples\nPySpark MapType (also called map type) is a data type to represent Python Dictionary (dict) to store key-value pair, a MapType object comprises three fields, keyType (a DataType), valueType (a DataType) and valueContainsNull (a BooleanType).\n\nWhat is PySpark MapType\nPySpark MapType is used to represent map key-value pair similar to python Dictionary (Dict), it extends DataType class which is a superclass of all types in PySpark and takes two mandatory arguments keyType and valueType of type DataType and one optional boolean argument valueContainsNull. keyType and valueType can be any type that extends the DataType class. for e.g StringType, IntegerType, ArrayType, MapType, StructType (struct) e.t.c.\n1. Create PySpark MapType\nIn order to use MapType data type first, you need to import it from pyspark.sql.types.MapType and use MapType() constructor to create a map object.\n\nfrom pyspark.sql.types import StringType, MapType\nmapCol = MapType(StringType(),StringType(),False)\nThe First param keyType is used to specify the type of the key in the map.\nThe Second param valueType is used to specify the type of the value in the map.\nThird parm valueContainsNull is an optional boolean type that is used to specify if the value of the second param can accept Null/None values.\nThe key of the map won’t accept None/Null values.\nPySpark provides several SQL functions to work with MapType.\n2. Create MapType From StructType\nLet’s see how to create a MapType by using PySpark StructType & StructField, StructType() constructor takes list of StructField, StructField takes a fieldname and type of the value.\n\n\nfrom pyspark.sql.types import StructField, StructType, StringType, MapType\nschema = StructType([\n    StructField('name', StringType(), True),\n    StructField('properties', MapType(StringType(),StringType()),True)\n])\nNow let’s create a DataFrame by using above StructType schema.\n\n\nfrom pyspark.sql import SparkSession\nspark = SparkSession.builder.appName('SparkByExamples.com').getOrCreate()\ndataDictionary = [\n        ('James',{'hair':'black','eye':'brown'}),\n        ('Michael',{'hair':'brown','eye':None}),\n        ('Robert',{'hair':'red','eye':'black'}),\n        ('Washington',{'hair':'grey','eye':'grey'}),\n        ('Jefferson',{'hair':'brown','eye':''})\n        ]\ndf = spark.createDataFrame(data=dataDictionary, schema = schema)\ndf.printSchema()\ndf.show(truncate=False)\ndf.printSchema() yields the Schema and df.show() yields the DataFrame output.\n\n3. Access PySpark MapType Elements\nLet’s see how to extract the key and values from the PySpark DataFrame Dictionary column. Here I have used PySpark map transformation to read the values of properties (MapType column)\n\n\ndf3=df.rdd.map(lambda x: \\\n    (x.name,x.properties[\"hair\"],x.properties[\"eye\"])) \\\n    .toDF([\"name\",\"hair\",\"eye\"])\ndf3.printSchema()\ndf3.show()\n\nLet’s use another way to get the value of a key from Map using getItem() of Column type, this method takes a key as an argument and returns a value.\n\n\ndf.withColumn(\"hair\",df.properties.getItem(\"hair\")) \\\n  .withColumn(\"eye\",df.properties.getItem(\"eye\")) \\\n  .drop(\"properties\") \\\n  .show()\n\ndf.withColumn(\"hair\",df.properties[\"hair\"]) \\\n  .withColumn(\"eye\",df.properties[\"eye\"]) \\\n  .drop(\"properties\") \\\n  .show()\n"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"664440e8-8bb1-4301-b87e-9693c63fe48e"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["import pyspark \nfrom pyspark.sql import SparkSession\nspark = SparkSession.builder.appName(\"sparkbyexample.com\").getOrCreate()\ndataDictonary = [('james',{'eye':'balck','hair':'black'}),\n                 ('krishna',{'eye':'black','hair':'white'}),\n                 ('mani',{'eye':'red','hair':'brown'})    \n]\ndatrcolumns = StructType([StructField(\"name\",StringType(),True),\n                          StructField(\"properties\",MapType(StringType(),StringType()),True)])\n    \nDF6= spark.createDataFrame(data = dataDictonary,schema=datrcolumns)\n#DF6.show(truncate = False)\nDF7 = DF6.rdd.map(lambda x:(x.name,x.properties[\"hair\"],x.properties[\"eye\"])).toDF([\"name\",\"hair\",\"eye\"])\nDF8=DF6.rdd.map(lambda x:(x.properties[\"eye\"],x.properties[\"hair\"],x.name)).toDF([\"eye\",\"hair\",\"name\"])\n#DF7.show()\n#DF8.show()\nDF6.withColumn(\"hair\",DF6.properties.getItem(\"hair\")).withColumn(\"eye\",DF6.properties.getItem(\"eye\")).drop(\"properties\").show()\nDF6.withColumn(\"eye\",DF6.properties.getItem(\"eye\")).withColumn(\"hair\",DF6.properties.getItem(\"hair\")).show()\nDF6.withColumn(\"eye\",DF6.properties[\"eye\"]).withColumn(\"hair\",DF6.properties[\"hair\"]).drop(\"properties\").show()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"b0d49167-8006-4ddc-9995-0d6c82a81c72"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"+-------+-----+-----+\n|   name| hair|  eye|\n+-------+-----+-----+\n|  james|black|balck|\n|krishna|white|black|\n|   mani|brown|  red|\n+-------+-----+-----+\n\n+-------+--------------------+-----+-----+\n|   name|          properties|  eye| hair|\n+-------+--------------------+-----+-----+\n|  james|{eye -> balck, ha...|balck|black|\n|krishna|{eye -> black, ha...|black|white|\n|   mani|{eye -> red, hair...|  red|brown|\n+-------+--------------------+-----+-----+\n\n+-------+-----+-----+\n|   name|  eye| hair|\n+-------+-----+-----+\n|  james|balck|black|\n|krishna|black|white|\n|   mani|  red|brown|\n+-------+-----+-----+\n\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["+-------+-----+-----+\n|   name| hair|  eye|\n+-------+-----+-----+\n|  james|black|balck|\n|krishna|white|black|\n|   mani|brown|  red|\n+-------+-----+-----+\n\n+-------+--------------------+-----+-----+\n|   name|          properties|  eye| hair|\n+-------+--------------------+-----+-----+\n|  james|{eye -> balck, ha...|balck|black|\n|krishna|{eye -> black, ha...|black|white|\n|   mani|{eye -> red, hair...|  red|brown|\n+-------+--------------------+-----+-----+\n\n+-------+-----+-----+\n|   name|  eye| hair|\n+-------+-----+-----+\n|  james|balck|black|\n|krishna|black|white|\n|   mani|  red|brown|\n+-------+-----+-----+\n\n"]}}],"execution_count":0},{"cell_type":"code","source":["from pyspark.sql.functions import explode,map_keys,map_values\nDF6.select(DF6.name,explode(\"properties\")).show()\nDF6.select(DF6.name,map_keys(DF6.properties)).show()\nDF6.select(DF6.name,map_values(DF6.properties)).show()\n"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"e9e8adca-36b6-45b3-81ef-c5a25dcccd78"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"+-------+----+-----+\n|   name| key|value|\n+-------+----+-----+\n|  james| eye|balck|\n|  james|hair|black|\n|krishna| eye|black|\n|krishna|hair|white|\n|   mani| eye|  red|\n|   mani|hair|brown|\n+-------+----+-----+\n\n+-------+--------------------+\n|   name|map_keys(properties)|\n+-------+--------------------+\n|  james|         [eye, hair]|\n|krishna|         [eye, hair]|\n|   mani|         [eye, hair]|\n+-------+--------------------+\n\n+-------+----------------------+\n|   name|map_values(properties)|\n+-------+----------------------+\n|  james|        [balck, black]|\n|krishna|        [black, white]|\n|   mani|          [red, brown]|\n+-------+----------------------+\n\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["+-------+----+-----+\n|   name| key|value|\n+-------+----+-----+\n|  james| eye|balck|\n|  james|hair|black|\n|krishna| eye|black|\n|krishna|hair|white|\n|   mani| eye|  red|\n|   mani|hair|brown|\n+-------+----+-----+\n\n+-------+--------------------+\n|   name|map_keys(properties)|\n+-------+--------------------+\n|  james|         [eye, hair]|\n|krishna|         [eye, hair]|\n|   mani|         [eye, hair]|\n+-------+--------------------+\n\n+-------+----------------------+\n|   name|map_values(properties)|\n+-------+----------------------+\n|  james|        [balck, black]|\n|krishna|        [black, white]|\n|   mani|          [red, brown]|\n+-------+----------------------+\n\n"]}}],"execution_count":0},{"cell_type":"code","source":["\n"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"c6882579-a6ba-40a8-88f3-be34da4d37e9"}},"outputs":[],"execution_count":0}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"Krishna_workspace","dashboards":[],"notebookMetadata":{"pythonIndentUnit":4},"language":"python","widgets":{},"notebookOrigID":98536813389210}},"nbformat":4,"nbformat_minor":0}
