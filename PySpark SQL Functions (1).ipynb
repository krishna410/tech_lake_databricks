{"cells":[{"cell_type":"code","source":["PySpark Aggregate Functions with Examples\nPySpark provides built-in standard Aggregate functions defines in DataFrame API, these come in handy when we need to make aggregate operations on DataFrame columns. Aggregate functions operate on a group of rows and calculate a single return value for every group.\n\nAll these aggregate functions accept input as, Column type or column name in a string and several other arguments based on the function and return Column type.\nWhen possible try to leverage standard library as they are little bit more compile-time safety, handles null and perform better when compared to UDF’s. If your application is critical on performance try to avoid using custom UDF at all costs as these are not guarantee on performance.\n\nPySpark Aggregate Functions\nPySpark SQL Aggregate functions are grouped as “agg_funcs” in Pyspark. Below is a list of functions defined under this group. Click on each link to learn with example.\nPySpark Aggregate Functions Examples\nFirst, let’s create a DataFrame to work with PySpark aggregate functions. All examples provided here are also available at PySpark Examples GitHub project.\nsimpleData = [(\"James\", \"Sales\", 3000),\n    (\"Michael\", \"Sales\", 4600),\n    (\"Robert\", \"Sales\", 4100),\n    (\"Maria\", \"Finance\", 3000),\n    (\"James\", \"Sales\", 3000),\n    (\"Scott\", \"Finance\", 3300),\n    (\"Jen\", \"Finance\", 3900),\n    (\"Jeff\", \"Marketing\", 3000),\n    (\"Kumar\", \"Marketing\", 2000),\n    (\"Saif\", \"Sales\", 4100)\n  ]\nschema = [\"employee_name\", \"department\", \"salary\"]\ndf = spark.createDataFrame(data=simpleData, schema = schema)\ndf.printSchema()\ndf.show(truncate=False)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"dfbaf0fa-b4af-4985-b7b8-29b180d14019"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["approx_count_distinct Aggregate Function\nIn PySpark approx_count_distinct() function returns the count of distinct items in a group.\n//approx_count_distinct()\nprint(\"approx_count_distinct: \" + \\\n      str(df.select(approx_count_distinct(\"salary\")).collect()[0][0]))\n\n//Prints approx_count_distinct: 6\n\navg (average) Aggregate Function\navg() function returns the average of values in the input column.\n//avg\nprint(\"avg: \" + str(df.select(avg(\"salary\")).collect()[0][0]))\n\n//Prints avg: 3400.0\n\ncollect_list Aggregate Function\ncollect_list() function returns all values from an input column with duplicates.\n\ncountDistinct Aggregate Function\ncountDistinct() function returns the number of distinct elements in a columns\n\ncount function\ncount() function returns number of elements in a column.\nprint(\"count: \"+str(df.select(count(\"salary\")).collect()[0]))\n\nPrints county: 10\n\ngrouping function\ngrouping() Indicates whether a given input column is aggregated or not. returns 1 for aggregated or 0 for not aggregated in the result. If you try grouping directly on the salary column you will get below error.\nException in thread \"main\" org.apache.spark.sql.AnalysisException:\n  // grouping() can only be used with GroupingSets/Cube/Rollup\nfirst function\nfirst() function returns the first element in a column when ignoreNulls is set to true, it returns the first non-null element.\n\n//first\ndf.select(first(\"salary\")).show(truncate=False)\nlast function\nlast() function returns the last element in a column. when ignoreNulls is set to true, it returns the last non-null element.\n//last\ndf.select(last(\"salary\")).show(truncate=False)\nmax function\nmax() function returns the maximum value in a column.\ndf.select(max(\"salary\")).show(truncate=False)\nmin function\nmin() function\ndf.select(min(\"salary\")).show(truncate=False)\nsum function\nsum() function Returns the sum of all values in a column.\ndf.select(sum(\"salary\")).show(truncate=False)\nsumDistinct function\nsumDistinct() function returns the sum of all distinct values in a column.\ndf.select(sumDistinct(\"salary\")).show(truncate=False)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"117abab3-60d7-4420-97fc-fbe79c794b70"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["import pyspark \nfrom pyspark.sql.functions import col,avg,approx_count_distinct,collect_list,collect_set,countDistinct,count,first,last,min,max,sum,sum_distinct\nsimpleData = [(\"James\", \"Sales\", 3000),\n    (\"Michael\", \"Sales\", 4600),\n    (\"Robert\", \"Sales\", 4100),\n    (\"Maria\", \"Finance\", 3000),\n    (\"James\", \"Sales\", 3000),\n    (\"Scott\", \"Finance\", 3300),\n    (\"Jen\", \"Finance\", 3900),\n    (\"Jeff\", \"Marketing\", 3000),\n    (\"Kumar\", \"Marketing\", 2000),\n    (\"Saif\", \"Sales\", 4100)\n  ]\nschema = [\"employee_name\", \"department\", \"salary\"]\ndf=spark.createDataFrame(data = simpleData,schema = schema)\ndf.show()\nprint(\"approx_count_distinct: \"+\\\n    str(df.select(approx_count_distinct(\"salary\")).collect()[0][0]\n     ))\n\nprint(\"avg: \"+str(df.select(avg(\"salary\")).collect()[0][0]))\n\ndf.select(collect_list(\"salary\")).show(truncate = False)\ndf.select(collect_set(\"salary\")).show(truncate = False)\ndf.select(countDistinct(\"department\",\"salary\")).show(truncate = False)\nprint(\"countDistinct:\"+str(df.select(countDistinct(\"department\",\"salary\")).collect()[0][0]))\nprint(\"count:\"+str(df.select(count(\"salary\")).collect()[0][0]))\ndf.select(first(\"salary\")).show()\ndf.select(last(\"salary\")).show()\ndf.select(min(\"salary\")).show()\ndf.select(max(\"salary\")).show()\ndf.select(sum(\"salary\")).show()\ndf.select(sum_distinct(\"salary\")).show()\n\n"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"fa905d94-9ab4-475c-8b05-c0ed85c9a413"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"+-------------+----------+------+\n|employee_name|department|salary|\n+-------------+----------+------+\n|        James|     Sales|  3000|\n|      Michael|     Sales|  4600|\n|       Robert|     Sales|  4100|\n|        Maria|   Finance|  3000|\n|        James|     Sales|  3000|\n|        Scott|   Finance|  3300|\n|          Jen|   Finance|  3900|\n|         Jeff| Marketing|  3000|\n|        Kumar| Marketing|  2000|\n|         Saif|     Sales|  4100|\n+-------------+----------+------+\n\napprox_count_distinct: 6\navg: 3400.0\n+------------------------------------------------------------+\n|collect_list(salary)                                        |\n+------------------------------------------------------------+\n|[3000, 4600, 4100, 3000, 3000, 3300, 3900, 3000, 2000, 4100]|\n+------------------------------------------------------------+\n\n+------------------------------------+\n|collect_set(salary)                 |\n+------------------------------------+\n|[4600, 3000, 3900, 4100, 3300, 2000]|\n+------------------------------------+\n\n+----------------------------------+\n|count(DISTINCT department, salary)|\n+----------------------------------+\n|8                                 |\n+----------------------------------+\n\ncountDistinct:8\ncount:10\n+-------------+\n|first(salary)|\n+-------------+\n|         3000|\n+-------------+\n\n+------------+\n|last(salary)|\n+------------+\n|        4100|\n+------------+\n\n+-----------+\n|min(salary)|\n+-----------+\n|       2000|\n+-----------+\n\n+-----------+\n|max(salary)|\n+-----------+\n|       4600|\n+-----------+\n\n+-----------+\n|sum(salary)|\n+-----------+\n|      34000|\n+-----------+\n\n+--------------------+\n|sum(DISTINCT salary)|\n+--------------------+\n|               20900|\n+--------------------+\n\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["+-------------+----------+------+\n|employee_name|department|salary|\n+-------------+----------+------+\n|        James|     Sales|  3000|\n|      Michael|     Sales|  4600|\n|       Robert|     Sales|  4100|\n|        Maria|   Finance|  3000|\n|        James|     Sales|  3000|\n|        Scott|   Finance|  3300|\n|          Jen|   Finance|  3900|\n|         Jeff| Marketing|  3000|\n|        Kumar| Marketing|  2000|\n|         Saif|     Sales|  4100|\n+-------------+----------+------+\n\napprox_count_distinct: 6\navg: 3400.0\n+------------------------------------------------------------+\n|collect_list(salary)                                        |\n+------------------------------------------------------------+\n|[3000, 4600, 4100, 3000, 3000, 3300, 3900, 3000, 2000, 4100]|\n+------------------------------------------------------------+\n\n+------------------------------------+\n|collect_set(salary)                 |\n+------------------------------------+\n|[4600, 3000, 3900, 4100, 3300, 2000]|\n+------------------------------------+\n\n+----------------------------------+\n|count(DISTINCT department, salary)|\n+----------------------------------+\n|8                                 |\n+----------------------------------+\n\ncountDistinct:8\ncount:10\n+-------------+\n|first(salary)|\n+-------------+\n|         3000|\n+-------------+\n\n+------------+\n|last(salary)|\n+------------+\n|        4100|\n+------------+\n\n+-----------+\n|min(salary)|\n+-----------+\n|       2000|\n+-----------+\n\n+-----------+\n|max(salary)|\n+-----------+\n|       4600|\n+-----------+\n\n+-----------+\n|sum(salary)|\n+-----------+\n|      34000|\n+-----------+\n\n+--------------------+\n|sum(DISTINCT salary)|\n+--------------------+\n|               20900|\n+--------------------+\n\n"]}}],"execution_count":0},{"cell_type":"code","source":["PySpark Window Functions\nPySpark Window functions are used to calculate results such as the rank, row number e.t.c over a range of input rows. In this article, I’ve explained the concept of window functions, syntax, and finally how to use them with PySpark SQL and PySpark DataFrame API. These come in handy when we need to make aggregate operations in a specific window frame on DataFrame columns.\n\nWhen possible try to leverage standard library as they are little bit more compile-time safety, handles null and perform better when compared to UDF’s. If your application is critical on performance try to avoid using custom UDF at all costs as these are not guarantee on performance\n1. Window Functions\nPySpark Window functions operate on a group of rows (like frame, partition) and return a single value for every input row. PySpark SQL supports three kinds of window functions:\n\nranking functions\nanalytic functions\naggregate functions\nThe below table defines Ranking and Analytic functions and for aggregate functions, we can use any existing aggregate functions as a window function.\nTo perform an operation on a group first, we need to partition the data using Window.partitionBy() , and for row number and rank function we need to additionally order by on partition data using orderBy clause.\nClick on each link to know more about these functions along with the Scala examples.\n\nWINDOW FUNCTIONS USAGE & SYNTAX\tPYSPARK WINDOW FUNCTIONS DESCRIPTION\nrow_number(): Column\tReturns a sequential number starting from 1 within a window partition\nrank(): Column\tReturns the rank of rows within a window partition, with gaps.\npercent_rank(): Column\tReturns the percentile rank of rows within a window partition.\ndense_rank(): Column\tReturns the rank of rows within a window partition without any gaps. Where as Rank() returns rank with gaps.\nntile(n: Int): Column\tReturns the ntile id in a window partition\ncume_dist(): Column\tReturns the cumulative distribution of values within a window partition\nlag(e: Column, offset: Int): Column\nlag(columnName: String, offset: Int): Column\nlag(columnName: String, offset: Int, defaultValue: Any): Column\treturns the value that is `offset` rows before the current row, and `null` if there is less than `offset` rows before the current row.\nlead(columnName: String, offset: Int): Column\nlead(columnName: String, offset: Int): Column\nlead(columnName: String, offset: Int, defaultValue: Any): Column\treturns the value that is `offset` rows after the current row, and `null` if there is less than `offset` rows after the current row."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"c9cc5bd0-59fd-477b-8b96-ca42a5585e62"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["import pyspark\nfrom pyspark.sql import SparkSession\nspark = SparkSession.builder.appName(\"sparkbyexample.com\").getOrCreate()\nsimpleData = ((\"James\", \"Sales\", 3000), \\\n    (\"Michael\", \"Sales\", 4600),  \\\n    (\"Robert\", \"Sales\", 4100),   \\\n    (\"Maria\", \"Finance\", 3000),  \\\n    (\"James\", \"Sales\", 3000),    \\\n    (\"Scott\", \"Finance\", 3300),  \\\n    (\"Jen\", \"Finance\", 3900),    \\\n    (\"Jeff\", \"Marketing\", 3000), \\\n    (\"Kumar\", \"Marketing\", 2000),\\\n    (\"Saif\", \"Sales\", 4100) \\\n  )\ncolumns1 = [\"employee_name\",\"department\",\"salary\"]\ndf10 = spark.createDataFrame(data =simpleData, schema = columns1)\ndf10.show(truncate = False)\n"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"4b93c16e-88fb-4d99-9610-b7d5f7ae289d"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"+-------------+----------+------+\n|employee_name|department|salary|\n+-------------+----------+------+\n|James        |Sales     |3000  |\n|Michael      |Sales     |4600  |\n|Robert       |Sales     |4100  |\n|Maria        |Finance   |3000  |\n|James        |Sales     |3000  |\n|Scott        |Finance   |3300  |\n|Jen          |Finance   |3900  |\n|Jeff         |Marketing |3000  |\n|Kumar        |Marketing |2000  |\n|Saif         |Sales     |4100  |\n+-------------+----------+------+\n\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["+-------------+----------+------+\n|employee_name|department|salary|\n+-------------+----------+------+\n|James        |Sales     |3000  |\n|Michael      |Sales     |4600  |\n|Robert       |Sales     |4100  |\n|Maria        |Finance   |3000  |\n|James        |Sales     |3000  |\n|Scott        |Finance   |3300  |\n|Jen          |Finance   |3900  |\n|Jeff         |Marketing |3000  |\n|Kumar        |Marketing |2000  |\n|Saif         |Sales     |4100  |\n+-------------+----------+------+\n\n"]}}],"execution_count":0},{"cell_type":"code","source":["2. PySpark Window Ranking functions\n2.1 row_number Window Function\nrow_number() window function is used to give the sequential row number starting from 1 to the result of each window partition.\nfrom pyspark.sql.window import Window\nfrom pyspark.sql.functions import row_number\nwindowSpec  = Window.partitionBy(\"department\").orderBy(\"salary\")\n\ndf.withColumn(\"row_number\",row_number().over(windowSpec)) \\\n    .show(truncate=False)\n2.2 rank Window Function\nrank() window function is used to provide a rank to the result within a window partition. This function leaves gaps in rank when there are ties.\n\"\"\"rank\"\"\"\nfrom pyspark.sql.functions import rank\ndf.withColumn(\"rank\",rank().over(windowSpec)) \\\n    .show()\n2.3 dense_rank Window Function\ndense_rank() window function is used to get the result with rank of rows within a window partition without any gaps. This is similar to rank() function difference being rank function leaves gaps in rank when there are ties.\n\"\"\"dens_rank\"\"\"\nfrom pyspark.sql.functions import dense_rank\ndf.withColumn(\"dense_rank\",dense_rank().over(windowSpec)) \\\n    .show()\n2.4 percent_rank Window Function\n\"\"\" percent_rank \"\"\"\nfrom pyspark.sql.functions import percent_rank\ndf.withColumn(\"percent_rank\",percent_rank().over(windowSpec)) \\\n    .show()\n2.5 ntile Window Function\nntile() window function returns the relative rank of result rows within a window partition. In below example we have used 2 as an argument to ntile hence it returns ranking between 2 values (1 and 2)\n\"\"\"ntile\"\"\"\nfrom pyspark.sql.functions import ntile\ndf.withColumn(\"ntile\",ntile(2).over(windowSpec)) \\\n    .show()\n3. PySpark Window Analytic functions\n3.1 cume_dist Window Function\ncume_dist() window function is used to get the cumulative distribution of values within a window partition.\n\nThis is the same as the DENSE_RANK function in SQL.\n\"\"\" cume_dist \"\"\"\nfrom pyspark.sql.functions import cume_dist    \ndf.withColumn(\"cume_dist\",cume_dist().over(windowSpec)) \\\n   .show()\n3.2 lag Window Function\nThis is the same as the LAG function in SQL.\n\"\"\"lag\"\"\"\nfrom pyspark.sql.functions import lag    \ndf.withColumn(\"lag\",lag(\"salary\",2).over(windowSpec)) \\\n      .show()\n3.3 lead Window Function\nThis is the same as the LEAD function in SQL.\n \"\"\"lead\"\"\"\nfrom pyspark.sql.functions import lead    \ndf.withColumn(\"lead\",lead(\"salary\",2).over(windowSpec)) \\\n    .show()\n4. PySpark Window Aggregate Functions\nIn this section, I will explain how to calculate sum, min, max for each department using PySpark SQL Aggregate window functions and WindowSpec. When working with Aggregate functions, we don’t need to use order by clause.\nwindowSpecAgg  = Window.partitionBy(\"department\")\nfrom pyspark.sql.functions import col,avg,sum,min,max,row_number \ndf.withColumn(\"row\",row_number().over(windowSpec)) \\\n  .withColumn(\"avg\", avg(col(\"salary\")).over(windowSpecAgg)) \\\n  .withColumn(\"sum\", sum(col(\"salary\")).over(windowSpecAgg)) \\\n  .withColumn(\"min\", min(col(\"salary\")).over(windowSpecAgg)) \\\n  .withColumn(\"max\", max(col(\"salary\")).over(windowSpecAgg)) \\\n  .where(col(\"row\")==1).select(\"department\",\"avg\",\"sum\",\"min\",\"max\") \\\n  .show()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"0b077331-4975-49b1-b237-9ecaaef8b506"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["from pyspark.sql.window import Window\nfrom pyspark.sql.functions import row_number\nwindowspec=Window.partitionBy(\"department\").orderBy(\"salary\")\ndf10.show()\nwindowspec=Window.partitionBy(\"department\").orderBy(\"salary\")\ndf10.withColumn(\"row_number\",row_number().over(windowspec)).show(truncate=False)\n"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"464cbcbb-f8c8-43ac-9d8d-e19e0246e968"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)\n\u001B[0;32m<command-3085729503368848>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m      2\u001B[0m \u001B[0;32mfrom\u001B[0m \u001B[0mpyspark\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msql\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mfunctions\u001B[0m \u001B[0;32mimport\u001B[0m \u001B[0mrow_number\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      3\u001B[0m \u001B[0mwindowspec\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mWindow\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mpartitionBy\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"department\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0morderBy\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"salary\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 4\u001B[0;31m \u001B[0mdf10\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mshow\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      5\u001B[0m \u001B[0mwindowspec\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mWindow\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mpartitionBy\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"department\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0morderBy\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"salary\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      6\u001B[0m \u001B[0mdf10\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mwithColumn\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"row_number\"\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0mrow_number\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mover\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mwindowspec\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mshow\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mtruncate\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;32mFalse\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;31mNameError\u001B[0m: name 'df10' is not defined","errorSummary":"<span class='ansi-red-fg'>NameError</span>: name 'df10' is not defined","metadata":{},"errorTraceType":"ansi","type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/plain":["\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)\n\u001B[0;32m<command-3085729503368848>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m      2\u001B[0m \u001B[0;32mfrom\u001B[0m \u001B[0mpyspark\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msql\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mfunctions\u001B[0m \u001B[0;32mimport\u001B[0m \u001B[0mrow_number\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      3\u001B[0m \u001B[0mwindowspec\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mWindow\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mpartitionBy\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"department\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0morderBy\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"salary\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 4\u001B[0;31m \u001B[0mdf10\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mshow\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      5\u001B[0m \u001B[0mwindowspec\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mWindow\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mpartitionBy\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"department\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0morderBy\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"salary\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      6\u001B[0m \u001B[0mdf10\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mwithColumn\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"row_number\"\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0mrow_number\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mover\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mwindowspec\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mshow\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mtruncate\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;32mFalse\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;31mNameError\u001B[0m: name 'df10' is not defined"]}}],"execution_count":0},{"cell_type":"code","source":["from pyspark.sql.window import Window\nfrom pyspark.sql.functions import row_number,rank\nwindowsec1 = Window.partitionBy(\"department\").orderBy(\"salary\")\ndf10.withColumn(\"rank\",rank().over(windowsec1))\\\n    .withColumn(\"row_number\",row_number().over(windowsec1)).show(truncate = False)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"45b37c32-ec49-4f8a-b852-8ece255ea2a1"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)\n\u001B[0;32m<command-3085729503368849>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m      2\u001B[0m \u001B[0;32mfrom\u001B[0m \u001B[0mpyspark\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msql\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mfunctions\u001B[0m \u001B[0;32mimport\u001B[0m \u001B[0mrow_number\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0mrank\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      3\u001B[0m \u001B[0mwindowsec1\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mWindow\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mpartitionBy\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"department\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0morderBy\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"salary\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 4\u001B[0;31m \u001B[0mdf10\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mwithColumn\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"rank\"\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0mrank\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mover\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mwindowsec1\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;31m\\\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      5\u001B[0m     \u001B[0;34m.\u001B[0m\u001B[0mwithColumn\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"row_number\"\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0mrow_number\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mover\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mwindowsec1\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mshow\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mtruncate\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;32mFalse\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;31mNameError\u001B[0m: name 'df10' is not defined","errorSummary":"<span class='ansi-red-fg'>NameError</span>: name 'df10' is not defined","metadata":{},"errorTraceType":"ansi","type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/plain":["\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)\n\u001B[0;32m<command-3085729503368849>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m      2\u001B[0m \u001B[0;32mfrom\u001B[0m \u001B[0mpyspark\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msql\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mfunctions\u001B[0m \u001B[0;32mimport\u001B[0m \u001B[0mrow_number\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0mrank\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      3\u001B[0m \u001B[0mwindowsec1\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mWindow\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mpartitionBy\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"department\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0morderBy\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"salary\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 4\u001B[0;31m \u001B[0mdf10\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mwithColumn\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"rank\"\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0mrank\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mover\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mwindowsec1\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;31m\\\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      5\u001B[0m     \u001B[0;34m.\u001B[0m\u001B[0mwithColumn\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"row_number\"\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0mrow_number\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mover\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mwindowsec1\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mshow\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mtruncate\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;32mFalse\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;31mNameError\u001B[0m: name 'df10' is not defined"]}}],"execution_count":0},{"cell_type":"code","source":["from pyspark.sql.window import Window\nfrom pyspark.sql.functions import row_number,rank,dense_rank\nwindowspec2 = Window.partitionBy(\"department\").orderBy(\"salary\")\ndf10.withColumn(\"dense_rank\",dense_rank().over(windowspec2))\\\n    .withColumn(\"row_number\",row_number().over(windowspec2))\\\n    .withColumn(\"rank\",rank().over(windowspec2)).show(truncate = False)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"1c80af2d-e04f-42bd-9433-9c314ceba826"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"+-------------+----------+------+----------+----------+----+\n|employee_name|department|salary|dense_rank|row_number|rank|\n+-------------+----------+------+----------+----------+----+\n|Maria        |Finance   |3000  |1         |1         |1   |\n|Scott        |Finance   |3300  |2         |2         |2   |\n|Jen          |Finance   |3900  |3         |3         |3   |\n|Kumar        |Marketing |2000  |1         |1         |1   |\n|Jeff         |Marketing |3000  |2         |2         |2   |\n|James        |Sales     |3000  |1         |1         |1   |\n|James        |Sales     |3000  |1         |2         |1   |\n|Robert       |Sales     |4100  |2         |3         |3   |\n|Saif         |Sales     |4100  |2         |4         |3   |\n|Michael      |Sales     |4600  |3         |5         |5   |\n+-------------+----------+------+----------+----------+----+\n\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["+-------------+----------+------+----------+----------+----+\n|employee_name|department|salary|dense_rank|row_number|rank|\n+-------------+----------+------+----------+----------+----+\n|Maria        |Finance   |3000  |1         |1         |1   |\n|Scott        |Finance   |3300  |2         |2         |2   |\n|Jen          |Finance   |3900  |3         |3         |3   |\n|Kumar        |Marketing |2000  |1         |1         |1   |\n|Jeff         |Marketing |3000  |2         |2         |2   |\n|James        |Sales     |3000  |1         |1         |1   |\n|James        |Sales     |3000  |1         |2         |1   |\n|Robert       |Sales     |4100  |2         |3         |3   |\n|Saif         |Sales     |4100  |2         |4         |3   |\n|Michael      |Sales     |4600  |3         |5         |5   |\n+-------------+----------+------+----------+----------+----+\n\n"]}}],"execution_count":0},{"cell_type":"code","source":["from pyspark.sql.window import Window\nfrom pyspark.sql.functions import ntile,percent_rank,row_number,rank,dense_rank\nwindowspec = Window.partitionBy(\"department\").orderBy(\"salary\")\ndf10.withColumn(\"percent_rank\",percent_rank().over(windowspec))\\\n    .withColumn(\"dense_rank\",dense_rank().over(windowspec))\\\n    .withColumn(\"rank\",rank().over(windowspec))\\\n    .withColumn(\"row_number\",row_number().over(windowspec))\\\n    .withColumn(\"ntile\",ntile(3).over(windowspec)).show(truncate = False)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"4a320400-db75-486e-94b1-1c81261b1fc4"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)\n\u001B[0;32m<command-3085729503368852>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m      2\u001B[0m \u001B[0;32mfrom\u001B[0m \u001B[0mpyspark\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msql\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mfunctions\u001B[0m \u001B[0;32mimport\u001B[0m \u001B[0mntile\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0mpercent_rank\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0mrow_number\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0mrank\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0mdense_rank\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      3\u001B[0m \u001B[0mwindowspec\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mWindow\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mpartitionBy\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"department\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0morderBy\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"salary\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 4\u001B[0;31m \u001B[0mdf10\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mwithColumn\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"percent_rank\"\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0mpercent_rank\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mover\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mwindowspec\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;31m\\\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      5\u001B[0m     \u001B[0;34m.\u001B[0m\u001B[0mwithColumn\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"dense_rank\"\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0mdense_rank\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mover\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mwindowspec\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;31m\\\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      6\u001B[0m     \u001B[0;34m.\u001B[0m\u001B[0mwithColumn\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"rank\"\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0mrank\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mover\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mwindowspec\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;31m\\\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;31mNameError\u001B[0m: name 'df10' is not defined","errorSummary":"<span class='ansi-red-fg'>NameError</span>: name 'df10' is not defined","metadata":{},"errorTraceType":"ansi","type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/plain":["\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)\n\u001B[0;32m<command-3085729503368852>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m      2\u001B[0m \u001B[0;32mfrom\u001B[0m \u001B[0mpyspark\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msql\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mfunctions\u001B[0m \u001B[0;32mimport\u001B[0m \u001B[0mntile\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0mpercent_rank\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0mrow_number\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0mrank\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0mdense_rank\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      3\u001B[0m \u001B[0mwindowspec\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mWindow\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mpartitionBy\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"department\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0morderBy\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"salary\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 4\u001B[0;31m \u001B[0mdf10\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mwithColumn\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"percent_rank\"\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0mpercent_rank\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mover\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mwindowspec\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;31m\\\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      5\u001B[0m     \u001B[0;34m.\u001B[0m\u001B[0mwithColumn\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"dense_rank\"\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0mdense_rank\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mover\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mwindowspec\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;31m\\\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      6\u001B[0m     \u001B[0;34m.\u001B[0m\u001B[0mwithColumn\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"rank\"\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0mrank\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mover\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mwindowspec\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;31m\\\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;31mNameError\u001B[0m: name 'df10' is not defined"]}}],"execution_count":0},{"cell_type":"code","source":["from pyspark.sql.window import Window\nfrom pyspark.sql.functions import lag,lead,cume_dist,percent_rank,dense_rank,rank,row_number,ntile\nwindowspec3 =Window.partitionBy(\"department\").orderBy(\"salary\")\ndf10.withColumn(\"cume_dist\",cume_dist().over(windowspec3))\\\n    .withColumn(\"lead\",lead(\"salary\").over(windowspec3))\\\n    .withColumn(\"lead1\",lead(\"salary\",2).over(windowspec3))\\\n    .withColumn(\"lag\",lag(\"salary\").over(windowspec3))\\\n    .withColumn(\"lag1\",lag(\"salary\",2).over(windowspec3)).show(truncate = False)\n"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"a8d6adbc-3bea-4af2-aaba-2d3181bb01c1"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"+-------------+----------+------+------------------+----+-----+----+----+\n|employee_name|department|salary|cume_dist         |lead|lead1|lag |lag1|\n+-------------+----------+------+------------------+----+-----+----+----+\n|Maria        |Finance   |3000  |0.3333333333333333|3300|3900 |null|null|\n|Scott        |Finance   |3300  |0.6666666666666666|3900|null |3000|null|\n|Jen          |Finance   |3900  |1.0               |null|null |3300|3000|\n|Kumar        |Marketing |2000  |0.5               |3000|null |null|null|\n|Jeff         |Marketing |3000  |1.0               |null|null |2000|null|\n|James        |Sales     |3000  |0.4               |3000|4100 |null|null|\n|James        |Sales     |3000  |0.4               |4100|4100 |3000|null|\n|Robert       |Sales     |4100  |0.8               |4100|4600 |3000|3000|\n|Saif         |Sales     |4100  |0.8               |4600|null |4100|3000|\n|Michael      |Sales     |4600  |1.0               |null|null |4100|4100|\n+-------------+----------+------+------------------+----+-----+----+----+\n\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["+-------------+----------+------+------------------+----+-----+----+----+\n|employee_name|department|salary|cume_dist         |lead|lead1|lag |lag1|\n+-------------+----------+------+------------------+----+-----+----+----+\n|Maria        |Finance   |3000  |0.3333333333333333|3300|3900 |null|null|\n|Scott        |Finance   |3300  |0.6666666666666666|3900|null |3000|null|\n|Jen          |Finance   |3900  |1.0               |null|null |3300|3000|\n|Kumar        |Marketing |2000  |0.5               |3000|null |null|null|\n|Jeff         |Marketing |3000  |1.0               |null|null |2000|null|\n|James        |Sales     |3000  |0.4               |3000|4100 |null|null|\n|James        |Sales     |3000  |0.4               |4100|4100 |3000|null|\n|Robert       |Sales     |4100  |0.8               |4100|4600 |3000|3000|\n|Saif         |Sales     |4100  |0.8               |4600|null |4100|3000|\n|Michael      |Sales     |4600  |1.0               |null|null |4100|4100|\n+-------------+----------+------+------------------+----+-----+----+----+\n\n"]}}],"execution_count":0},{"cell_type":"code","source":["from pyspark.sql.window import Window\nfrom pyspark.sql.functions import avg,min,max,sum,row_number,col\nwindowspwc = Window.partitionBy(\"department\").orderBy(\"salary\")\ndf10.withColumn(\"row_number\",row_number().over(windowspwc))\\\n    .withColumn(\"avg\",avg(col(\"salary\")).over(windowspwc))\\\n    .withColumn(\"sum\",sum(col(\"salary\")).over(windowspwc))\\\n    .withColumn(\"min\",min(col(\"salary\")).over(windowspwc))\\\n    .withColumn(\"max\",max(col(\"salary\")).over(windowspwc))\\\n     .where(col(\"row_number\")==2).select(\"employee_name\",\"department\",\"salary\",\"row_number\").show(truncate = False)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"80caeb3a-db01-48ab-a324-2877b4c2d081"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)\n\u001B[0;32m<command-3085729503368854>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m      2\u001B[0m \u001B[0;32mfrom\u001B[0m \u001B[0mpyspark\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msql\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mfunctions\u001B[0m \u001B[0;32mimport\u001B[0m \u001B[0mavg\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0mmin\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0mmax\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0msum\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0mrow_number\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0mcol\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      3\u001B[0m \u001B[0mwindowspwc\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mWindow\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mpartitionBy\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"department\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0morderBy\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"salary\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 4\u001B[0;31m \u001B[0mdf10\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mwithColumn\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"row_number\"\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0mrow_number\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mover\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mwindowspwc\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;31m\\\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      5\u001B[0m     \u001B[0;34m.\u001B[0m\u001B[0mwithColumn\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"avg\"\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0mavg\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mcol\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"salary\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mover\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mwindowspwc\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;31m\\\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      6\u001B[0m     \u001B[0;34m.\u001B[0m\u001B[0mwithColumn\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"sum\"\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0msum\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mcol\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"salary\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mover\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mwindowspwc\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;31m\\\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;31mNameError\u001B[0m: name 'df10' is not defined","errorSummary":"<span class='ansi-red-fg'>NameError</span>: name 'df10' is not defined","metadata":{},"errorTraceType":"ansi","type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/plain":["\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)\n\u001B[0;32m<command-3085729503368854>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m      2\u001B[0m \u001B[0;32mfrom\u001B[0m \u001B[0mpyspark\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msql\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mfunctions\u001B[0m \u001B[0;32mimport\u001B[0m \u001B[0mavg\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0mmin\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0mmax\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0msum\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0mrow_number\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0mcol\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      3\u001B[0m \u001B[0mwindowspwc\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mWindow\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mpartitionBy\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"department\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0morderBy\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"salary\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 4\u001B[0;31m \u001B[0mdf10\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mwithColumn\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"row_number\"\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0mrow_number\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mover\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mwindowspwc\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;31m\\\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      5\u001B[0m     \u001B[0;34m.\u001B[0m\u001B[0mwithColumn\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"avg\"\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0mavg\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mcol\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"salary\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mover\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mwindowspwc\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;31m\\\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      6\u001B[0m     \u001B[0;34m.\u001B[0m\u001B[0mwithColumn\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"sum\"\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0msum\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mcol\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"salary\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mover\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mwindowspwc\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;31m\\\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;31mNameError\u001B[0m: name 'df10' is not defined"]}}],"execution_count":0},{"cell_type":"code","source":["PySpark SQL Date and Timestamp Functions\nPySpark Date and Timestamp Functions are supported on DataFrame and SQL queries and they work similarly to traditional SQL, Date and Time are very important if you are using PySpark for ETL. Most of all these functions accept input as, Date type, Timestamp type, or String. If a String used, it should be in a default format that can be cast to date.\n\nDateType default format is yyyy-MM-dd \nTimestampType default format is yyyy-MM-dd HH:mm:ss.SSSS\nReturns null if the input is a string that can not be cast to Date or Timestamp.\nPySpark SQL provides several Date & Timestamp functions hence keep an eye on and understand these. Always you should choose these functions instead of writing your own functions (UDF) as these functions are compile-time safe, handles null, and perform better when compared to PySpark UDF. If your PySpark application is critical on performance try to avoid using custom UDF at all costs as these are not guarantee performance.\nFor readable purposes, I’ve grouped these functions into the following groups.\n\nDate Functions\nTimestamp Functions\nDate and Timestamp Window Functions\nBefore you use any examples below, make sure you Create PySpark Sparksession and import SQL functions.\nfrom pyspark.sql.functions import *\nPySpark SQL Date Functions\nBelow are some of the PySpark SQL Date functions, these functions operate on the just Date.\n\nThe default format of the PySpark Date is yyyy-MM-dd.\n\nPYSPARK DATE FUNCTION\tDATE FUNCTION DESCRIPTION\ncurrent_date()\tReturns the current date as a date column.\ndate_format(dateExpr,format)\tConverts a date/timestamp/string to a value of string in the format specified by the date format given by the second argument.\nto_date()\tConverts the column into `DateType` by casting rules to `DateType`.\nto_date(column, fmt)\tConverts the column into a `DateType` with a specified format\nadd_months(Column, numMonths)\tReturns the date that is `numMonths` after `startDate`.\ndate_add(column, days)\ndate_sub(column, days)\tReturns the date that is `days` days after `start`\ndatediff(end, start)\tReturns the number of days from `start` to `end`.\nmonths_between(end, start)\tReturns number of months between dates `start` and `end`. A whole number is returned if both inputs have the same day of month or both are the last day of their respective months. Otherwise, the difference is calculated assuming 31 days per month.\nmonths_between(end, start, roundOff)\tReturns number of months between dates `end` and `start`. If `roundOff` is set to true, the result is rounded off to 8 digits; it is not rounded otherwise.\nnext_day(column, dayOfWeek)\tReturns the first date which is later than the value of the `date` column that is on the specified day of the week.\nFor example, `next_day('2015-07-27', \"Sunday\")` returns 2015-08-02 because that is the first Sunday after 2015-07-27.\ntrunc(column, format)\tReturns date truncated to the unit specified by the format.\nFor example, `trunc(\"2018-11-19 12:01:19\", \"year\")` returns 2018-01-01\nformat: 'year', 'yyyy', 'yy' to truncate by year,\n'month', 'mon', 'mm' to truncate by month\ndate_trunc(format, timestamp)\tReturns timestamp truncated to the unit specified by the format.\nFor example, `date_trunc(\"year\", \"2018-11-19 12:01:19\")` returns 2018-01-01 00:00:00\nformat: 'year', 'yyyy', 'yy' to truncate by year,\n'month', 'mon', 'mm' to truncate by month,\n'day', 'dd' to truncate by day,\nOther options are: 'second', 'minute', 'hour', 'week', 'month', 'quarter'\nyear(column)\tExtracts the year as an integer from a given date/timestamp/string\nquarter(column)\tExtracts the quarter as an integer from a given date/timestamp/string.\nmonth(column)\tExtracts the month as an integer from a given date/timestamp/string\ndayofweek(column)\tExtracts the day of the week as an integer from a given date/timestamp/string. Ranges from 1 for a Sunday through to 7 for a Saturday\ndayofmonth(column)\tExtracts the day of the month as an integer from a given date/timestamp/string.\ndayofyear(column)\tExtracts the day of the year as an integer from a given date/timestamp/string.\nweekofyear(column)\tExtracts the week number as an integer from a given date/timestamp/string. A week is considered to start on a Monday and week 1 is the first week with more than 3 days, as defined by ISO 8601\nlast_day(column)\tReturns the last day of the month which the given date belongs to. For example, input \"2015-07-27\" returns \"2015-07-31\" since July 31 is the last day of the month in July 2015.\nfrom_unixtime(column)\tConverts the number of seconds from unix epoch (1970-01-01 00:00:00 UTC) to a string representing the timestamp of that moment in the current system time zone in the yyyy-MM-dd HH:mm:ss format.\nfrom_unixtime(column, f)\tConverts the number of seconds from unix epoch (1970-01-01 00:00:00 UTC) to a string representing the timestamp of that moment in the current system time zone in the given format.\nunix_timestamp()\tReturns the current Unix timestamp (in seconds) as a long\nunix_timestamp(column)\tConverts time string in format yyyy-MM-dd HH:mm:ss to Unix timestamp (in seconds), using the default timezone and the default locale.\nunix_timestamp(column, p)\tConverts time string with given pattern to Unix timestamp (in seconds).\nPySpark SQL Timestamp Functions\nBelow are some of the PySpark SQL Timestamp functions, these functions operate on both date and timestamp values.\nPySpark SQL Timestamp Functions\nBelow are some of the PySpark SQL Timestamp functions, these functions operate on both date and timestamp values.\nThe default format of the Spark Timestamp is yyyy-MM-dd HH:mm:ss.SSSS\n\nPYSPARK TIMESTAMP FUNCTION SIGNATURE\tTIMESTAMP FUNCTION DESCRIPTION\ncurrent_timestamp ()\tReturns the current timestamp as a timestamp column\nhour(column)\tExtracts the hours as an integer from a given date/timestamp/string.\nminute(column)\tExtracts the minutes as an integer from a given date/timestamp/string.\nsecond(column)\tExtracts the seconds as an integer from a given date/timestamp/string.\nto_timestamp(column)\tConverts to a timestamp by casting rules to `TimestampType`.\nto_timestamp(column, fmt)\tConverts time string with the given pattern to timestamp.\nDate and Timestamp Window Functions\nBelow are PySpark Data and Timestamp window functions.\nDATE & TIME WINDOW FUNCTION SYNTAX\tDATE & TIME WINDOW FUNCTION DESCRIPTION\nwindow(timeColumn: Column, windowDuration: String,\nslideDuration: String, startTime: String): Column\tBucketize rows into one or more time windows given a timestamp specifying column. Window starts are inclusive but the window ends are exclusive, e.g. 12:05 will be in the window [12:05,12:10) but not in [12:00,12:05). Windows can support microsecond precision. Windows in the order of months are not supported.\nwindow(timeColumn: Column, windowDuration: String, slideDuration: String): Column\tBucketize rows into one or more time windows given a timestamp specifying column. Window starts are inclusive but the window ends are exclusive, e.g. 12:05 will be in the window [12:05,12:10) but not in [12:00,12:05). Windows can support microsecond precision. Windows in the order of months are not supported. The windows start beginning at 1970-01-01 00:00:00 UTC\nwindow(timeColumn: Column, windowDuration: String): Column\tGenerates tumbling time windows given a timestamp specifying column. Window starts are inclusive but the window ends are exclusive, e.g. 12:05 will be in the window [12:05,12:10) but not in [12:00,12:05). Windows can support microsecond precision. Windows in the order of months are not supported. The windows start beginning at 1970-01-01 00:00:00 UTC.\nPySpark SQL Date and Timestamp Functions Examples\nFollowing are the most used PySpark SQL Date and Timestamp Functions with examples, you can use these on DataFrame and SQL expressions."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"354237aa-e016-4b2a-b13d-48adc52059ac"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["import pyspark\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.functions import *\nspark = SparkSession.builder.appName(\"sparkby examples\").getOrCreate()\ndata99 = ( (\"1\",\"2020-02-01\"),(\"2\",\"2019-03-01\"),(\"3\",\"2021-12-31\"))\n#df11 = spark.createDataFrame(data = data99,schema=[\"id\",\"date\"])\ndf11 = spark.createDataFrame(data99 ,[\"id\",\"input\"])\ndf11.show()\ndf11.withColumn(\"current_date\",current_date().alias(\"currentdat\")).show()\ndf11.withColumn(\"date_format\",date_format(col(\"input\"),\"MM-dd-yyyy\")).show()\ndf11.withColumn(\"to_date\",to_date(col(\"input\"),\"yyyy-MM-dd\")).show()\ndf11.withColumn(\"datediff\",datediff(current_date(),col(\"input\"))).show()\ndf11.withColumn(\"months_between\",months_between(current_date(),col(\"input\"))).show()\n"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"c821ed1e-f916-4b4b-bae7-81dbb358727d"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"+---+----------+\n| id|     input|\n+---+----------+\n|  1|2020-02-01|\n|  2|2019-03-01|\n|  3|2021-12-31|\n+---+----------+\n\n+---+----------+------------+\n| id|     input|current_date|\n+---+----------+------------+\n|  1|2020-02-01|  2022-09-28|\n|  2|2019-03-01|  2022-09-28|\n|  3|2021-12-31|  2022-09-28|\n+---+----------+------------+\n\n+---+----------+-----------+\n| id|     input|date_format|\n+---+----------+-----------+\n|  1|2020-02-01| 02-01-2020|\n|  2|2019-03-01| 03-01-2019|\n|  3|2021-12-31| 12-31-2021|\n+---+----------+-----------+\n\n+---+----------+----------+\n| id|     input|   to_date|\n+---+----------+----------+\n|  1|2020-02-01|2020-02-01|\n|  2|2019-03-01|2019-03-01|\n|  3|2021-12-31|2021-12-31|\n+---+----------+----------+\n\n+---+----------+--------+\n| id|     input|datediff|\n+---+----------+--------+\n|  1|2020-02-01|     970|\n|  2|2019-03-01|    1307|\n|  3|2021-12-31|     271|\n+---+----------+--------+\n\n+---+----------+--------------+\n| id|     input|months_between|\n+---+----------+--------------+\n|  1|2020-02-01|   31.87096774|\n|  2|2019-03-01|   42.87096774|\n|  3|2021-12-31|    8.90322581|\n+---+----------+--------------+\n\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["+---+----------+\n| id|     input|\n+---+----------+\n|  1|2020-02-01|\n|  2|2019-03-01|\n|  3|2021-12-31|\n+---+----------+\n\n+---+----------+------------+\n| id|     input|current_date|\n+---+----------+------------+\n|  1|2020-02-01|  2022-09-28|\n|  2|2019-03-01|  2022-09-28|\n|  3|2021-12-31|  2022-09-28|\n+---+----------+------------+\n\n+---+----------+-----------+\n| id|     input|date_format|\n+---+----------+-----------+\n|  1|2020-02-01| 02-01-2020|\n|  2|2019-03-01| 03-01-2019|\n|  3|2021-12-31| 12-31-2021|\n+---+----------+-----------+\n\n+---+----------+----------+\n| id|     input|   to_date|\n+---+----------+----------+\n|  1|2020-02-01|2020-02-01|\n|  2|2019-03-01|2019-03-01|\n|  3|2021-12-31|2021-12-31|\n+---+----------+----------+\n\n+---+----------+--------+\n| id|     input|datediff|\n+---+----------+--------+\n|  1|2020-02-01|     970|\n|  2|2019-03-01|    1307|\n|  3|2021-12-31|     271|\n+---+----------+--------+\n\n+---+----------+--------------+\n| id|     input|months_between|\n+---+----------+--------------+\n|  1|2020-02-01|   31.87096774|\n|  2|2019-03-01|   42.87096774|\n|  3|2021-12-31|    8.90322581|\n+---+----------+--------------+\n\n"]}}],"execution_count":0},{"cell_type":"code","source":["df11.select(current_date().alias(\"current_date\")).show()\ndf11.select(col(\"input\"),date_format(col(\"input\"),\"dd-MM-yyyy\").alias(\"date_format\")).show()\ndf11.select(col(\"input\"),datediff(current_date(),col(\"input\")).alias(\"datediff\")).show()\ndf11.select(col(\"input\"),to_date(col(\"input\"),\"yyyy-MM-dd\").alias(\"to_date\")).show()\ndf11.select(col(\"input\"),months_between(current_date(),col(\"input\")).alias(\"months_between\")).show()\ndf11.select(col(\"input\"),trunc(col(\"input\"),\"month\").alias(\"month_trunc\"),\n            trunc(col(\"input\"),\"year\").alias(\"year_trunc\"),\n            trunc(col(\"input\"),\"day\").alias(\"Day_trunc\")).show()\ndf11.select(col(\"input\"),add_months(col(\"input\"),2).alias(\"add_month\"),\n            add_months(col(\"input\"),-2).alias(\"sub_months\"),\n            date_add(col(\"input\"),2).alias(\"date_add\"),\n            date_sub(col(\"input\"),2).alias(\"date_sub\")).show()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"ab9014f0-0cb8-49f6-b181-7d0cfc508b9c"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"+------------+\n|current_date|\n+------------+\n|  2022-09-27|\n|  2022-09-27|\n|  2022-09-27|\n+------------+\n\n+----------+-----------+\n|     input|date_format|\n+----------+-----------+\n|2020-02-01| 01-02-2020|\n|2019-03-01| 01-03-2019|\n|2021-12-31| 31-12-2021|\n+----------+-----------+\n\n+----------+--------+\n|     input|datediff|\n+----------+--------+\n|2020-02-01|     969|\n|2019-03-01|    1306|\n|2021-12-31|     270|\n+----------+--------+\n\n+----------+----------+\n|     input|   to_date|\n+----------+----------+\n|2020-02-01|2020-02-01|\n|2019-03-01|2019-03-01|\n|2021-12-31|2021-12-31|\n+----------+----------+\n\n+----------+--------------+\n|     input|months_between|\n+----------+--------------+\n|2020-02-01|   31.83870968|\n|2019-03-01|   42.83870968|\n|2021-12-31|    8.87096774|\n+----------+--------------+\n\n+----------+-----------+----------+---------+\n|     input|month_trunc|year_trunc|Day_trunc|\n+----------+-----------+----------+---------+\n|2020-02-01| 2020-02-01|2020-01-01|     null|\n|2019-03-01| 2019-03-01|2019-01-01|     null|\n|2021-12-31| 2021-12-01|2021-01-01|     null|\n+----------+-----------+----------+---------+\n\n+----------+----------+----------+----------+----------+\n|     input| add_month|sub_months|  date_add|  date_sub|\n+----------+----------+----------+----------+----------+\n|2020-02-01|2020-04-01|2019-12-01|2020-02-03|2020-01-30|\n|2019-03-01|2019-05-01|2019-01-01|2019-03-03|2019-02-27|\n|2021-12-31|2022-02-28|2021-10-31|2022-01-02|2021-12-29|\n+----------+----------+----------+----------+----------+\n\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["+------------+\n|current_date|\n+------------+\n|  2022-09-27|\n|  2022-09-27|\n|  2022-09-27|\n+------------+\n\n+----------+-----------+\n|     input|date_format|\n+----------+-----------+\n|2020-02-01| 01-02-2020|\n|2019-03-01| 01-03-2019|\n|2021-12-31| 31-12-2021|\n+----------+-----------+\n\n+----------+--------+\n|     input|datediff|\n+----------+--------+\n|2020-02-01|     969|\n|2019-03-01|    1306|\n|2021-12-31|     270|\n+----------+--------+\n\n+----------+----------+\n|     input|   to_date|\n+----------+----------+\n|2020-02-01|2020-02-01|\n|2019-03-01|2019-03-01|\n|2021-12-31|2021-12-31|\n+----------+----------+\n\n+----------+--------------+\n|     input|months_between|\n+----------+--------------+\n|2020-02-01|   31.83870968|\n|2019-03-01|   42.83870968|\n|2021-12-31|    8.87096774|\n+----------+--------------+\n\n+----------+-----------+----------+---------+\n|     input|month_trunc|year_trunc|Day_trunc|\n+----------+-----------+----------+---------+\n|2020-02-01| 2020-02-01|2020-01-01|     null|\n|2019-03-01| 2019-03-01|2019-01-01|     null|\n|2021-12-31| 2021-12-01|2021-01-01|     null|\n+----------+-----------+----------+---------+\n\n+----------+----------+----------+----------+----------+\n|     input| add_month|sub_months|  date_add|  date_sub|\n+----------+----------+----------+----------+----------+\n|2020-02-01|2020-04-01|2019-12-01|2020-02-03|2020-01-30|\n|2019-03-01|2019-05-01|2019-01-01|2019-03-03|2019-02-27|\n|2021-12-31|2022-02-28|2021-10-31|2022-01-02|2021-12-29|\n+----------+----------+----------+----------+----------+\n\n"]}}],"execution_count":0},{"cell_type":"code","source":["df11.select(col(\"input\"),year(col(\"input\")).alias(\"year\"),\n            month(col(\"input\")).alias(\"month\"),\n            next_day(col(\"input\"),\"tuesday\").alias(\"next_day\"),\n            weekofyear(col(\"input\")).alias(\"weekofyear\")).show(truncate = False)\n            "],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"48bd615a-f847-4c6c-b187-f5c0bd4547a2"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"+----------+----+-----+----------+----------+\n|input     |year|month|next_day  |weekofyear|\n+----------+----+-----+----------+----------+\n|2020-02-01|2020|2    |2020-02-04|5         |\n|2019-03-01|2019|3    |2019-03-05|9         |\n|2021-12-31|2021|12   |2022-01-04|52        |\n+----------+----+-----+----------+----------+\n\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["+----------+----+-----+----------+----------+\n|input     |year|month|next_day  |weekofyear|\n+----------+----+-----+----------+----------+\n|2020-02-01|2020|2    |2020-02-04|5         |\n|2019-03-01|2019|3    |2019-03-05|9         |\n|2021-12-31|2021|12   |2022-01-04|52        |\n+----------+----+-----+----------+----------+\n\n"]}}],"execution_count":0},{"cell_type":"code","source":["df11.select(col(\"input\"),dayofyear(col(\"input\")).alias(\"dayofyear\"),\n            dayofmonth(col(\"input\")).alias(\"dayofmonth\"),\n            dayofweek(col(\"input\")).alias(\"dayofweek\")\n           ).show(truncate = False)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"ab908db6-03d3-4edc-9af7-f983de462a9f"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"+----------+---------+----------+---------+\n|input     |dayofyear|dayofmonth|dayofweek|\n+----------+---------+----------+---------+\n|2020-02-01|32       |1         |7        |\n|2019-03-01|60       |1         |6        |\n|2021-12-31|365      |31        |6        |\n+----------+---------+----------+---------+\n\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["+----------+---------+----------+---------+\n|input     |dayofyear|dayofmonth|dayofweek|\n+----------+---------+----------+---------+\n|2020-02-01|32       |1         |7        |\n|2019-03-01|60       |1         |6        |\n|2021-12-31|365      |31        |6        |\n+----------+---------+----------+---------+\n\n"]}}],"execution_count":0},{"cell_type":"code","source":["data1=[[\"1\",\"02-01-2020 11 01 19 06\"],[\"2\",\"03-01-2019 12 01 19 406\"],[\"3\",\"03-01-2021 12 01 19 406\"]]\ndf12 = spark.createDataFrame(data1,[\"id\",\"input\"])\ndf12.show(truncate = False)\ndf12.select(current_timestamp().alias(\"currenttimestamp\")).show(1,truncate=False)\ndf12.select(col(\"input\"),to_timestamp(col(\"input\"),\"dd-MM-yyyy HH mm ss SSS\").alias(\"currenttimestamp\")).show(truncate = False)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"4c8f1327-90eb-4c21-a25c-f53c458e0c32"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"+---+-----------------------+\n|id |input                  |\n+---+-----------------------+\n|1  |02-01-2020 11 01 19 06 |\n|2  |03-01-2019 12 01 19 406|\n|3  |03-01-2021 12 01 19 406|\n+---+-----------------------+\n\n+-----------------------+\n|currenttimestamp       |\n+-----------------------+\n|2022-09-27 14:49:10.679|\n+-----------------------+\nonly showing top 1 row\n\n+-----------------------+-----------------------+\n|input                  |currenttimestamp       |\n+-----------------------+-----------------------+\n|02-01-2020 11 01 19 06 |2020-01-02 11:01:19.06 |\n|03-01-2019 12 01 19 406|2019-01-03 12:01:19.406|\n|03-01-2021 12 01 19 406|2021-01-03 12:01:19.406|\n+-----------------------+-----------------------+\n\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["+---+-----------------------+\n|id |input                  |\n+---+-----------------------+\n|1  |02-01-2020 11 01 19 06 |\n|2  |03-01-2019 12 01 19 406|\n|3  |03-01-2021 12 01 19 406|\n+---+-----------------------+\n\n+-----------------------+\n|currenttimestamp       |\n+-----------------------+\n|2022-09-27 14:49:10.679|\n+-----------------------+\nonly showing top 1 row\n\n+-----------------------+-----------------------+\n|input                  |currenttimestamp       |\n+-----------------------+-----------------------+\n|02-01-2020 11 01 19 06 |2020-01-02 11:01:19.06 |\n|03-01-2019 12 01 19 406|2019-01-03 12:01:19.406|\n|03-01-2021 12 01 19 406|2021-01-03 12:01:19.406|\n+-----------------------+-----------------------+\n\n"]}}],"execution_count":0},{"cell_type":"code","source":["  df12.select(col(\"input\"),hour(col(\"input\")).alias(\"hour\"),\n             minute(col(\"input\")).alias(\"minute\"),\n              second(col(\"input\")).alias(\"seconds\")\n             ).show(truncate = False)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"12f019e9-1f9d-43b3-a3af-00c6cf12df30"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"+-----------------------+----+------+-------+\n|input                  |hour|minute|seconds|\n+-----------------------+----+------+-------+\n|02-01-2020 11 01 19 06 |null|null  |null   |\n|03-01-2019 12 01 19 406|null|null  |null   |\n|03-01-2021 12 01 19 406|null|null  |null   |\n+-----------------------+----+------+-------+\n\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["+-----------------------+----+------+-------+\n|input                  |hour|minute|seconds|\n+-----------------------+----+------+-------+\n|02-01-2020 11 01 19 06 |null|null  |null   |\n|03-01-2019 12 01 19 406|null|null  |null   |\n|03-01-2021 12 01 19 406|null|null  |null   |\n+-----------------------+----+------+-------+\n\n"]}}],"execution_count":0},{"cell_type":"code","source":["PySpark JSON Functions with Examples\nPySpark JSON functions are used to query or extract the elements from JSON string of DataFrame column by path, convert it to struct, mapt type e.t.c, In this article, I will explain the most used JSON SQL functions with Python examples.\n\n1. PySpark JSON Functions\nfrom_json() – Converts JSON string into Struct type or Map type.\nto_json() – Converts MapType or Struct type to JSON string.\njson_tuple() – Extract the Data from JSON and create them as a new columns.\nget_json_object() – Extracts JSON element from a JSON string based on json path specified.\nschema_of_json() – Create schema string from JSON string\n1.1. Create DataFrame with Column contains JSON String\nIn order to explain these JSON functions first, let’s create DataFrame with a column contains JSON string.\nfrom pyspark.sql import SparkSession,Row\nspark = SparkSession.builder.appName('SparkByExamples.com').getOrCreate()\n\njsonString=\"\"\"{\"Zipcode\":704,\"ZipCodeType\":\"STANDARD\",\"City\":\"PARC PARQUE\",\"State\":\"PR\"}\"\"\"\ndf=spark.createDataFrame([(1, jsonString)],[\"id\",\"value\"])\ndf.show(truncate=False)\n2. PySpark JSON Functions Examples\n2.1. from_json()\nPySpark from_json() function is used to convert JSON string into Struct type or Map type. The below example converts JSON string to Map key-value pair. I will leave it to you to convert to struct type. Refer, Convert JSON string to Struct type column.\n\n\n#Convert JSON string column to Map type\nfrom pyspark.sql.types import MapType,StringType\nfrom pyspark.sql.functions import from_json\ndf2=df.withColumn(\"value\",from_json(df.value,MapType(StringType(),StringType())))\ndf2.printSchema()\ndf2.show(truncate=False)\n2.2. to_json()\nto_json() function is used to convert DataFrame columns MapType or Struct type to JSON string. Here, I am using df2 that created from above from_json() example.\n\nfrom pyspark.sql.functions import to_json,col\ndf2.withColumn(\"value\",to_json(col(\"value\"))) \\\n   .show(truncate=False)\n2.3. json_tuple()\nFunction json_tuple() is used the query or extract the elements from JSON column and create the result as a new columns.\nfrom pyspark.sql.functions import json_tuple\ndf.select(col(\"id\"),json_tuple(col(\"value\"),\"Zipcode\",\"ZipCodeType\",\"City\")) \\\n    .toDF(\"id\",\"Zipcode\",\"ZipCodeType\",\"City\") \\\n    .show(truncate=False)\n2.4. get_json_object()\nget_json_object() is used to extract the JSON string based on path from the JSON column.\nfrom pyspark.sql.functions import get_json_object\ndf.select(col(\"id\"),get_json_object(col(\"value\"),\"$.ZipCodeType\").alias(\"ZipCodeType\")) \\\n    .show(truncate=False)\n\n2.5. schema_of_json()\nUse schema_of_json() to create schema string from JSON string column.\nfrom pyspark.sql.functions import schema_of_json,lit\nschemaStr=spark.range(1) \\\n    .select(schema_of_json(lit(\"\"\"{\"Zipcode\":704,\"ZipCodeType\":\"STANDARD\",\"City\":\"PARC PARQUE\",\"State\":\"PR\"}\"\"\"))) \\\n    .collect()[0][0]\nprint(schemaStr)\n\n\nhttps://sparkbyexamples.com/spark/spark-streaming-read-json-files-from-directory/"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"dcd6a92d-1046-466d-869f-11383258833f"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["import pyspark\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.types import StringType,MapType,StructType\nfrom pyspark.sql.functions import *\n\nspark = SparkSession.builder.appName(\"pysparkByexamples.com\").getOrCreate()\njsonString = \"\"\"{\"zipcode\":704,\"zipCodetype\":\"STANDARD\",\"city\":\"PARC PARQUE\",\"state\":\"PR\"}\"\"\"\ndf33 = spark.createDataFrame([(1,jsonString)],[\"id\",\"value\"])\ndf33.printSchema()\ndf33.show(truncate = False)\n\ndf44=df33.withColumn(\"value\",from_json(df33.value,MapType(StringType(),StringType())))\ndf44.printSchema()\ndf44.show(truncate = False)\n\n#val schema = new StructType()\n#            .add(\"zipcode\",StringType,true)\n #           .add(\"zipCodetype\",StringType,true)\n  #          .add(\"city\",StringType,true)\n   #         .add(\"state\",StringType,true)\n#val df55 = df33.withColumn(\"value\",from_json(df33.value,schema))\n#df55.printSchema()\n#df55.show(truncate = False)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"d866d417-3bd7-4d42-ae3d-6618ece3cfb1"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"root\n |-- id: long (nullable = true)\n |-- value: string (nullable = true)\n\n+---+--------------------------------------------------------------------------+\n|id |value                                                                     |\n+---+--------------------------------------------------------------------------+\n|1  |{\"zipcode\":704,\"zipCodetype\":\"STANDARD\",\"city\":\"PARC PARQUE\",\"state\":\"PR\"}|\n+---+--------------------------------------------------------------------------+\n\nroot\n |-- id: long (nullable = true)\n |-- value: map (nullable = true)\n |    |-- key: string\n |    |-- value: string (valueContainsNull = true)\n\n+---+---------------------------------------------------------------------------+\n|id |value                                                                      |\n+---+---------------------------------------------------------------------------+\n|1  |{zipcode -> 704, zipCodetype -> STANDARD, city -> PARC PARQUE, state -> PR}|\n+---+---------------------------------------------------------------------------+\n\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["root\n |-- id: long (nullable = true)\n |-- value: string (nullable = true)\n\n+---+--------------------------------------------------------------------------+\n|id |value                                                                     |\n+---+--------------------------------------------------------------------------+\n|1  |{\"zipcode\":704,\"zipCodetype\":\"STANDARD\",\"city\":\"PARC PARQUE\",\"state\":\"PR\"}|\n+---+--------------------------------------------------------------------------+\n\nroot\n |-- id: long (nullable = true)\n |-- value: map (nullable = true)\n |    |-- key: string\n |    |-- value: string (valueContainsNull = true)\n\n+---+---------------------------------------------------------------------------+\n|id |value                                                                      |\n+---+---------------------------------------------------------------------------+\n|1  |{zipcode -> 704, zipCodetype -> STANDARD, city -> PARC PARQUE, state -> PR}|\n+---+---------------------------------------------------------------------------+\n\n"]}}],"execution_count":0},{"cell_type":"code","source":["df55 = df44.withColumn(\"value\",to_json(df44.value))\ndf55.show(truncate = False)\n\njsonString=\"\"\"{\"Zipcode\":704,\"ZipCodeType\":\"STANDARD\",\"City\":\"PARC PARQUE\",\"State\":\"PR\"}\"\"\"\ndf=spark.createDataFrame([(1, jsonString)],[\"id\",\"value\"])\n\ndf.select(col(\"id\"),json_tuple(col(\"value\"),\"Zipcode\",\"ZipCodeType\",\"City\",\"State\"))\\\n            .toDF(\"id\",\"zipcode\",\"zipCodetype\",\"city\",\"state\")\\\n            .show(truncate = False)\n\ndf.select(col(\"id\"),get_json_object(col(\"value\"),\"$.ZipCodeType\").alias(\"zipCodetype\")).show(truncate = False)\n"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"3400aea3-1593-4a17-bff2-16751a4892cd"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"+---+----------------------------------------------------------------------------+\n|id |value                                                                       |\n+---+----------------------------------------------------------------------------+\n|1  |{\"zipcode\":\"704\",\"zipCodetype\":\"STANDARD\",\"city\":\"PARC PARQUE\",\"state\":\"PR\"}|\n+---+----------------------------------------------------------------------------+\n\n+---+-------+-----------+-----------+-----+\n|id |zipcode|zipCodetype|city       |state|\n+---+-------+-----------+-----------+-----+\n|1  |704    |STANDARD   |PARC PARQUE|PR   |\n+---+-------+-----------+-----------+-----+\n\n+---+-----------+\n|id |zipCodetype|\n+---+-----------+\n|1  |STANDARD   |\n+---+-----------+\n\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["+---+----------------------------------------------------------------------------+\n|id |value                                                                       |\n+---+----------------------------------------------------------------------------+\n|1  |{\"zipcode\":\"704\",\"zipCodetype\":\"STANDARD\",\"city\":\"PARC PARQUE\",\"state\":\"PR\"}|\n+---+----------------------------------------------------------------------------+\n\n+---+-------+-----------+-----------+-----+\n|id |zipcode|zipCodetype|city       |state|\n+---+-------+-----------+-----------+-----+\n|1  |704    |STANDARD   |PARC PARQUE|PR   |\n+---+-------+-----------+-----------+-----+\n\n+---+-----------+\n|id |zipCodetype|\n+---+-----------+\n|1  |STANDARD   |\n+---+-----------+\n\n"]}}],"execution_count":0},{"cell_type":"code","source":["1. PySpark UDF Introduction\n1.1 What is UDF?\nUDF’s a.k.a User Defined Functions, If you are coming from SQL background, UDF’s are nothing new to you as most of the traditional RDBMS databases support User Defined Functions, these functions need to register in the database library and use them on SQL as regular functions.\n\nPySpark UDF’s are similar to UDF on traditional databases. In PySpark, you create a function in a Python syntax and wrap it with PySpark SQL udf() or register it as udf and use it on DataFrame and SQL respectively.\n\n1.2 Why do we need a UDF?\nUDF’s are used to extend the functions of the framework and re-use these functions on multiple DataFrame’s. For example, you wanted to convert every first letter of a word in a name string to a capital case; PySpark build-in features don’t have this function hence you can create it a UDF and reuse this as needed on many Data Frames. UDF’s are once created they can be re-used on several DataFrame’s and SQL expressions.\n\nBefore you create any UDF, do your research to check if the similar function you wanted is already available in Spark SQL Functions. PySpark SQL provides several predefined common functions and many more new functions are added with every release. hence, It is best to check before you reinventing the wheel.\nWhen you creating UDF’s you need to design them very carefully otherwise you will come across optimization & performance issues.\n\n2. Create PySpark UDF\n2.1 Create a DataFrame\nBefore we jump in creating a UDF, first let’s create a PySpark DataFrame.\nspark = SparkSession.builder.appName('SparkByExamples.com').getOrCreate()\n\ncolumns = [\"Seqno\",\"Name\"]\ndata = [(\"1\", \"john jones\"),\n    (\"2\", \"tracey smith\"),\n    (\"3\", \"amy sanders\")]\n\ndf = spark.createDataFrame(data=data,schema=columns)\n\ndf.show(truncate=False)\n2.2 Create a Python Function\nThe first step in creating a UDF is creating a Python function. Below snippet creates a function convertCase() which takes a string parameter and converts the first letter of every word to capital letter. UDF’s take parameters of your choice and returns a value.\n\n\ndef convertCase(str):\n    resStr=\"\"\n    arr = str.split(\" \")\n    for x in arr:\n       resStr= resStr + x[0:1].upper() + x[1:len(x)] + \" \"\n    return resStr \n2.3 Convert a Python function to PySpark UDF\nNow convert this function convertCase() to UDF by passing the function to PySpark SQL udf(), this function is available at org.apache.spark.sql.functions.udf package. Make sure you import this package before using it.\n\nPySpark SQL udf() function returns org.apache.spark.sql.expressions.UserDefinedFunction class object.\n\n\n\"\"\" Converting function to UDF \"\"\"\nconvertUDF = udf(lambda z: convertCase(z),StringType())\nNote: The default type of the udf() is StringType hence, you can also write the above statement without return type.\n\n\"\"\" Converting function to UDF \nStringType() is by default hence not required \"\"\"\nconvertUDF = udf(lambda z: convertCase(z)) \n3. Using UDF with DataFrame\n3.1 Using UDF with PySpark DataFrame select()\nNow you can use convertUDF() on a DataFrame column as a regular build-in function.\n\n\ndf.select(col(\"Seqno\"), \\\n    convertUDF(col(\"Name\")).alias(\"Name\") ) \\\n   .show(truncate=False)\n3.2 Using UDF with PySpark DataFrame withColumn()\nYou could also use udf on DataFrame withColumn() function, to explain this I will create another upperCase() function which converts the input string to upper case.\n\n\ndef upperCase(str):\n    return str.upper()\nLet’s convert upperCase() python function to UDF and then use it with DataFrame withColumn(). Below example converts the values of “Name” column to upper case and creates a new column “Curated Name”\n\n\nupperCaseUDF = udf(lambda z:upperCase(z),StringType())   \n\ndf.withColumn(\"Cureated Name\", upperCaseUDF(col(\"Name\"))) \\\n  .show(truncate=False)\n3.3 Registering PySpark UDF & use it on SQL\nIn order to use convertCase() function on PySpark SQL, you need to register the function with PySpark by using spark.udf.register().\n\n\n\"\"\" Using UDF on SQL \"\"\"\nspark.udf.register(\"convertUDF\", convertCase,StringType())\ndf.createOrReplaceTempView(\"NAME_TABLE\")\nspark.sql(\"select Seqno, convertUDF(Name) as Name from NAME_TABLE\") \\\n     .show(truncate=False)\nThis yields the same output as 3.1 example.\n\n4. Creating UDF using annotation\nIn the previous sections, you have learned creating a UDF is a 2 step process, first, you need to create a Python function, second convert function to UDF using SQL udf() function, however, you can avoid these two steps and create it with just a single step by using annotations.\n\n\n@udf(returnType=StringType()) \ndef upperCase(str):\n    return str.upper()\n\ndf.withColumn(\"Cureated Name\", upperCase(col(\"Name\"))) \\\n.show(truncate=False)\nThis results same output as section 3.2\n5. Special Handling\n5.1 Execution order\nOne thing to aware is in PySpark/Spark does not guarantee the order of evaluation of subexpressions meaning expressions are not guarantee to evaluated left-to-right or in any other fixed order. PySpark reorders the execution for query optimization and planning hence, AND, OR, WHERE and HAVING expression will have side effects.\n\nSo when you are designing and using UDF, you have to be very careful especially with null handling as these results runtime exceptions.\n\n\n\"\"\" \nNo guarantee Name is not null will execute first\nIf convertUDF(Name) like '%John%' execute first then \nyou will get runtime error\n\"\"\"\nspark.sql(\"select Seqno, convertUDF(Name) as Name from NAME_TABLE \" + \\ \n         \"where Name is not null and convertUDF(Name) like '%John%'\") \\\n     .show(truncate=False)  \n5.2 Handling null check\nUDF’s are error-prone when not designed carefully. for example, when you have a column that contains the value null on some records\n\n\n\"\"\" null check \"\"\"\n\ncolumns = [\"Seqno\",\"Name\"]\ndata = [(\"1\", \"john jones\"),\n    (\"2\", \"tracey smith\"),\n    (\"3\", \"amy sanders\"),\n    ('4',None)]\n\ndf2 = spark.createDataFrame(data=data,schema=columns)\ndf2.show(truncate=False)\ndf2.createOrReplaceTempView(\"NAME_TABLE2\")\n\nspark.sql(\"select convertUDF(Name) from NAME_TABLE2\") \\\n     .show(truncate=False)\nNote that from the above snippet, record with “Seqno 4” has value “None” for “name” column. Since we are not handling null with UDF function, using this on DataFrame returns below error. Note that in Python None is considered null.\n\n\nAttributeError: 'NoneType' object has no attribute 'split'\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:456)\n\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:81)\n\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:64)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:410)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440)\nBelow points to remember\n\nIts always best practice to check for null inside a UDF function rather than checking for null outside.\nIn any case, if you can’t do a null check in UDF at lease use IF or CASE WHEN to check for null and call UDF conditionally.\n\nspark.udf.register(\"_nullsafeUDF\", lambda str: convertCase(str) if not str is None else \"\" , StringType())\n\nspark.sql(\"select _nullsafeUDF(Name) from NAME_TABLE2\") \\\n     .show(truncate=False)\n\nspark.sql(\"select Seqno, _nullsafeUDF(Name) as Name from NAME_TABLE2 \" + \\\n          \" where Name is not null and _nullsafeUDF(Name) like '%John%'\") \\\n     .show(truncate=False)    \nThis executes successfully without errors as we are checking for null/none while registering UDF.\n\n5.3 Performance concern using UDF\nUDF’s are a black box to PySpark hence it can’t apply optimization and you will lose all the optimization PySpark does on Dataframe/Dataset. When possible you should use Spark SQL built-in functions as these functions provide optimization. Consider creating UDF only when existing built-in SQL function doesn’t have it.\n"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"438a4e3d-6d3f-45ff-b616-517d522a4b3a"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["import pyspark \nfrom pyspark.sql import SparkSession\nspark = SparkSession.builder.appName(\"sparkbyExample\").getOrCreate()\ncolumns=[\"seqno\",\"name\"]\ndata2 = [(\"1\",\"john jones\"),\n    (\"2\", \"tracey smith\"),\n    (\"3\", \"amy sanders\")]\ndf55 = spark.createDataFrame(data = data2,schema = columns)\ndf55.show()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"0dd7a596-6be8-4c27-941c-905cd5e8ce12"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"+-----+------------+\n|seqno|        name|\n+-----+------------+\n|    1|  john jones|\n|    2|tracey smith|\n|    3| amy sanders|\n+-----+------------+\n\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["+-----+------------+\n|seqno|        name|\n+-----+------------+\n|    1|  john jones|\n|    2|tracey smith|\n|    3| amy sanders|\n+-----+------------+\n\n"]}}],"execution_count":0},{"cell_type":"code","source":["from pyspark.sql.functions import udf\n\ndef convertCase(str):\nresStr = \"\"\narr = str.split(\" \")\nfor x in arr :\n    resStr = resStr + x[0:1].upper() + x[1:len(x)] + \" \"\nreturn resStr\n\n"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"8b64c8e5-f654-4ab2-a657-2a58eee5a10b"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"\u001B[0;36m  File \u001B[0;32m\"<command-860607162882995>\"\u001B[0;36m, line \u001B[0;32m4\u001B[0m\n\u001B[0;31m    resStr = \"\"\u001B[0m\n\u001B[0m    ^\u001B[0m\n\u001B[0;31mIndentationError\u001B[0m\u001B[0;31m:\u001B[0m expected an indented block\n","errorSummary":"<span class='ansi-red-fg'>IndentationError</span>: expected an indented block (<command-860607162882995>, line 4)","metadata":{},"errorTraceType":"ansi","type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/plain":["\u001B[0;36m  File \u001B[0;32m\"<command-860607162882995>\"\u001B[0;36m, line \u001B[0;32m4\u001B[0m\n\u001B[0;31m    resStr = \"\"\u001B[0m\n\u001B[0m    ^\u001B[0m\n\u001B[0;31mIndentationError\u001B[0m\u001B[0;31m:\u001B[0m expected an indented block\n"]}}],"execution_count":0},{"cell_type":"code","source":["def convertCase(str):\n    resStr=\"\"\n    arr = str.split(\" \")\n    for x in arr:\n       resStr= resStr + x[0:1].upper() + x[1:len(x)] + \" \"\n    return resStr"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"024577d7-d107-4bc4-8b77-46af86c5816d"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["from pyspark.sql.types import StringType\nconvertUDF = udf(lambda z: convertCase(z),StringType())"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"3dec5c7d-b0ff-4f95-a67c-df995498681b"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["from pyspark.sql.functions import col,udf\ndf55.select(col(\"seqno\"),convertUDF(col(\"name\")).alias(\"name\"))\ndf55.show(truncate = False)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"f27442cc-c120-45ce-b2be-2a182e190a90"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"+-----+------------+\n|seqno|name        |\n+-----+------------+\n|1    |john jones  |\n|2    |tracey smith|\n|3    |amy sanders |\n+-----+------------+\n\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["+-----+------------+\n|seqno|name        |\n+-----+------------+\n|1    |john jones  |\n|2    |tracey smith|\n|3    |amy sanders |\n+-----+------------+\n\n"]}}],"execution_count":0},{"cell_type":"code","source":[""],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"be51b974-49ce-4aa3-bd19-c726d3f19a26"}},"outputs":[],"execution_count":0}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"PySpark SQL Functions","dashboards":[],"notebookMetadata":{"pythonIndentUnit":4},"language":"python","widgets":{},"notebookOrigID":755754218939065}},"nbformat":4,"nbformat_minor":0}
