{"cells":[{"cell_type":"code","source":["PySpark Aggregate Functions with Examples\nPySpark provides built-in standard Aggregate functions defines in DataFrame API, these come in handy when we need to make aggregate operations on DataFrame columns. Aggregate functions operate on a group of rows and calculate a single return value for every group.\n\nAll these aggregate functions accept input as, Column type or column name in a string and several other arguments based on the function and return Column type.\nWhen possible try to leverage standard library as they are little bit more compile-time safety, handles null and perform better when compared to UDF’s. If your application is critical on performance try to avoid using custom UDF at all costs as these are not guarantee on performance.\n\nPySpark Aggregate Functions\nPySpark SQL Aggregate functions are grouped as “agg_funcs” in Pyspark. Below is a list of functions defined under this group. Click on each link to learn with example.\nPySpark Aggregate Functions Examples\nFirst, let’s create a DataFrame to work with PySpark aggregate functions. All examples provided here are also available at PySpark Examples GitHub project.\nsimpleData = [(\"James\", \"Sales\", 3000),\n    (\"Michael\", \"Sales\", 4600),\n    (\"Robert\", \"Sales\", 4100),\n    (\"Maria\", \"Finance\", 3000),\n    (\"James\", \"Sales\", 3000),\n    (\"Scott\", \"Finance\", 3300),\n    (\"Jen\", \"Finance\", 3900),\n    (\"Jeff\", \"Marketing\", 3000),\n    (\"Kumar\", \"Marketing\", 2000),\n    (\"Saif\", \"Sales\", 4100)\n  ]\nschema = [\"employee_name\", \"department\", \"salary\"]\ndf = spark.createDataFrame(data=simpleData, schema = schema)\ndf.printSchema()\ndf.show(truncate=False)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"dfbaf0fa-b4af-4985-b7b8-29b180d14019"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["approx_count_distinct Aggregate Function\nIn PySpark approx_count_distinct() function returns the count of distinct items in a group.\n//approx_count_distinct()\nprint(\"approx_count_distinct: \" + \\\n      str(df.select(approx_count_distinct(\"salary\")).collect()[0][0]))\n\n//Prints approx_count_distinct: 6\n\navg (average) Aggregate Function\navg() function returns the average of values in the input column.\n//avg\nprint(\"avg: \" + str(df.select(avg(\"salary\")).collect()[0][0]))\n\n//Prints avg: 3400.0\n\ncollect_list Aggregate Function\ncollect_list() function returns all values from an input column with duplicates.\n\ncountDistinct Aggregate Function\ncountDistinct() function returns the number of distinct elements in a columns\n\ncount function\ncount() function returns number of elements in a column.\nprint(\"count: \"+str(df.select(count(\"salary\")).collect()[0]))\n\nPrints county: 10\n\ngrouping function\ngrouping() Indicates whether a given input column is aggregated or not. returns 1 for aggregated or 0 for not aggregated in the result. If you try grouping directly on the salary column you will get below error.\nException in thread \"main\" org.apache.spark.sql.AnalysisException:\n  // grouping() can only be used with GroupingSets/Cube/Rollup\nfirst function\nfirst() function returns the first element in a column when ignoreNulls is set to true, it returns the first non-null element.\n\n//first\ndf.select(first(\"salary\")).show(truncate=False)\nlast function\nlast() function returns the last element in a column. when ignoreNulls is set to true, it returns the last non-null element.\n//last\ndf.select(last(\"salary\")).show(truncate=False)\nmax function\nmax() function returns the maximum value in a column.\ndf.select(max(\"salary\")).show(truncate=False)\nmin function\nmin() function\ndf.select(min(\"salary\")).show(truncate=False)\nsum function\nsum() function Returns the sum of all values in a column.\ndf.select(sum(\"salary\")).show(truncate=False)\nsumDistinct function\nsumDistinct() function returns the sum of all distinct values in a column.\ndf.select(sumDistinct(\"salary\")).show(truncate=False)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"117abab3-60d7-4420-97fc-fbe79c794b70"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["import pyspark \nfrom pyspark.sql.functions import col,avg,approx_count_distinct,collect_list,collect_set,countDistinct,count,first,last,min,max,sum,sum_distinct\nsimpleData = [(\"James\", \"Sales\", 3000),\n    (\"Michael\", \"Sales\", 4600),\n    (\"Robert\", \"Sales\", 4100),\n    (\"Maria\", \"Finance\", 3000),\n    (\"James\", \"Sales\", 3000),\n    (\"Scott\", \"Finance\", 3300),\n    (\"Jen\", \"Finance\", 3900),\n    (\"Jeff\", \"Marketing\", 3000),\n    (\"Kumar\", \"Marketing\", 2000),\n    (\"Saif\", \"Sales\", 4100)\n  ]\nschema = [\"employee_name\", \"department\", \"salary\"]\ndf=spark.createDataFrame(data = simpleData,schema = schema)\ndf.show()\nprint(\"approx_count_distinct: \"+\\\n    str(df.select(approx_count_distinct(\"salary\")).collect()[0][0]\n     ))\n\nprint(\"avg: \"+str(df.select(avg(\"salary\")).collect()[0][0]))\n\ndf.select(collect_list(\"salary\")).show(truncate = False)\ndf.select(collect_set(\"salary\")).show(truncate = False)\ndf.select(countDistinct(\"department\",\"salary\")).show(truncate = False)\nprint(\"countDistinct:\"+str(df.select(countDistinct(\"department\",\"salary\")).collect()[0][0]))\nprint(\"count:\"+str(df.select(count(\"salary\")).collect()[0][0]))\ndf.select(first(\"salary\")).show()\ndf.select(last(\"salary\")).show()\ndf.select(min(\"salary\")).show()\ndf.select(max(\"salary\")).show()\ndf.select(sum(\"salary\")).show()\ndf.select(sum_distinct(\"salary\")).show()\n\n"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"fa905d94-9ab4-475c-8b05-c0ed85c9a413"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"+-------------+----------+------+\n|employee_name|department|salary|\n+-------------+----------+------+\n|        James|     Sales|  3000|\n|      Michael|     Sales|  4600|\n|       Robert|     Sales|  4100|\n|        Maria|   Finance|  3000|\n|        James|     Sales|  3000|\n|        Scott|   Finance|  3300|\n|          Jen|   Finance|  3900|\n|         Jeff| Marketing|  3000|\n|        Kumar| Marketing|  2000|\n|         Saif|     Sales|  4100|\n+-------------+----------+------+\n\napprox_count_distinct: 6\navg: 3400.0\n+------------------------------------------------------------+\n|collect_list(salary)                                        |\n+------------------------------------------------------------+\n|[3000, 4600, 4100, 3000, 3000, 3300, 3900, 3000, 2000, 4100]|\n+------------------------------------------------------------+\n\n+------------------------------------+\n|collect_set(salary)                 |\n+------------------------------------+\n|[4600, 3000, 3900, 4100, 3300, 2000]|\n+------------------------------------+\n\n+----------------------------------+\n|count(DISTINCT department, salary)|\n+----------------------------------+\n|8                                 |\n+----------------------------------+\n\ncountDistinct:8\ncount:10\n+-------------+\n|first(salary)|\n+-------------+\n|         3000|\n+-------------+\n\n+------------+\n|last(salary)|\n+------------+\n|        4100|\n+------------+\n\n+-----------+\n|min(salary)|\n+-----------+\n|       2000|\n+-----------+\n\n+-----------+\n|max(salary)|\n+-----------+\n|       4600|\n+-----------+\n\n+-----------+\n|sum(salary)|\n+-----------+\n|      34000|\n+-----------+\n\n+--------------------+\n|sum(DISTINCT salary)|\n+--------------------+\n|               20900|\n+--------------------+\n\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["+-------------+----------+------+\n|employee_name|department|salary|\n+-------------+----------+------+\n|        James|     Sales|  3000|\n|      Michael|     Sales|  4600|\n|       Robert|     Sales|  4100|\n|        Maria|   Finance|  3000|\n|        James|     Sales|  3000|\n|        Scott|   Finance|  3300|\n|          Jen|   Finance|  3900|\n|         Jeff| Marketing|  3000|\n|        Kumar| Marketing|  2000|\n|         Saif|     Sales|  4100|\n+-------------+----------+------+\n\napprox_count_distinct: 6\navg: 3400.0\n+------------------------------------------------------------+\n|collect_list(salary)                                        |\n+------------------------------------------------------------+\n|[3000, 4600, 4100, 3000, 3000, 3300, 3900, 3000, 2000, 4100]|\n+------------------------------------------------------------+\n\n+------------------------------------+\n|collect_set(salary)                 |\n+------------------------------------+\n|[4600, 3000, 3900, 4100, 3300, 2000]|\n+------------------------------------+\n\n+----------------------------------+\n|count(DISTINCT department, salary)|\n+----------------------------------+\n|8                                 |\n+----------------------------------+\n\ncountDistinct:8\ncount:10\n+-------------+\n|first(salary)|\n+-------------+\n|         3000|\n+-------------+\n\n+------------+\n|last(salary)|\n+------------+\n|        4100|\n+------------+\n\n+-----------+\n|min(salary)|\n+-----------+\n|       2000|\n+-----------+\n\n+-----------+\n|max(salary)|\n+-----------+\n|       4600|\n+-----------+\n\n+-----------+\n|sum(salary)|\n+-----------+\n|      34000|\n+-----------+\n\n+--------------------+\n|sum(DISTINCT salary)|\n+--------------------+\n|               20900|\n+--------------------+\n\n"]}}],"execution_count":0},{"cell_type":"code","source":["PySpark Window Functions\nPySpark Window functions are used to calculate results such as the rank, row number e.t.c over a range of input rows. In this article, I’ve explained the concept of window functions, syntax, and finally how to use them with PySpark SQL and PySpark DataFrame API. These come in handy when we need to make aggregate operations in a specific window frame on DataFrame columns.\n\nWhen possible try to leverage standard library as they are little bit more compile-time safety, handles null and perform better when compared to UDF’s. If your application is critical on performance try to avoid using custom UDF at all costs as these are not guarantee on performance\n1. Window Functions\nPySpark Window functions operate on a group of rows (like frame, partition) and return a single value for every input row. PySpark SQL supports three kinds of window functions:\n\nranking functions\nanalytic functions\naggregate functions\nThe below table defines Ranking and Analytic functions and for aggregate functions, we can use any existing aggregate functions as a window function.\nTo perform an operation on a group first, we need to partition the data using Window.partitionBy() , and for row number and rank function we need to additionally order by on partition data using orderBy clause.\nClick on each link to know more about these functions along with the Scala examples.\n\nWINDOW FUNCTIONS USAGE & SYNTAX\tPYSPARK WINDOW FUNCTIONS DESCRIPTION\nrow_number(): Column\tReturns a sequential number starting from 1 within a window partition\nrank(): Column\tReturns the rank of rows within a window partition, with gaps.\npercent_rank(): Column\tReturns the percentile rank of rows within a window partition.\ndense_rank(): Column\tReturns the rank of rows within a window partition without any gaps. Where as Rank() returns rank with gaps.\nntile(n: Int): Column\tReturns the ntile id in a window partition\ncume_dist(): Column\tReturns the cumulative distribution of values within a window partition\nlag(e: Column, offset: Int): Column\nlag(columnName: String, offset: Int): Column\nlag(columnName: String, offset: Int, defaultValue: Any): Column\treturns the value that is `offset` rows before the current row, and `null` if there is less than `offset` rows before the current row.\nlead(columnName: String, offset: Int): Column\nlead(columnName: String, offset: Int): Column\nlead(columnName: String, offset: Int, defaultValue: Any): Column\treturns the value that is `offset` rows after the current row, and `null` if there is less than `offset` rows after the current row."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"c9cc5bd0-59fd-477b-8b96-ca42a5585e62"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["import pyspark\nfrom pyspark.sql import SparkSession\nspark = SparkSession.builder.appName(\"sparkbyexample.com\").getOrCreate()\nsimpleData = ((\"James\", \"Sales\", 3000), \\\n    (\"Michael\", \"Sales\", 4600),  \\\n    (\"Robert\", \"Sales\", 4100),   \\\n    (\"Maria\", \"Finance\", 3000),  \\\n    (\"James\", \"Sales\", 3000),    \\\n    (\"Scott\", \"Finance\", 3300),  \\\n    (\"Jen\", \"Finance\", 3900),    \\\n    (\"Jeff\", \"Marketing\", 3000), \\\n    (\"Kumar\", \"Marketing\", 2000),\\\n    (\"Saif\", \"Sales\", 4100) \\\n  )\ncolumns1 = [\"employee_name\",\"department\",\"salary\"]\ndf10 = spark.createDataFrame(data =simpleData, schema = columns1)\ndf10.show(truncate = False)\n"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"4b93c16e-88fb-4d99-9610-b7d5f7ae289d"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"+-------------+----------+------+\n|employee_name|department|salary|\n+-------------+----------+------+\n|James        |Sales     |3000  |\n|Michael      |Sales     |4600  |\n|Robert       |Sales     |4100  |\n|Maria        |Finance   |3000  |\n|James        |Sales     |3000  |\n|Scott        |Finance   |3300  |\n|Jen          |Finance   |3900  |\n|Jeff         |Marketing |3000  |\n|Kumar        |Marketing |2000  |\n|Saif         |Sales     |4100  |\n+-------------+----------+------+\n\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["+-------------+----------+------+\n|employee_name|department|salary|\n+-------------+----------+------+\n|James        |Sales     |3000  |\n|Michael      |Sales     |4600  |\n|Robert       |Sales     |4100  |\n|Maria        |Finance   |3000  |\n|James        |Sales     |3000  |\n|Scott        |Finance   |3300  |\n|Jen          |Finance   |3900  |\n|Jeff         |Marketing |3000  |\n|Kumar        |Marketing |2000  |\n|Saif         |Sales     |4100  |\n+-------------+----------+------+\n\n"]}}],"execution_count":0},{"cell_type":"code","source":["2. PySpark Window Ranking functions\n2.1 row_number Window Function\nrow_number() window function is used to give the sequential row number starting from 1 to the result of each window partition.\nfrom pyspark.sql.window import Window\nfrom pyspark.sql.functions import row_number\nwindowSpec  = Window.partitionBy(\"department\").orderBy(\"salary\")\n\ndf.withColumn(\"row_number\",row_number().over(windowSpec)) \\\n    .show(truncate=False)\n2.2 rank Window Function\nrank() window function is used to provide a rank to the result within a window partition. This function leaves gaps in rank when there are ties.\n\"\"\"rank\"\"\"\nfrom pyspark.sql.functions import rank\ndf.withColumn(\"rank\",rank().over(windowSpec)) \\\n    .show()\n2.3 dense_rank Window Function\ndense_rank() window function is used to get the result with rank of rows within a window partition without any gaps. This is similar to rank() function difference being rank function leaves gaps in rank when there are ties.\n\"\"\"dens_rank\"\"\"\nfrom pyspark.sql.functions import dense_rank\ndf.withColumn(\"dense_rank\",dense_rank().over(windowSpec)) \\\n    .show()\n2.4 percent_rank Window Function\n\"\"\" percent_rank \"\"\"\nfrom pyspark.sql.functions import percent_rank\ndf.withColumn(\"percent_rank\",percent_rank().over(windowSpec)) \\\n    .show()\n2.5 ntile Window Function\nntile() window function returns the relative rank of result rows within a window partition. In below example we have used 2 as an argument to ntile hence it returns ranking between 2 values (1 and 2)\n\"\"\"ntile\"\"\"\nfrom pyspark.sql.functions import ntile\ndf.withColumn(\"ntile\",ntile(2).over(windowSpec)) \\\n    .show()\n3. PySpark Window Analytic functions\n3.1 cume_dist Window Function\ncume_dist() window function is used to get the cumulative distribution of values within a window partition.\n\nThis is the same as the DENSE_RANK function in SQL.\n\"\"\" cume_dist \"\"\"\nfrom pyspark.sql.functions import cume_dist    \ndf.withColumn(\"cume_dist\",cume_dist().over(windowSpec)) \\\n   .show()\n3.2 lag Window Function\nThis is the same as the LAG function in SQL.\n\"\"\"lag\"\"\"\nfrom pyspark.sql.functions import lag    \ndf.withColumn(\"lag\",lag(\"salary\",2).over(windowSpec)) \\\n      .show()\n3.3 lead Window Function\nThis is the same as the LEAD function in SQL.\n \"\"\"lead\"\"\"\nfrom pyspark.sql.functions import lead    \ndf.withColumn(\"lead\",lead(\"salary\",2).over(windowSpec)) \\\n    .show()\n4. PySpark Window Aggregate Functions\nIn this section, I will explain how to calculate sum, min, max for each department using PySpark SQL Aggregate window functions and WindowSpec. When working with Aggregate functions, we don’t need to use order by clause.\nwindowSpecAgg  = Window.partitionBy(\"department\")\nfrom pyspark.sql.functions import col,avg,sum,min,max,row_number \ndf.withColumn(\"row\",row_number().over(windowSpec)) \\\n  .withColumn(\"avg\", avg(col(\"salary\")).over(windowSpecAgg)) \\\n  .withColumn(\"sum\", sum(col(\"salary\")).over(windowSpecAgg)) \\\n  .withColumn(\"min\", min(col(\"salary\")).over(windowSpecAgg)) \\\n  .withColumn(\"max\", max(col(\"salary\")).over(windowSpecAgg)) \\\n  .where(col(\"row\")==1).select(\"department\",\"avg\",\"sum\",\"min\",\"max\") \\\n  .show()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"0b077331-4975-49b1-b237-9ecaaef8b506"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["from pyspark.sql.window import Window\nfrom pyspark.sql.functions import row_number\nwindowspec=Window.partitionBy(\"department\").orderBy(\"salary\")\ndf10.show()\nwindowspec=Window.partitionBy(\"department\").orderBy(\"salary\")\ndf10.withColumn(\"row_number\",row_number().over(windowspec)).show(truncate=False)\n"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"464cbcbb-f8c8-43ac-9d8d-e19e0246e968"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"+-------------+----------+------+\n|employee_name|department|salary|\n+-------------+----------+------+\n|        James|     Sales|  3000|\n|      Michael|     Sales|  4600|\n|       Robert|     Sales|  4100|\n|        Maria|   Finance|  3000|\n|        James|     Sales|  3000|\n|        Scott|   Finance|  3300|\n|          Jen|   Finance|  3900|\n|         Jeff| Marketing|  3000|\n|        Kumar| Marketing|  2000|\n|         Saif|     Sales|  4100|\n+-------------+----------+------+\n\n+-------------+----------+------+----------+\n|employee_name|department|salary|row_number|\n+-------------+----------+------+----------+\n|Maria        |Finance   |3000  |1         |\n|Scott        |Finance   |3300  |2         |\n|Jen          |Finance   |3900  |3         |\n|Kumar        |Marketing |2000  |1         |\n|Jeff         |Marketing |3000  |2         |\n|James        |Sales     |3000  |1         |\n|James        |Sales     |3000  |2         |\n|Robert       |Sales     |4100  |3         |\n|Saif         |Sales     |4100  |4         |\n|Michael      |Sales     |4600  |5         |\n+-------------+----------+------+----------+\n\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["+-------------+----------+------+\n|employee_name|department|salary|\n+-------------+----------+------+\n|        James|     Sales|  3000|\n|      Michael|     Sales|  4600|\n|       Robert|     Sales|  4100|\n|        Maria|   Finance|  3000|\n|        James|     Sales|  3000|\n|        Scott|   Finance|  3300|\n|          Jen|   Finance|  3900|\n|         Jeff| Marketing|  3000|\n|        Kumar| Marketing|  2000|\n|         Saif|     Sales|  4100|\n+-------------+----------+------+\n\n+-------------+----------+------+----------+\n|employee_name|department|salary|row_number|\n+-------------+----------+------+----------+\n|Maria        |Finance   |3000  |1         |\n|Scott        |Finance   |3300  |2         |\n|Jen          |Finance   |3900  |3         |\n|Kumar        |Marketing |2000  |1         |\n|Jeff         |Marketing |3000  |2         |\n|James        |Sales     |3000  |1         |\n|James        |Sales     |3000  |2         |\n|Robert       |Sales     |4100  |3         |\n|Saif         |Sales     |4100  |4         |\n|Michael      |Sales     |4600  |5         |\n+-------------+----------+------+----------+\n\n"]}}],"execution_count":0},{"cell_type":"code","source":["from pyspark.sql.window import Window\nfrom pyspark.sql.functions import row_number,rank\nwindowsec1 = Window.partitionBy(\"department\").orderBy(\"salary\")\ndf10.withColumn(\"rank\",rank().over(windowsec1))\\\n    .withColumn(\"row_number\",row_number().over(windowsec1)).show(truncate = False)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"45b37c32-ec49-4f8a-b852-8ece255ea2a1"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"+-------------+----------+------+----+----------+\n|employee_name|department|salary|rank|row_number|\n+-------------+----------+------+----+----------+\n|Maria        |Finance   |3000  |1   |1         |\n|Scott        |Finance   |3300  |2   |2         |\n|Jen          |Finance   |3900  |3   |3         |\n|Kumar        |Marketing |2000  |1   |1         |\n|Jeff         |Marketing |3000  |2   |2         |\n|James        |Sales     |3000  |1   |1         |\n|James        |Sales     |3000  |1   |2         |\n|Robert       |Sales     |4100  |3   |3         |\n|Saif         |Sales     |4100  |3   |4         |\n|Michael      |Sales     |4600  |5   |5         |\n+-------------+----------+------+----+----------+\n\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["+-------------+----------+------+----+----------+\n|employee_name|department|salary|rank|row_number|\n+-------------+----------+------+----+----------+\n|Maria        |Finance   |3000  |1   |1         |\n|Scott        |Finance   |3300  |2   |2         |\n|Jen          |Finance   |3900  |3   |3         |\n|Kumar        |Marketing |2000  |1   |1         |\n|Jeff         |Marketing |3000  |2   |2         |\n|James        |Sales     |3000  |1   |1         |\n|James        |Sales     |3000  |1   |2         |\n|Robert       |Sales     |4100  |3   |3         |\n|Saif         |Sales     |4100  |3   |4         |\n|Michael      |Sales     |4600  |5   |5         |\n+-------------+----------+------+----+----------+\n\n"]}}],"execution_count":0},{"cell_type":"code","source":["from pyspark.sql.window import Window\nfrom pyspark.sql.functions import row_number,rank,dense_rank\nwindowspec2 = Window.partitionBy(\"department\").orderBy(\"salary\")\ndf10.withColumn(\"dense_rank\",dense_rank().over(windowspec2))\\\n    .withColumn(\"row_number\",row_number().over(windowspec2))\\\n    .withColumn(\"rank\",rank().over(windowspec2)).show(truncate = False)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"1c80af2d-e04f-42bd-9433-9c314ceba826"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"+-------------+----------+------+----------+----------+----+\n|employee_name|department|salary|dense_rank|row_number|rank|\n+-------------+----------+------+----------+----------+----+\n|Maria        |Finance   |3000  |1         |1         |1   |\n|Scott        |Finance   |3300  |2         |2         |2   |\n|Jen          |Finance   |3900  |3         |3         |3   |\n|Kumar        |Marketing |2000  |1         |1         |1   |\n|Jeff         |Marketing |3000  |2         |2         |2   |\n|James        |Sales     |3000  |1         |1         |1   |\n|James        |Sales     |3000  |1         |2         |1   |\n|Robert       |Sales     |4100  |2         |3         |3   |\n|Saif         |Sales     |4100  |2         |4         |3   |\n|Michael      |Sales     |4600  |3         |5         |5   |\n+-------------+----------+------+----------+----------+----+\n\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["+-------------+----------+------+----------+----------+----+\n|employee_name|department|salary|dense_rank|row_number|rank|\n+-------------+----------+------+----------+----------+----+\n|Maria        |Finance   |3000  |1         |1         |1   |\n|Scott        |Finance   |3300  |2         |2         |2   |\n|Jen          |Finance   |3900  |3         |3         |3   |\n|Kumar        |Marketing |2000  |1         |1         |1   |\n|Jeff         |Marketing |3000  |2         |2         |2   |\n|James        |Sales     |3000  |1         |1         |1   |\n|James        |Sales     |3000  |1         |2         |1   |\n|Robert       |Sales     |4100  |2         |3         |3   |\n|Saif         |Sales     |4100  |2         |4         |3   |\n|Michael      |Sales     |4600  |3         |5         |5   |\n+-------------+----------+------+----------+----------+----+\n\n"]}}],"execution_count":0},{"cell_type":"code","source":["from pyspark.sql.window import Window\nfrom pyspark.sql.functions import ntile,percent_rank,row_number,rank,dense_rank\nwindowspec = Window.partitionBy(\"department\").orderBy(\"salary\")\ndf10.withColumn(\"percent_rank\",percent_rank().over(windowspec))\\\n    .withColumn(\"dense_rank\",dense_rank().over(windowspec))\\\n    .withColumn(\"rank\",rank().over(windowspec))\\\n    .withColumn(\"row_number\",row_number().over(windowspec))\\\n    .withColumn(\"ntile\",ntile(3).over(windowspec)).show(truncate = False)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"4a320400-db75-486e-94b1-1c81261b1fc4"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"+-------------+----------+------+------------+----------+----+----------+-----+\n|employee_name|department|salary|percent_rank|dense_rank|rank|row_number|ntile|\n+-------------+----------+------+------------+----------+----+----------+-----+\n|Maria        |Finance   |3000  |0.0         |1         |1   |1         |1    |\n|Scott        |Finance   |3300  |0.5         |2         |2   |2         |2    |\n|Jen          |Finance   |3900  |1.0         |3         |3   |3         |3    |\n|Kumar        |Marketing |2000  |0.0         |1         |1   |1         |1    |\n|Jeff         |Marketing |3000  |1.0         |2         |2   |2         |2    |\n|James        |Sales     |3000  |0.0         |1         |1   |1         |1    |\n|James        |Sales     |3000  |0.0         |1         |1   |2         |1    |\n|Robert       |Sales     |4100  |0.5         |2         |3   |3         |2    |\n|Saif         |Sales     |4100  |0.5         |2         |3   |4         |2    |\n|Michael      |Sales     |4600  |1.0         |3         |5   |5         |3    |\n+-------------+----------+------+------------+----------+----+----------+-----+\n\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["+-------------+----------+------+------------+----------+----+----------+-----+\n|employee_name|department|salary|percent_rank|dense_rank|rank|row_number|ntile|\n+-------------+----------+------+------------+----------+----+----------+-----+\n|Maria        |Finance   |3000  |0.0         |1         |1   |1         |1    |\n|Scott        |Finance   |3300  |0.5         |2         |2   |2         |2    |\n|Jen          |Finance   |3900  |1.0         |3         |3   |3         |3    |\n|Kumar        |Marketing |2000  |0.0         |1         |1   |1         |1    |\n|Jeff         |Marketing |3000  |1.0         |2         |2   |2         |2    |\n|James        |Sales     |3000  |0.0         |1         |1   |1         |1    |\n|James        |Sales     |3000  |0.0         |1         |1   |2         |1    |\n|Robert       |Sales     |4100  |0.5         |2         |3   |3         |2    |\n|Saif         |Sales     |4100  |0.5         |2         |3   |4         |2    |\n|Michael      |Sales     |4600  |1.0         |3         |5   |5         |3    |\n+-------------+----------+------+------------+----------+----+----------+-----+\n\n"]}}],"execution_count":0},{"cell_type":"code","source":["from pyspark.sql.window import Window\nfrom pyspark.sql.functions import lag,lead,cume_dist,percent_rank,dense_rank,rank,row_number,ntile\nwindowspec3 =Window.partitionBy(\"department\").orderBy(\"salary\")\ndf10.withColumn(\"cume_dist\",cume_dist().over(windowspec3))\\\n    .withColumn(\"lead\",lead(\"salary\").over(windowspec3))\\\n    .withColumn(\"lead1\",lead(\"salary\",2).over(windowspec3))\\\n    .withColumn(\"lag\",lag(\"salary\").over(windowspec3))\\\n    .withColumn(\"lag1\",lag(\"salary\",2).over(windowspec3)).show(truncate = False)\n"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"a8d6adbc-3bea-4af2-aaba-2d3181bb01c1"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"+-------------+----------+------+------------------+----+-----+----+----+\n|employee_name|department|salary|cume_dist         |lead|lead1|lag |lag1|\n+-------------+----------+------+------------------+----+-----+----+----+\n|Maria        |Finance   |3000  |0.3333333333333333|3300|3900 |null|null|\n|Scott        |Finance   |3300  |0.6666666666666666|3900|null |3000|null|\n|Jen          |Finance   |3900  |1.0               |null|null |3300|3000|\n|Kumar        |Marketing |2000  |0.5               |3000|null |null|null|\n|Jeff         |Marketing |3000  |1.0               |null|null |2000|null|\n|James        |Sales     |3000  |0.4               |3000|4100 |null|null|\n|James        |Sales     |3000  |0.4               |4100|4100 |3000|null|\n|Robert       |Sales     |4100  |0.8               |4100|4600 |3000|3000|\n|Saif         |Sales     |4100  |0.8               |4600|null |4100|3000|\n|Michael      |Sales     |4600  |1.0               |null|null |4100|4100|\n+-------------+----------+------+------------------+----+-----+----+----+\n\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["+-------------+----------+------+------------------+----+-----+----+----+\n|employee_name|department|salary|cume_dist         |lead|lead1|lag |lag1|\n+-------------+----------+------+------------------+----+-----+----+----+\n|Maria        |Finance   |3000  |0.3333333333333333|3300|3900 |null|null|\n|Scott        |Finance   |3300  |0.6666666666666666|3900|null |3000|null|\n|Jen          |Finance   |3900  |1.0               |null|null |3300|3000|\n|Kumar        |Marketing |2000  |0.5               |3000|null |null|null|\n|Jeff         |Marketing |3000  |1.0               |null|null |2000|null|\n|James        |Sales     |3000  |0.4               |3000|4100 |null|null|\n|James        |Sales     |3000  |0.4               |4100|4100 |3000|null|\n|Robert       |Sales     |4100  |0.8               |4100|4600 |3000|3000|\n|Saif         |Sales     |4100  |0.8               |4600|null |4100|3000|\n|Michael      |Sales     |4600  |1.0               |null|null |4100|4100|\n+-------------+----------+------+------------------+----+-----+----+----+\n\n"]}}],"execution_count":0},{"cell_type":"code","source":[""],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"80caeb3a-db01-48ab-a324-2877b4c2d081"}},"outputs":[],"execution_count":0}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"PySpark SQL Functions","dashboards":[],"notebookMetadata":{"pythonIndentUnit":4},"language":"python","widgets":{},"notebookOrigID":755754218939065}},"nbformat":4,"nbformat_minor":0}
