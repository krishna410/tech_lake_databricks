{"cells":[{"cell_type":"code","source":["columns = [\"language\",\"user_count\"]\ndata = [(\"java\",\"20000\"),(\"python\",\"10000\"),(\"scala\",\"3000\")]"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"799dc7b1-77ed-479d-81a0-415f2011bb35"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["first, let’s create a Spark RDD from a collection List by calling parallelize() function from SparkContext . We would need this rdd object for all our examples below.\nspark = SparkSession.builder.appName('SparkByExamples.com').getOrCreate()\nrdd = spark.sparkContext.parallelize(data)\n"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"125935b7-4878-4636-abc7-74c0dd87633c"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["import pyspark\nfrom pyspark.sql import SparkSession\nspark = SparkSession.builder.appName('SparkByExamples.com').getOrCreate()\nrdd = spark.sparkContext.parallelize(data)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"989a6f68-cc4c-4a41-b58d-aee3bdb22a24"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["dfFromRDD1= rdd.toDF()\ndfFromRDD1.printSchema()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"e77cee2b-68d1-4090-be40-1789e9469889"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"root\n |-- _1: string (nullable = true)\n |-- _2: string (nullable = true)\n\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["root\n |-- _1: string (nullable = true)\n |-- _2: string (nullable = true)\n\n"]}}],"execution_count":0},{"cell_type":"code","source":["columns = [\"language\",\"user_count\"]\ndfFromrRDD = rdd.toDF(columns)\ndfFromrRDD.printSchema()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"6862abdd-4a53-450c-ae41-387920a39c39"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"root\n |-- language: string (nullable = true)\n |-- user_count: string (nullable = true)\n\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["root\n |-- language: string (nullable = true)\n |-- user_count: string (nullable = true)\n\n"]}}],"execution_count":0},{"cell_type":"code","source":["\ndfFromRDD2 = spark.createDataFrame(rdd).toDF(*columns)\n"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"34707d93-dad1-4384-97f7-ef7b74910d65"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["\ndfFromData2 = spark.createDataFrame(data).toDF(*columns)\n"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"36ff40e6-72e6-42ff-ba2f-f98c95f6755f"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["import pyspark\nfrom pyspark.sql import Row\nrowData = map(lambda x: Row(*x), data) \ndfFromData3 = spark.createDataFrame(rowData,columns)\n"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"5063797c-fdd0-488a-a63b-38b64b8a6a8c"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["\nfrom pyspark.sql.types import StructType,StructField, StringType, IntegerType\ndata2 = [(\"James\",\"\",\"Smith\",\"36636\",\"M\",3000),\n    (\"Michael\",\"Rose\",\"\",\"40288\",\"M\",4000),\n    (\"Robert\",\"\",\"Williams\",\"42114\",\"M\",4000),\n    (\"Maria\",\"Anne\",\"Jones\",\"39192\",\"F\",4000),\n    (\"Jen\",\"Mary\",\"Brown\",\"\",\"F\",-1)\n  ]\n\nschema = StructType([ \\\n    StructField(\"firstname\",StringType(),True), \\\n    StructField(\"middlename\",StringType(),True), \\\n    StructField(\"lastname\",StringType(),True), \\\n    StructField(\"id\", StringType(), True), \\\n    StructField(\"gender\", StringType(), True), \\\n    StructField(\"salary\", IntegerType(), True) \\\n  ])\n \ndf = spark.createDataFrame(data=data2,schema=schema)\ndf.printSchema()\ndf.show(truncate=False)\n"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"8b80ccf7-aff3-4da3-b08c-5df96c3ceaa6"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["import pyspark\nfrom pyspark.sql.types import StructType,StructField,StringType,IntegerType\ndata2 = [(\"James\",\"\",\"Smith\",36636,\"M\",3000),\n    (\"Michael\",\"Rose\",\"\",40288,\"M\",4000),\n    (\"Robert\",\"\",\"Williams\",42114,\"M\",4000),\n    (\"Maria\",\"Anne\",\"Jones\",39192,\"F\",4000),\n    (\"Jen\",\"Mary\",\"Brown\",1234,\"F\",-1)]\n\nschema = StructType([\\\n         StructField(\"firstname\",StringType(),True), \\\n         StructField(\"middlename\",StringType(),True), \\\n         StructField(\"lastname\",StringType(),True), \\\n         StructField(\"id\",IntegerType(),True), \\\n         StructField(\"gender\",StringType(),True), \\\n         StructField(\"salary\",IntegerType(),True) \\\n                 ])\ndf = spark.createDataFrame(data = data2,schema=schema)\ndf.printSchema()\ndf.show(truncate=False)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"0067fa84-9bbd-4521-a90f-5e64b21587c2"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"root\n |-- firstname: string (nullable = true)\n |-- middlename: string (nullable = true)\n |-- lastname: string (nullable = true)\n |-- id: integer (nullable = true)\n |-- gender: string (nullable = true)\n |-- salary: integer (nullable = true)\n\n+---------+----------+--------+-----+------+------+\n|firstname|middlename|lastname|id   |gender|salary|\n+---------+----------+--------+-----+------+------+\n|James    |          |Smith   |36636|M     |3000  |\n|Michael  |Rose      |        |40288|M     |4000  |\n|Robert   |          |Williams|42114|M     |4000  |\n|Maria    |Anne      |Jones   |39192|F     |4000  |\n|Jen      |Mary      |Brown   |1234 |F     |-1    |\n+---------+----------+--------+-----+------+------+\n\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["root\n |-- firstname: string (nullable = true)\n |-- middlename: string (nullable = true)\n |-- lastname: string (nullable = true)\n |-- id: integer (nullable = true)\n |-- gender: string (nullable = true)\n |-- salary: integer (nullable = true)\n\n+---------+----------+--------+-----+------+------+\n|firstname|middlename|lastname|id   |gender|salary|\n+---------+----------+--------+-----+------+------+\n|James    |          |Smith   |36636|M     |3000  |\n|Michael  |Rose      |        |40288|M     |4000  |\n|Robert   |          |Williams|42114|M     |4000  |\n|Maria    |Anne      |Jones   |39192|F     |4000  |\n|Jen      |Mary      |Brown   |1234 |F     |-1    |\n+---------+----------+--------+-----+------+------+\n\n"]}}],"execution_count":0},{"cell_type":"code","source":["1. Create Empty RDD in PySpark\nCreate an empty RDD by using emptyRDD() of SparkContext for example spark.sparkContext.emptyRDD()."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"ce2bf3e2-ebc8-48e3-9701-6d25c9aef974"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["\nfrom pyspark.sql import SparkSession\nspark = SparkSession.builder.appName('SparkByExamples.com').getOrCreate()\n\n#Creates Empty RDD\nemptyRDD = spark.sparkContext.emptyRDD()\nprint(emptyRDD)\n\n#Diplays\n#EmptyRDD[188] at emptyRDD\n\n#Creates Empty RDD using parallelize\nrdd2= spark.sparkContext.parallelize([])\nprint(rdd2)\n\n#EmptyRDD[205] at emptyRDD at NativeMethodAccessorImpl.java:0\n#ParallelCollectionRDD[206] at readRDDFromFile at PythonRDD.scala:262\n\n"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"a4386a48-fd37-4b4a-bebc-d61c0011fcb2"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["import pyspark\nfrom pyspark.sql import SparkSession\nspark = SparkSession.builder.appName('SparkByExamples.com').getOrCreate()\n#create empty RDD\nemptyRDD = spark.sparkContext.emptyRDD()\nprint(emptyRDD)\n"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"a7577f7d-3547-4f29-a62d-07d9133ae382"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"EmptyRDD[2] at emptyRDD at NativeMethodAccessorImpl.java:0\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["EmptyRDD[2] at emptyRDD at NativeMethodAccessorImpl.java:0\n"]}}],"execution_count":0},{"cell_type":"code","source":["emptyRRD2 = spark.sparkContext.Parallelize([])\nprint(emptyRDD2)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"f557abc9-2748-4df1-bd00-91c4a0e047f7"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n\u001B[0;31mAttributeError\u001B[0m                            Traceback (most recent call last)\n\u001B[0;32m<command-392957826950563>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[0;32m----> 1\u001B[0;31m \u001B[0memptyRRD2\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mspark\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msparkContext\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mParallelize\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      2\u001B[0m \u001B[0mprint\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0memptyRDD2\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;31mAttributeError\u001B[0m: 'RemoteContext' object has no attribute 'Parallelize'","errorSummary":"<span class='ansi-red-fg'>AttributeError</span>: 'RemoteContext' object has no attribute 'Parallelize'","metadata":{},"errorTraceType":"ansi","type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/plain":["\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n\u001B[0;31mAttributeError\u001B[0m                            Traceback (most recent call last)\n\u001B[0;32m<command-392957826950563>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[0;32m----> 1\u001B[0;31m \u001B[0memptyRRD2\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mspark\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msparkContext\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mParallelize\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      2\u001B[0m \u001B[0mprint\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0memptyRDD2\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;31mAttributeError\u001B[0m: 'RemoteContext' object has no attribute 'Parallelize'"]}}],"execution_count":0},{"cell_type":"code","source":["2. Create Empty DataFrame with Schema (StructType)\nIn order to create an empty PySpark DataFrame manually with schema ( column names & data types) first, Create a schema using StructType and StructField ."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"4ec1599f-9c06-4030-a940-6709bf9eb018"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["from pyspark.sql.types import StructType,StructField,StringType,IntegerType,DateType\nschema = StructType([\n    StructField('fisstname',StringType(), True),\n    StructField('Lastname',StringType(),True),\n    StructField('Middlename',StringType(),True),\n    StructField('Id',IntegerType(),True),\n    StructField('Date',DateType(),True)\n])\n#Now use the empty RDD created above and pass it to createDataFrame() of SparkSession along with the schema for column names & data types.\n#create empty RRD\ndf= spark.createDataFrame(emptyRDD,schema)\ndf.printSchema()\n"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"712a7b31-c7a3-445c-a6f0-7387f248c5e6"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"root\n |-- fisstname: string (nullable = true)\n |-- Lastname: string (nullable = true)\n |-- Middlename: string (nullable = true)\n |-- Id: integer (nullable = true)\n |-- Date: date (nullable = true)\n\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["root\n |-- fisstname: string (nullable = true)\n |-- Lastname: string (nullable = true)\n |-- Middlename: string (nullable = true)\n |-- Id: integer (nullable = true)\n |-- Date: date (nullable = true)\n\n"]}}],"execution_count":0},{"cell_type":"code","source":["3. Convert Empty RDD to DataFrame\nYou can also create empty DataFrame by converting empty RDD to DataFrame using toDF()."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"e51bc929-5cf6-4987-9bfe-1eb06cf19fbc"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["#convert empty RDD to dataframe DF()\ndf1 = emptyRDD.toDF(schema)\ndf1.printSchema()\n"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"227ad40d-e0cb-489e-9318-7bcc24280f7f"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"root\n |-- fisstname: string (nullable = true)\n |-- Lastname: string (nullable = true)\n |-- Middlename: string (nullable = true)\n |-- Id: integer (nullable = true)\n |-- Date: date (nullable = true)\n\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["root\n |-- fisstname: string (nullable = true)\n |-- Lastname: string (nullable = true)\n |-- Middlename: string (nullable = true)\n |-- Id: integer (nullable = true)\n |-- Date: date (nullable = true)\n\n"]}}],"execution_count":0},{"cell_type":"code","source":["4. Create Empty DataFrame with Schema.\nSo far I have covered creating an empty DataFrame from RDD, but here will create it manually with schema and without RDD."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"847f45ad-59be-4ad7-9a21-162499d18a2f"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["df2 = spark.createDataFrame([],schema)\ndf2.printSchema()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"dfac87dd-d64c-4715-a06d-ea4fcbec27b9"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"root\n |-- fisstname: string (nullable = true)\n |-- Lastname: string (nullable = true)\n |-- Middlename: string (nullable = true)\n |-- Id: integer (nullable = true)\n |-- Date: date (nullable = true)\n\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["root\n |-- fisstname: string (nullable = true)\n |-- Lastname: string (nullable = true)\n |-- Middlename: string (nullable = true)\n |-- Id: integer (nullable = true)\n |-- Date: date (nullable = true)\n\n"]}}],"execution_count":0},{"cell_type":"code","source":["#5. Create Empty DataFrame without Schema (no columns)\n#To create empty DataFrame with out schema (no columns) just create a empty schema and use it while creating PySpark DataFrame."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"6df178d5-4e58-423f-8c42-59addda895a9"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["#create empty dataframe woth no schema(no columns)\ndf3= spark.createDataFrame([],StructType([]))\ndf3.printSchema()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"1c5223ac-5feb-4c62-8f20-5337bfac21b0"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"root\n\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["root\n\n"]}}],"execution_count":0},{"cell_type":"code","source":["#Convert PySpark RDD to DataFrame\n#1. Create PySpark RDD\n#First, let’s create an RDD by passing Python list object to sparkContext.parallelize() function. We would need this rdd object for all our examples below.\n\n#In PySpark, when you have data in a list meaning you have a collection of data in a PySpark driver memory when you create an RDD, this collection is going to be parallelized.\n\n\nfrom pyspark.sql import SparkSession\nspark = SparkSession.builder.appName('SparkByExamples.com').getOrCreate()\ndept = [(\"Finance\",10),(\"Marketing\",20),(\"Sales\",30),(\"IT\",40)]\nrdd = spark.sparkContext.parallelize(dept)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"80f0e116-1593-4cde-8a66-2cb0030477d1"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["import pyspark\nfrom pyspark.sql import SparkSession\nspark= SparkSession.builder.appName('SparkByExamples.com').getOrCreate()\ndept = [(\"finanace\",10),(\"Marketing\",20),(\"sales\",30),(\"IT\",40)]\nrdd = spark.sparkContext.parallelize(dept)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"7edc36b2-1338-4e21-857e-0dd15b92a190"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["2. Convert PySpark RDD to DataFrame\nConverting PySpark RDD to DataFrame can be done using toDF(), createDataFrame(). In this section, I will explain these two methods.\n2.1 Using rdd.toDF() function\nPySpark provides toDF() function in RDD which can be used to convert RDD into Dataframe\n\n\ndf = rdd.toDF()\ndf.printSchema()\ndf.show(truncate=False)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"4181ec8a-dd3c-44fd-bc2d-99f136845529"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["df = rdd.toDF()\ndf.printSchema()\ndf.show(truncate=False)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"4ee8f0a7-8df6-47d5-943e-36c742d1daea"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"root\n |-- _1: string (nullable = true)\n |-- _2: long (nullable = true)\n\n+---------+---+\n|_1       |_2 |\n+---------+---+\n|finanace |10 |\n|Marketing|20 |\n|sales    |30 |\n|IT       |40 |\n+---------+---+\n\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["root\n |-- _1: string (nullable = true)\n |-- _2: long (nullable = true)\n\n+---------+---+\n|_1       |_2 |\n+---------+---+\n|finanace |10 |\n|Marketing|20 |\n|sales    |30 |\n|IT       |40 |\n+---------+---+\n\n"]}}],"execution_count":0},{"cell_type":"code","source":["depColumns = [(\"dept_name\"),(\"dept_id\")]\ndf2 = rdd.toDF(depColumns)\ndf2.printSchema()\ndf2.show(truncate = False)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"28b43a85-2d35-433c-bc38-0f5581bfe130"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"root\n |-- dept_name: string (nullable = true)\n |-- dept_id: long (nullable = true)\n\n+---------+-------+\n|dept_name|dept_id|\n+---------+-------+\n|finanace |10     |\n|Marketing|20     |\n|sales    |30     |\n|IT       |40     |\n+---------+-------+\n\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["root\n |-- dept_name: string (nullable = true)\n |-- dept_id: long (nullable = true)\n\n+---------+-------+\n|dept_name|dept_id|\n+---------+-------+\n|finanace |10     |\n|Marketing|20     |\n|sales    |30     |\n|IT       |40     |\n+---------+-------+\n\n"]}}],"execution_count":0},{"cell_type":"code","source":["2.2 Using PySpark createDataFrame() function\nSparkSession class provides createDataFrame() method to create DataFrame and it takes rdd object as an argument.\n\n\ndeptDF = spark.createDataFrame(rdd, schema = deptColumns)\ndeptDF.printSchema()\ndeptDF.show(truncate=False)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":true,"inputWidgets":{},"nuid":"00527605-2cc4-481e-ad83-0c8c5dd11594"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"\u001B[0;36m  File \u001B[0;32m\"<command-392957826950577>\"\u001B[0;36m, line \u001B[0;32m1\u001B[0m\n\u001B[0;31m    2.2 Using PySpark createDataFrame() function\u001B[0m\n\u001B[0m        ^\u001B[0m\n\u001B[0;31mSyntaxError\u001B[0m\u001B[0;31m:\u001B[0m invalid syntax\n","errorSummary":"<span class='ansi-red-fg'>SyntaxError</span>: invalid syntax (<command-392957826950577>, line 1)","metadata":{},"errorTraceType":"ansi","type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/plain":["\u001B[0;36m  File \u001B[0;32m\"<command-392957826950577>\"\u001B[0;36m, line \u001B[0;32m1\u001B[0m\n\u001B[0;31m    2.2 Using PySpark createDataFrame() function\u001B[0m\n\u001B[0m        ^\u001B[0m\n\u001B[0;31mSyntaxError\u001B[0m\u001B[0;31m:\u001B[0m invalid syntax\n"]}}],"execution_count":0},{"cell_type":"code","source":["\ndeptDF = spark.createDataFrame(rdd,schema= depColumns)\ndeptDF.printSchema()\ndeptDF.show(truncate = False)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"2d2952d6-402d-46dd-9c00-5c85ef794e32"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"root\n |-- dept_name: string (nullable = true)\n |-- dept_id: long (nullable = true)\n\n+---------+-------+\n|dept_name|dept_id|\n+---------+-------+\n|finanace |10     |\n|Marketing|20     |\n|sales    |30     |\n|IT       |40     |\n+---------+-------+\n\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["root\n |-- dept_name: string (nullable = true)\n |-- dept_id: long (nullable = true)\n\n+---------+-------+\n|dept_name|dept_id|\n+---------+-------+\n|finanace |10     |\n|Marketing|20     |\n|sales    |30     |\n|IT       |40     |\n+---------+-------+\n\n"]}}],"execution_count":0},{"cell_type":"code","source":["import pyspark\nfrom pyspark.sql.types  import StructType,StructField,StringType,IntegerType, DateType\ndeptschema = StructType([\n    StructField(\"dept_name\", StringType(),True),\n    StructField(\"Dept_id\",IntegerType(),True)\n])\n\nDf4 = spark.createDataFrame(rdd , schema = deptschema)\nDf4.printSchema()\nDf4.show(truncate = False)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"867582b6-0257-4679-bfa6-1fffcb3cc1ba"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"root\n |-- dept_name: string (nullable = true)\n |-- Dept_id: integer (nullable = true)\n\n+---------+-------+\n|dept_name|Dept_id|\n+---------+-------+\n|finanace |10     |\n|Marketing|20     |\n|sales    |30     |\n|IT       |40     |\n+---------+-------+\n\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["root\n |-- dept_name: string (nullable = true)\n |-- Dept_id: integer (nullable = true)\n\n+---------+-------+\n|dept_name|Dept_id|\n+---------+-------+\n|finanace |10     |\n|Marketing|20     |\n|sales    |30     |\n|IT       |40     |\n+---------+-------+\n\n"]}}],"execution_count":0},{"cell_type":"code","source":["\nConvert PySpark DataFrame to Pandas\n\nimport pyspark\nfrom pyspark.sql import SparkSession\n\nspark = SparkSession.builder.appName('SparkByExamples.com').getOrCreate()\n\ndata = [(\"James\",\"\",\"Smith\",\"36636\",\"M\",60000),\n        (\"Michael\",\"Rose\",\"\",\"40288\",\"M\",70000),\n        (\"Robert\",\"\",\"Williams\",\"42114\",\"\",400000),\n        (\"Maria\",\"Anne\",\"Jones\",\"39192\",\"F\",500000),\n        (\"Jen\",\"Mary\",\"Brown\",\"\",\"F\",0)]\n\ncolumns = [\"first_name\",\"middle_name\",\"last_name\",\"dob\",\"gender\",\"salary\"]\npysparkDF = spark.createDataFrame(data = data, schema = columns)\npysparkDF.printSchema()\npysparkDF.show(truncate=False)\n"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"6da92770-7cd7-4b59-8bd4-38a9abea15eb"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["import pyspark\nfrom pyspark.sql import SparkSession\nspark = SparkSession.builder.appName('SparkByExamples.com').getOrCreate()\ndata = [(\"James\",\"\",\"Smith\",\"36636\",\"M\",60000),\n        (\"Michael\",\"Rose\",\"\",\"40288\",\"M\",70000),\n        (\"Robert\",\"\",\"Williams\",\"42114\",\"\",400000),\n        (\"Maria\",\"Anne\",\"Jones\",\"39192\",\"F\",500000),\n        (\"Jen\",\"Mary\",\"Brown\",\"\",\"F\",0)\n       ]\ncolumns = [\"first_name\",\"middle_name\",\"last_name\",\"dob\",\"gender\",\"salary\"]\npysparkDF = spark.createDataFrame(data = data , schema = columns)\npysparkDF.printSchema()\npysparkDF.show(truncate = False)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"dc43c63b-a204-4b47-8753-aa076d6b4b23"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"root\n |-- first_name: string (nullable = true)\n |-- middle_name: string (nullable = true)\n |-- last_name: string (nullable = true)\n |-- dob: string (nullable = true)\n |-- gender: string (nullable = true)\n |-- salary: long (nullable = true)\n\n+----------+-----------+---------+-----+------+------+\n|first_name|middle_name|last_name|dob  |gender|salary|\n+----------+-----------+---------+-----+------+------+\n|James     |           |Smith    |36636|M     |60000 |\n|Michael   |Rose       |         |40288|M     |70000 |\n|Robert    |           |Williams |42114|      |400000|\n|Maria     |Anne       |Jones    |39192|F     |500000|\n|Jen       |Mary       |Brown    |     |F     |0     |\n+----------+-----------+---------+-----+------+------+\n\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["root\n |-- first_name: string (nullable = true)\n |-- middle_name: string (nullable = true)\n |-- last_name: string (nullable = true)\n |-- dob: string (nullable = true)\n |-- gender: string (nullable = true)\n |-- salary: long (nullable = true)\n\n+----------+-----------+---------+-----+------+------+\n|first_name|middle_name|last_name|dob  |gender|salary|\n+----------+-----------+---------+-----+------+------+\n|James     |           |Smith    |36636|M     |60000 |\n|Michael   |Rose       |         |40288|M     |70000 |\n|Robert    |           |Williams |42114|      |400000|\n|Maria     |Anne       |Jones    |39192|F     |500000|\n|Jen       |Mary       |Brown    |     |F     |0     |\n+----------+-----------+---------+-----+------+------+\n\n"]}}],"execution_count":0},{"cell_type":"code","source":["columnsdattypes = StructType([StructField(\"first_name\", StringType(), True),\n                              StructField(\"middle_name\",StringType(),True),\n                              StructField(\"last_name\",StringType(),True),\n                              StructField(\"dob\",StringType(),True),\n                              StructField(\"gender\",StringType(), True),\n                              StructField(\"salary\",IntegerType(),True)\n                             ])\npysparkDf = spark.createDataFrame(data = data , schema = columnsdattypes)\npysparkDf.printSchema()\npysparkDf.show(truncate = False)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"081a1998-cd51-4034-ad6d-d4d8fcbda219"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"root\n |-- first_name: string (nullable = true)\n |-- middle_name: string (nullable = true)\n |-- last_name: string (nullable = true)\n |-- dob: string (nullable = true)\n |-- gender: string (nullable = true)\n |-- salary: integer (nullable = true)\n\n+----------+-----------+---------+-----+------+------+\n|first_name|middle_name|last_name|dob  |gender|salary|\n+----------+-----------+---------+-----+------+------+\n|James     |           |Smith    |36636|M     |60000 |\n|Michael   |Rose       |         |40288|M     |70000 |\n|Robert    |           |Williams |42114|      |400000|\n|Maria     |Anne       |Jones    |39192|F     |500000|\n|Jen       |Mary       |Brown    |     |F     |0     |\n+----------+-----------+---------+-----+------+------+\n\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["root\n |-- first_name: string (nullable = true)\n |-- middle_name: string (nullable = true)\n |-- last_name: string (nullable = true)\n |-- dob: string (nullable = true)\n |-- gender: string (nullable = true)\n |-- salary: integer (nullable = true)\n\n+----------+-----------+---------+-----+------+------+\n|first_name|middle_name|last_name|dob  |gender|salary|\n+----------+-----------+---------+-----+------+------+\n|James     |           |Smith    |36636|M     |60000 |\n|Michael   |Rose       |         |40288|M     |70000 |\n|Robert    |           |Williams |42114|      |400000|\n|Maria     |Anne       |Jones    |39192|F     |500000|\n|Jen       |Mary       |Brown    |     |F     |0     |\n+----------+-----------+---------+-----+------+------+\n\n"]}}],"execution_count":0},{"cell_type":"code","source":["Convert PySpark Dataframe to Pandas DataFrame\nPySpark DataFrame provides a method toPandas() to convert it to Python Pandas DataFrame."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"0ab2d7a2-6470-4594-b9e6-d9e3c589fdd9"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["pandasDF = pysparkDf.toPandas()\nprint(pandasDF)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"a806cdec-83af-4a92-aeea-7782bba74e4a"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"  first_name middle_name last_name    dob gender  salary\n0      James                 Smith  36636      M   60000\n1    Michael        Rose            40288      M   70000\n2     Robert              Williams  42114         400000\n3      Maria        Anne     Jones  39192      F  500000\n4        Jen        Mary     Brown             F       0\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["  first_name middle_name last_name    dob gender  salary\n0      James                 Smith  36636      M   60000\n1    Michael        Rose            40288      M   70000\n2     Robert              Williams  42114         400000\n3      Maria        Anne     Jones  39192      F  500000\n4        Jen        Mary     Brown             F       0\n"]}}],"execution_count":0},{"cell_type":"code","source":["import pyspark\nfrom pyspark.sql import SparkSession\nspark = SparkSession.builder.appName('SparkByExamples.com').getOrCreate()\ndata = [((\"James\",\"\",\"Smith\"),\"36636\",\"M\",60000),\n        ((\"Michael\",\"Rose\",\"\"),\"40288\",\"M\",70000),\n        ((\"Robert\",\"\",\"Williams\"),\"42114\",\"\",400000),\n        ((\"Maria\",\"Anne\",\"Jones\"),\"39192\",\"F\",500000),\n        ((\"Jen\",\"Mary\",\"Brown\"),\"\",\"F\",0)\n       ]\ncolumns = [\"first_name\",\"middle_name\",\"last_name\",\"dob\",\"gender\",\"salary\"]\n\npysparkDF = spark.createDataFrame(data = data , schema = columns)\npysparkDF.printSchema()\npysparkDF.show(truncate = False)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"8b75e277-7f85-4fa6-a0e6-e774772b896d"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n\u001B[0;31mIndexError\u001B[0m                                Traceback (most recent call last)\n\u001B[0;32m<command-1603522030302347>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m      9\u001B[0m        ]\n\u001B[1;32m     10\u001B[0m \u001B[0;31m#columns = [\"first_name\",\"middle_name\",\"last_name\",\"dob\",\"gender\",\"salary\"]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 11\u001B[0;31m \u001B[0mpysparkDF\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mspark\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mcreateDataFrame\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mdata\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mdata\u001B[0m \u001B[0;34m,\u001B[0m \u001B[0mschema\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mcolumns\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     12\u001B[0m \u001B[0mpysparkDF\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mprintSchema\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     13\u001B[0m \u001B[0mpysparkDF\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mshow\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mtruncate\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;32mFalse\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;32m/databricks/spark/python/pyspark/sql/session.py\u001B[0m in \u001B[0;36mcreateDataFrame\u001B[0;34m(self, data, schema, samplingRatio, verifySchema)\u001B[0m\n\u001B[1;32m    720\u001B[0m             return super(SparkSession, self).createDataFrame(\n\u001B[1;32m    721\u001B[0m                 data, schema, samplingRatio, verifySchema)\n\u001B[0;32m--> 722\u001B[0;31m         \u001B[0;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_create_dataframe\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mdata\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mschema\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0msamplingRatio\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mverifySchema\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    723\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    724\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0m_create_dataframe\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mdata\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mschema\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0msamplingRatio\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mverifySchema\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;32m/databricks/spark/python/pyspark/sql/session.py\u001B[0m in \u001B[0;36m_create_dataframe\u001B[0;34m(self, data, schema, samplingRatio, verifySchema)\u001B[0m\n\u001B[1;32m    752\u001B[0m                 \u001B[0mrdd\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mschema\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_createFromRDD\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mdata\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mmap\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mprepare\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mschema\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0msamplingRatio\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    753\u001B[0m             \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 754\u001B[0;31m                 \u001B[0mrdd\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mschema\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_createFromLocal\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mmap\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mprepare\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mdata\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mschema\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    755\u001B[0m             \u001B[0mjrdd\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_jvm\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mSerDeUtil\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mtoJavaArray\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mrdd\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_to_java_object_rdd\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    756\u001B[0m             \u001B[0mjdf\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_jsparkSession\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mapplySchemaToPythonRDD\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mjrdd\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mrdd\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mschema\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mjson\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;32m/databricks/spark/python/pyspark/sql/session.py\u001B[0m in \u001B[0;36m_createFromLocal\u001B[0;34m(self, data, schema)\u001B[0m\n\u001B[1;32m    528\u001B[0m         \u001B[0mwrite\u001B[0m \u001B[0mtemp\u001B[0m \u001B[0mfiles\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    529\u001B[0m         \"\"\"\n\u001B[0;32m--> 530\u001B[0;31m         \u001B[0mdata\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mschema\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_wrap_data_schema\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mdata\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mschema\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    531\u001B[0m         \u001B[0;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_sc\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mparallelize\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mdata\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mschema\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    532\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;32m/databricks/spark/python/pyspark/sql/session.py\u001B[0m in \u001B[0;36m_wrap_data_schema\u001B[0;34m(self, data, schema)\u001B[0m\n\u001B[1;32m    512\u001B[0m             \u001B[0;32mif\u001B[0m \u001B[0misinstance\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mschema\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m(\u001B[0m\u001B[0mlist\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mtuple\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    513\u001B[0m                 \u001B[0;32mfor\u001B[0m \u001B[0mi\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mname\u001B[0m \u001B[0;32min\u001B[0m \u001B[0menumerate\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mschema\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 514\u001B[0;31m                     \u001B[0mstruct\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mfields\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mi\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mname\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mname\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    515\u001B[0m                     \u001B[0mstruct\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mnames\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mi\u001B[0m\u001B[0;34m]\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mname\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    516\u001B[0m             \u001B[0mschema\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mstruct\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;31mIndexError\u001B[0m: list index out of range","errorSummary":"<span class='ansi-red-fg'>IndexError</span>: list index out of range","metadata":{},"errorTraceType":"ansi","type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/plain":["\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n\u001B[0;31mIndexError\u001B[0m                                Traceback (most recent call last)\n\u001B[0;32m<command-1603522030302347>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m      9\u001B[0m        ]\n\u001B[1;32m     10\u001B[0m \u001B[0;31m#columns = [\"first_name\",\"middle_name\",\"last_name\",\"dob\",\"gender\",\"salary\"]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 11\u001B[0;31m \u001B[0mpysparkDF\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mspark\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mcreateDataFrame\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mdata\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mdata\u001B[0m \u001B[0;34m,\u001B[0m \u001B[0mschema\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mcolumns\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     12\u001B[0m \u001B[0mpysparkDF\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mprintSchema\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     13\u001B[0m \u001B[0mpysparkDF\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mshow\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mtruncate\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;32mFalse\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;32m/databricks/spark/python/pyspark/sql/session.py\u001B[0m in \u001B[0;36mcreateDataFrame\u001B[0;34m(self, data, schema, samplingRatio, verifySchema)\u001B[0m\n\u001B[1;32m    720\u001B[0m             return super(SparkSession, self).createDataFrame(\n\u001B[1;32m    721\u001B[0m                 data, schema, samplingRatio, verifySchema)\n\u001B[0;32m--> 722\u001B[0;31m         \u001B[0;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_create_dataframe\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mdata\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mschema\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0msamplingRatio\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mverifySchema\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    723\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    724\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0m_create_dataframe\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mdata\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mschema\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0msamplingRatio\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mverifySchema\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;32m/databricks/spark/python/pyspark/sql/session.py\u001B[0m in \u001B[0;36m_create_dataframe\u001B[0;34m(self, data, schema, samplingRatio, verifySchema)\u001B[0m\n\u001B[1;32m    752\u001B[0m                 \u001B[0mrdd\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mschema\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_createFromRDD\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mdata\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mmap\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mprepare\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mschema\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0msamplingRatio\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    753\u001B[0m             \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 754\u001B[0;31m                 \u001B[0mrdd\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mschema\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_createFromLocal\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mmap\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mprepare\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mdata\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mschema\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    755\u001B[0m             \u001B[0mjrdd\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_jvm\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mSerDeUtil\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mtoJavaArray\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mrdd\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_to_java_object_rdd\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    756\u001B[0m             \u001B[0mjdf\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_jsparkSession\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mapplySchemaToPythonRDD\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mjrdd\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mrdd\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mschema\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mjson\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;32m/databricks/spark/python/pyspark/sql/session.py\u001B[0m in \u001B[0;36m_createFromLocal\u001B[0;34m(self, data, schema)\u001B[0m\n\u001B[1;32m    528\u001B[0m         \u001B[0mwrite\u001B[0m \u001B[0mtemp\u001B[0m \u001B[0mfiles\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    529\u001B[0m         \"\"\"\n\u001B[0;32m--> 530\u001B[0;31m         \u001B[0mdata\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mschema\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_wrap_data_schema\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mdata\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mschema\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    531\u001B[0m         \u001B[0;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_sc\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mparallelize\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mdata\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mschema\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    532\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;32m/databricks/spark/python/pyspark/sql/session.py\u001B[0m in \u001B[0;36m_wrap_data_schema\u001B[0;34m(self, data, schema)\u001B[0m\n\u001B[1;32m    512\u001B[0m             \u001B[0;32mif\u001B[0m \u001B[0misinstance\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mschema\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m(\u001B[0m\u001B[0mlist\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mtuple\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    513\u001B[0m                 \u001B[0;32mfor\u001B[0m \u001B[0mi\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mname\u001B[0m \u001B[0;32min\u001B[0m \u001B[0menumerate\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mschema\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 514\u001B[0;31m                     \u001B[0mstruct\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mfields\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mi\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mname\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mname\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    515\u001B[0m                     \u001B[0mstruct\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mnames\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mi\u001B[0m\u001B[0;34m]\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mname\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    516\u001B[0m             \u001B[0mschema\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mstruct\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;31mIndexError\u001B[0m: list index out of range"]}}],"execution_count":0},{"cell_type":"code","source":["data1 = [((\"James\",\"\",\"Smith\"),\"36636\",\"M\",60000),\\\n        ((\"Michael\",\"Rose\",\"\"),\"40288\",\"M\",70000),\\\n        ((\"Robert\",\"\",\"Williams\"),\"42114\",\"\",400000),\\\n        ((\"Maria\",\"Anne\",\"Jones\"),\"39192\",\"F\",500000),\\\n        ((\"Jen\",\"Mary\",\"Brown\"),\"\",\"F\",0)\\\n]\n\ndatastruct = StructType([\n    StructField(\"name\",StructType([StructField('first_name',StringType(),True),\n                                   StructField('middle_name',StringType(),True),\n                                   StructField('last_name',StringType(),True)])),\n     StructField('dob', StringType(), True),\n         StructField('gender', StringType(), True),\n         StructField('salary', IntegerType(), True)\n])\n\ndataDF = spark.createDataFrame(data = data1 , schema = datastruct)\ndataDF.printSchema()\npandaDF = dataDF.toPandas()\nprint(pandaDF)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"174e1848-a75b-46c2-bd39-37031ad0af46"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"root\n |-- name: struct (nullable = true)\n |    |-- first_name: string (nullable = true)\n |    |-- middle_name: string (nullable = true)\n |    |-- last_name: string (nullable = true)\n |-- dob: string (nullable = true)\n |-- gender: string (nullable = true)\n |-- salary: integer (nullable = true)\n\n                                                name    dob gender  salary\n0  {'first_name': 'James', 'middle_name': '', 'la...  36636      M   60000\n1  {'first_name': 'Michael', 'middle_name': 'Rose...  40288      M   70000\n2  {'first_name': 'Robert', 'middle_name': '', 'l...  42114         400000\n3  {'first_name': 'Maria', 'middle_name': 'Anne',...  39192      F  500000\n4  {'first_name': 'Jen', 'middle_name': 'Mary', '...             F       0\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["root\n |-- name: struct (nullable = true)\n |    |-- first_name: string (nullable = true)\n |    |-- middle_name: string (nullable = true)\n |    |-- last_name: string (nullable = true)\n |-- dob: string (nullable = true)\n |-- gender: string (nullable = true)\n |-- salary: integer (nullable = true)\n\n                                                name    dob gender  salary\n0  {'first_name': 'James', 'middle_name': '', 'la...  36636      M   60000\n1  {'first_name': 'Michael', 'middle_name': 'Rose...  40288      M   70000\n2  {'first_name': 'Robert', 'middle_name': '', 'l...  42114         400000\n3  {'first_name': 'Maria', 'middle_name': 'Anne',...  39192      F  500000\n4  {'first_name': 'Jen', 'middle_name': 'Mary', '...             F       0\n"]}}],"execution_count":0},{"cell_type":"code","source":["PySpark show() – Display DataFrame Contents in Table"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"37d46a3f-f728-4088-a927-5cd030ee8612"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["import pyspark\nfrom pyspark.sql import SparkSession\nspark = SparkSession.builder.appName('Sparkbyexample.com').getOrCreate()\ncolumns = [\"Seqno\",\"Quote\"]\ndata = [(\"1\", \"Be the change that you wish to see in the world\"),\n    (\"2\", \"Everyone thinks of changing the world, but no one thinks of changing himself.\"),\n    (\"3\", \"The purpose of our lives is to be happy.\"),\n    (\"4\", \"Be cool.\")]\ndataFrame = spark.createDataFrame(data = data , schema = columns)\ndataFrame.printSchema()\ndataframe.show(trunacte = False)# diplay full content in column.\ndataframe.show(2,trunacte = False)# diplay 2 rows and  full content in  two columns.\ndataFrame.show(truncate = False,vertical=True)# diplay columns  and  values vertically."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"72539cfa-fdeb-4d56-8f42-0d6822303996"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"root\n |-- Seqno: string (nullable = true)\n |-- Quote: string (nullable = true)\n\n-RECORD 0------------------------------------------------------------------------------\n Seqno | 1                                                                             \n Quote | Be the change that you wish to see in the world                               \n-RECORD 1------------------------------------------------------------------------------\n Seqno | 2                                                                             \n Quote | Everyone thinks of changing the world, but no one thinks of changing himself. \n-RECORD 2------------------------------------------------------------------------------\n Seqno | 3                                                                             \n Quote | The purpose of our lives is to be happy.                                      \n-RECORD 3------------------------------------------------------------------------------\n Seqno | 4                                                                             \n Quote | Be cool.                                                                      \n\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["root\n |-- Seqno: string (nullable = true)\n |-- Quote: string (nullable = true)\n\n-RECORD 0------------------------------------------------------------------------------\n Seqno | 1                                                                             \n Quote | Be the change that you wish to see in the world                               \n-RECORD 1------------------------------------------------------------------------------\n Seqno | 2                                                                             \n Quote | Everyone thinks of changing the world, but no one thinks of changing himself. \n-RECORD 2------------------------------------------------------------------------------\n Seqno | 3                                                                             \n Quote | The purpose of our lives is to be happy.                                      \n-RECORD 3------------------------------------------------------------------------------\n Seqno | 4                                                                             \n Quote | Be cool.                                                                      \n\n"]}}],"execution_count":0},{"cell_type":"code","source":["PySpark StructType & StructField Explained with Examples"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"6b23a413-b777-4710-8089-ca58b529c7e4"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["import pyspark\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.types import StructType,StructField,StringType,IntegerType,DateType\nspark = SparkSession.builder.appName('SparkByExample.com').getOrCreate()\n                                    \ndata1 = [(\"James\",\"\",\"Smith\",\"36636\",\"M\",3000),\n    (\"Michael\",\"Rose\",\"\",\"40288\",\"M\",4000),\n    (\"Robert\",\"\",\"Williams\",\"42114\",\"M\",4000),\n    (\"Maria\",\"Anne\",\"Jones\",\"39192\",\"F\",4000),\n    (\"Jen\",\"Mary\",\"Brown\",\"\",\"F\",-1)\n        ]\n       \nschemastruct = StructType([\\\n                          StructField(\"Firstname\",StringType(),True),\\\n                    StructField(\"middlename\",StringType(),True),\\\n                    StructField(\"lastname\",StringType(),True),\\\n                    StructField(\"id\",StringType(),True),\\\n                    StructField(\"gender\",StringType(),True),\\\n                    StructField(\"salary\",IntegerType(),True)\\\n                   ])\ndf = spark.createDataFrame(data = data1, schema = schemastruct)\ndf.printSchema()\ndf.show(truncate = False)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"b46666d3-0737-43f9-a2a3-595562218cae"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"root\n |-- Firstname: string (nullable = true)\n |-- middlename: string (nullable = true)\n |-- lastname: string (nullable = true)\n |-- id: string (nullable = true)\n |-- gender: string (nullable = true)\n |-- salary: integer (nullable = true)\n\n+---------+----------+--------+-----+------+------+\n|Firstname|middlename|lastname|id   |gender|salary|\n+---------+----------+--------+-----+------+------+\n|James    |          |Smith   |36636|M     |3000  |\n|Michael  |Rose      |        |40288|M     |4000  |\n|Robert   |          |Williams|42114|M     |4000  |\n|Maria    |Anne      |Jones   |39192|F     |4000  |\n|Jen      |Mary      |Brown   |     |F     |-1    |\n+---------+----------+--------+-----+------+------+\n\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["root\n |-- Firstname: string (nullable = true)\n |-- middlename: string (nullable = true)\n |-- lastname: string (nullable = true)\n |-- id: string (nullable = true)\n |-- gender: string (nullable = true)\n |-- salary: integer (nullable = true)\n\n+---------+----------+--------+-----+------+------+\n|Firstname|middlename|lastname|id   |gender|salary|\n+---------+----------+--------+-----+------+------+\n|James    |          |Smith   |36636|M     |3000  |\n|Michael  |Rose      |        |40288|M     |4000  |\n|Robert   |          |Williams|42114|M     |4000  |\n|Maria    |Anne      |Jones   |39192|F     |4000  |\n|Jen      |Mary      |Brown   |     |F     |-1    |\n+---------+----------+--------+-----+------+------+\n\n"]}}],"execution_count":0},{"cell_type":"code","source":["4. Defining Nested StructType object struct\nWhile working on DataFrame we often need to work with the nested struct column and this can be defined using StructType.\n"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"12392b75-73f2-4e8d-931a-69efcdcfb52a"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["nesteddata = [((\"James\",\"\",\"Smith\"),\"36636\",\"M\",3000),\n    ((\"Michael\",\"Rose\",\"\"),\"40288\",\"M\",4000),\n    ((\"Robert\",\"\",\"Williams\"),\"42114\",\"M\",4000),\n    ((\"Maria\",\"Anne\",\"Jones\"),\"39192\",\"F\",4000),\n    ((\"Jen\",\"Mary\",\"Brown\"),\"\",\"F\",-1)\n        ]\nnestedstruct = StructType([\n    StructField(\"name\",StructType([StructField(\"Firstname\",StringType(),True),\n                                    StructField(\"middlename\",StringType(),True),\n                                    StructField(\"lastname\",StringType(),True) \n                                  ])),\n    StructField(\"id\",StringType(),True),\n    StructField(\"gender\",StringType(),True),\n    StructField(\"salary\",IntegerType(),True)\n    \n])\n\nDF2 = spark.createDataFrame(data = nesteddata, schema = nestedstruct)\nDF2.printSchema()\nDF2.show(truncate = False)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"3cd171fd-48e2-46b8-8f9f-170247dd0eb7"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"root\n |-- name: struct (nullable = true)\n |    |-- Firstname: string (nullable = true)\n |    |-- middlename: string (nullable = true)\n |    |-- lastname: string (nullable = true)\n |-- id: string (nullable = true)\n |-- gender: string (nullable = true)\n |-- salary: integer (nullable = true)\n\n+--------------------+-----+------+------+\n|name                |id   |gender|salary|\n+--------------------+-----+------+------+\n|{James, , Smith}    |36636|M     |3000  |\n|{Michael, Rose, }   |40288|M     |4000  |\n|{Robert, , Williams}|42114|M     |4000  |\n|{Maria, Anne, Jones}|39192|F     |4000  |\n|{Jen, Mary, Brown}  |     |F     |-1    |\n+--------------------+-----+------+------+\n\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["root\n |-- name: struct (nullable = true)\n |    |-- Firstname: string (nullable = true)\n |    |-- middlename: string (nullable = true)\n |    |-- lastname: string (nullable = true)\n |-- id: string (nullable = true)\n |-- gender: string (nullable = true)\n |-- salary: integer (nullable = true)\n\n+--------------------+-----+------+------+\n|name                |id   |gender|salary|\n+--------------------+-----+------+------+\n|{James, , Smith}    |36636|M     |3000  |\n|{Michael, Rose, }   |40288|M     |4000  |\n|{Robert, , Williams}|42114|M     |4000  |\n|{Maria, Anne, Jones}|39192|F     |4000  |\n|{Jen, Mary, Brown}  |     |F     |-1    |\n+--------------------+-----+------+------+\n\n"]}}],"execution_count":0},{"cell_type":"code","source":["5. Adding & Changing struct of the DataFrame\nUsing PySpark SQL function struct(), we can change the struct of the existing DataFrame and add a new StructType to it. The below example demonstrates how to copy the columns from one structure to another and adding a new column. PySpark Column Class also provides some functions to work with the StructType column.\n\nfrom pyspark.sql.functions import col,struct,when\nupdatedDF = df2.withColumn(\"OtherInfo\", \n    struct(col(\"id\").alias(\"identifier\"),\n    col(\"gender\").alias(\"gender\"),\n    col(\"salary\").alias(\"salary\"),\n    when(col(\"salary\").cast(IntegerType()) < 2000,\"Low\")\n      .when(col(\"salary\").cast(IntegerType()) < 4000,\"Medium\")\n      .otherwise(\"High\").alias(\"Salary_Grade\")\n  )).drop(\"id\",\"gender\",\"salary\")\n\nupdatedDF.printSchema()\nupdatedDF.show(truncate=False)\n"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"a57a386f-f474-4389-988c-9be38c787c5e"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["import pyspark\nfrom pyspark.sql.functions import col,struct,when\n\nupdateDF2 = DF2.withColumn(\"otherinfo\",\n                           struct(col(\"id\").alias(\"identifier\"),col(\"gender\").alias(\"gender\"),col(\"salary\").alias(\"salary\"),\n                                  when(col(\"salary\").cast(IntegerType()) < 2000,\"Low\").when(col(\"salary\").cast(IntegerType()) < 4000,\"Medium\").otherwise(\"High\").alias(\"Salary_Grade\") ) ).drop(\"id\",\"gender\",\"salary\")\n\nupdateDF2.printSchema()\nupdateDF2.show(truncate = False)\n"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"e6ee7a04-72a4-4c32-9290-a2edf8cdf1a1"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"root\n |-- name: struct (nullable = true)\n |    |-- Firstname: string (nullable = true)\n |    |-- middlename: string (nullable = true)\n |    |-- lastname: string (nullable = true)\n |-- otherinfo: struct (nullable = false)\n |    |-- identifier: string (nullable = true)\n |    |-- gender: string (nullable = true)\n |    |-- salary: integer (nullable = true)\n |    |-- Salary_Grade: string (nullable = false)\n\n+--------------------+------------------------+\n|name                |otherinfo               |\n+--------------------+------------------------+\n|{James, , Smith}    |{36636, M, 3000, Medium}|\n|{Michael, Rose, }   |{40288, M, 4000, High}  |\n|{Robert, , Williams}|{42114, M, 4000, High}  |\n|{Maria, Anne, Jones}|{39192, F, 4000, High}  |\n|{Jen, Mary, Brown}  |{, F, -1, Low}          |\n+--------------------+------------------------+\n\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["root\n |-- name: struct (nullable = true)\n |    |-- Firstname: string (nullable = true)\n |    |-- middlename: string (nullable = true)\n |    |-- lastname: string (nullable = true)\n |-- otherinfo: struct (nullable = false)\n |    |-- identifier: string (nullable = true)\n |    |-- gender: string (nullable = true)\n |    |-- salary: integer (nullable = true)\n |    |-- Salary_Grade: string (nullable = false)\n\n+--------------------+------------------------+\n|name                |otherinfo               |\n+--------------------+------------------------+\n|{James, , Smith}    |{36636, M, 3000, Medium}|\n|{Michael, Rose, }   |{40288, M, 4000, High}  |\n|{Robert, , Williams}|{42114, M, 4000, High}  |\n|{Maria, Anne, Jones}|{39192, F, 4000, High}  |\n|{Jen, Mary, Brown}  |{, F, -1, Low}          |\n+--------------------+------------------------+\n\n"]}}],"execution_count":0},{"cell_type":"code","source":["6. Using SQL ArrayType and MapType\nSQL StructType also supports ArrayType and MapType to define the DataFrame columns for array and map collections respectively. On the below example, column hobbies defined as ArrayType(StringType) and properties defined as MapType(StringType,StringType) meaning both key and value as String."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"4f585692-e3c0-4190-8904-51f6c9cb73f6"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["import pyspark\nfrom pyspark.sql.types import ArrayType,MapType\narraystructschema = StructType([StructField(\"name\",StructType([StructField(\"firstname\",StringType(),True),\n                                                              StructField(\"middlename\",StringType(),True),\n                                                              StructField(\"lastname\",StringType(),True)])),\n                                StructField(\"hobbies\",ArrayType(StringType()),True),\n                                StructField(\"properties\",MapType(StringType(),StringType()),True)\n    \n]\n)\n\n#df3 = spark.createDataFrame(RDD ,schema = arraystructschema)\n#print(df2.schema.json)\n#df3.printSchema()\n#df3.Show(truncate= False)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"dcbc65c2-5728-4b9e-843c-4d51d783178b"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["\nimport json\nschemaFromJson = StructType.fromJson(json.loads(schema.json))\ndf3 = spark.createDataFrame(\n        spark.sparkContext.parallelize(structureData),schemaFromJson)\ndf3.printSchema()\n"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"76b416d5-4ce0-470c-a124-134a82306579"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)\n\u001B[0;32m<command-76370094242890>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[0;32mimport\u001B[0m \u001B[0mjson\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 2\u001B[0;31m \u001B[0mschemaFromJson\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mStructType\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mfromJson\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mjson\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mloads\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mschema\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mjson\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      3\u001B[0m df3 = spark.createDataFrame(\n\u001B[1;32m      4\u001B[0m         spark.sparkContext.parallelize(structureData),schemaFromJson)\n\u001B[1;32m      5\u001B[0m \u001B[0mdf3\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mprintSchema\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;31mNameError\u001B[0m: name 'schema' is not defined","errorSummary":"<span class='ansi-red-fg'>NameError</span>: name 'schema' is not defined","metadata":{},"errorTraceType":"ansi","type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/plain":["\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)\n\u001B[0;32m<command-76370094242890>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[0;32mimport\u001B[0m \u001B[0mjson\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 2\u001B[0;31m \u001B[0mschemaFromJson\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mStructType\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mfromJson\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mjson\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mloads\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mschema\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mjson\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      3\u001B[0m df3 = spark.createDataFrame(\n\u001B[1;32m      4\u001B[0m         spark.sparkContext.parallelize(structureData),schemaFromJson)\n\u001B[1;32m      5\u001B[0m \u001B[0mdf3\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mprintSchema\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;31mNameError\u001B[0m: name 'schema' is not defined"]}}],"execution_count":0},{"cell_type":"code","source":["from pyspark.sql.functions import fromDDL\n  ddlSchemaStr = \"'fullName' STRUCT<'first': STRING, 'last': STRING,'middle': STRING>,'age' INT,'gender' STRING\"\n  ddlSchema = StructType.fromDDL(ddlSchemaStr)\n  ddlSchema.printTreeString()\n"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"3f9dedff-6240-4f6c-b781-b662aa349418"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"\u001B[0;36m  File \u001B[0;32m\"<command-76370094242892>\"\u001B[0;36m, line \u001B[0;32m2\u001B[0m\n\u001B[0;31m    ddlSchemaStr = \"'fullName' STRUCT<'first': STRING, 'last': STRING,'middle': STRING>,'age' INT,'gender' STRING\"\u001B[0m\n\u001B[0m    ^\u001B[0m\n\u001B[0;31mIndentationError\u001B[0m\u001B[0;31m:\u001B[0m unexpected indent\n","errorSummary":"<span class='ansi-red-fg'>IndentationError</span>: unexpected indent (<command-76370094242892>, line 2)","metadata":{},"errorTraceType":"ansi","type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/plain":["\u001B[0;36m  File \u001B[0;32m\"<command-76370094242892>\"\u001B[0;36m, line \u001B[0;32m2\u001B[0m\n\u001B[0;31m    ddlSchemaStr = \"'fullName' STRUCT<'first': STRING, 'last': STRING,'middle': STRING>,'age' INT,'gender' STRING\"\u001B[0m\n\u001B[0m    ^\u001B[0m\n\u001B[0;31mIndentationError\u001B[0m\u001B[0;31m:\u001B[0m unexpected indent\n"]}}],"execution_count":0},{"cell_type":"code","source":["PySpark Row using on DataFrame and RDD\nIn PySpark Row class is available by importing pyspark.sql.Row which is represented as a record/row in DataFrame, one can create a Row object by using named arguments, or create a custom Row like class. In this article I will explain how to use Row class on RDD, DataFrame and its functions.\n\nBefore we start using it on RDD & DataFrame, let’s understand some basics of Row class.\nTo enable sorting by names, set the environment variable PYSPARK_ROW_FIELD_SORTING_ENABLED to true.\n1. Create a Row Object\nRow class extends the tuple hence it takes variable number of arguments, Row() is used to create the row object. Once the row object created, we can retrieve the data from Row using index similar to tuple.\n\nfrom pyspark.sql import Row\nrow=Row(\"James\",40)\nprint(row[0] +\",\"+str(row[1]))\n"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"2c4845d8-0ff4-4829-8459-a38f0d5d285b"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["from pyspark.sql import Row\nrow = Row(\"james\",40)\nprint(row[0] +\",\"+ str(row[1]))"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"3cd94ae1-3567-4563-8b95-a5cc2765b20f"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"james,40\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["james,40\n"]}}],"execution_count":0},{"cell_type":"code","source":["row = Row(name = \"james\", age = 11)\nprint(row.name,\\\n      row.age)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"38a5d0e2-73c6-4205-b135-23b09b4402be"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"james 11\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["james 11\n"]}}],"execution_count":0},{"cell_type":"code","source":["2. Create Custom Class from Row\nWe can also create a Row like class, for example “Person” and use it similar to Row object. This would be helpful when you wanted to create real time object and refer it’s properties. On below example, we have created a Person class and used similar to Row.\n\n\nPerson = Row(\"name\", \"age\")\np1=Person(\"James\", 40)\np2=Person(\"Alice\", 35)\nprint(p1.name +\",\"+p2.name)\n"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"9c2bbb05-288a-441e-9f15-d2d9aebb5a6f"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["person= Row(\"name\",\"age\")\np1 = person(\"james\",40)\np2=person(\"krishna\",38)\nprint(p1.name+\",\"+p2.name)\nprint(str(p1.age)+\",\"+str(p2.age))"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"e09d130a-5f3b-4bd5-a941-3a3e7c6a5a57"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"james,krishna\n40,38\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["james,krishna\n40,38\n"]}}],"execution_count":0},{"cell_type":"code","source":["\nPerson=Row(\"name\",\"lang\",\"state\")\ndata = [Person(\"James,,Smith\",[\"Java\",\"Scala\",\"C++\"],\"CA\"), \n    Person(\"Michael,Rose,\",[\"Spark\",\"Java\",\"C++\"],\"NJ\"),\n    Person(\"Robert,,Williams\",[\"CSharp\",\"VB\"],\"NV\")]\n"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"fde67e6d-592c-475a-95b8-122e997fb078"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)\n\u001B[0;32m<command-76370094242898>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[0;32m----> 1\u001B[0;31m \u001B[0mPerson\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mRow\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"name\"\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\"lang\"\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\"state\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      2\u001B[0m data = [Person(\"James,,Smith\",[\"Java\",\"Scala\",\"C++\"],\"CA\"), \n\u001B[1;32m      3\u001B[0m     \u001B[0mPerson\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"Michael,Rose,\"\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;34m\"Spark\"\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\"Java\"\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\"C++\"\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\"NJ\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      4\u001B[0m     Person(\"Robert,,Williams\",[\"CSharp\",\"VB\"],\"NV\")]\n\n\u001B[0;31mNameError\u001B[0m: name 'Row' is not defined","errorSummary":"<span class='ansi-red-fg'>NameError</span>: name 'Row' is not defined","metadata":{},"errorTraceType":"ansi","type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/plain":["\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)\n\u001B[0;32m<command-76370094242898>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[0;32m----> 1\u001B[0;31m \u001B[0mPerson\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mRow\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"name\"\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\"lang\"\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\"state\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      2\u001B[0m data = [Person(\"James,,Smith\",[\"Java\",\"Scala\",\"C++\"],\"CA\"), \n\u001B[1;32m      3\u001B[0m     \u001B[0mPerson\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"Michael,Rose,\"\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;34m\"Spark\"\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\"Java\"\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\"C++\"\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\"NJ\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      4\u001B[0m     Person(\"Robert,,Williams\",[\"CSharp\",\"VB\"],\"NV\")]\n\n\u001B[0;31mNameError\u001B[0m: name 'Row' is not defined"]}}],"execution_count":0},{"cell_type":"code","source":["import pyspark\nfrom pyspark.sql import Row\nPerson = Row(\"name\",\"lang\",\"state\")\ndata = [Person(\"james,,smith\",[\"java\",\"Scala\",\"C++\"],\"CA\"),\n        Person(\"Krishna,,Reddy\",[\"dotnet\",\"sql\",\"python\"],\"IND\"),\n        Person(\"Mani,,Reddy\",[\"telugu\",\"english\",\"Hindi\"],\"USA\")\n       ]\nDF4 = spark.createDataFrame(data= data)\nDF4.printSchema()\nDF4.show(truncate = False)\n\ncolumns = [\"name\",\"languagesAtSchool\",\"currentState\"]\ndf=spark.createDataFrame(data).toDF(*columns)\ndf.printSchema()\n"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"efe90234-f7ef-461d-a5b5-1babc0393239"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"root\n |-- name: string (nullable = true)\n |-- lang: array (nullable = true)\n |    |-- element: string (containsNull = true)\n |-- state: string (nullable = true)\n\n+--------------+------------------------+-----+\n|name          |lang                    |state|\n+--------------+------------------------+-----+\n|james,,smith  |[java, Scala, C++]      |CA   |\n|Krishna,,Reddy|[dotnet, sql, python]   |IND  |\n|Mani,,Reddy   |[telugu, english, Hindi]|USA  |\n+--------------+------------------------+-----+\n\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["root\n |-- name: string (nullable = true)\n |-- lang: array (nullable = true)\n |    |-- element: string (containsNull = true)\n |-- state: string (nullable = true)\n\n+--------------+------------------------+-----+\n|name          |lang                    |state|\n+--------------+------------------------+-----+\n|james,,smith  |[java, Scala, C++]      |CA   |\n|Krishna,,Reddy|[dotnet, sql, python]   |IND  |\n|Mani,,Reddy   |[telugu, english, Hindi]|USA  |\n+--------------+------------------------+-----+\n\n"]}}],"execution_count":0},{"cell_type":"code","source":["# change columns names\ncolumns = [\"name\",\"languageAtSchool\",\"currentState\"]\ndf5 = spark.createDataFrame(data).toDF(*columns)\ndf5.printSchema()\ndf5.show(truncate = False)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"23cba105-801b-41ed-8f28-fb1c34823ffb"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"root\n |-- name: string (nullable = true)\n |-- languageAtSchool: array (nullable = true)\n |    |-- element: string (containsNull = true)\n |-- currentState: string (nullable = true)\n\n+--------------+------------------------+------------+\n|name          |languageAtSchool        |currentState|\n+--------------+------------------------+------------+\n|james,,smith  |[java, Scala, C++]      |CA          |\n|Krishna,,Reddy|[dotnet, sql, python]   |IND         |\n|Mani,,Reddy   |[telugu, english, Hindi]|USA         |\n+--------------+------------------------+------------+\n\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["root\n |-- name: string (nullable = true)\n |-- languageAtSchool: array (nullable = true)\n |    |-- element: string (containsNull = true)\n |-- currentState: string (nullable = true)\n\n+--------------+------------------------+------------+\n|name          |languageAtSchool        |currentState|\n+--------------+------------------------+------------+\n|james,,smith  |[java, Scala, C++]      |CA          |\n|Krishna,,Reddy|[dotnet, sql, python]   |IND         |\n|Mani,,Reddy   |[telugu, english, Hindi]|USA         |\n+--------------+------------------------+------------+\n\n"]}}],"execution_count":0},{"cell_type":"code","source":["5. Create Nested Struct Using Row Class\nThe below example provides a way to create a struct type using the Row class. Alternatively, you can also create struct type using By Providing Schema using PySpark StructType & StructFields\n\n#Create DataFrame with struct using Row class\nfrom pyspark.sql import Row\ndata=[Row(name=\"James\",prop=Row(hair=\"black\",eye=\"blue\")),\n      Row(name=\"Ann\",prop=Row(hair=\"grey\",eye=\"black\"))]\ndf=spark.createDataFrame(data)\ndf.printSchema()\n"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"50661ecc-7739-4036-8c05-0ad2e777086f"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["data = [Row(name = \"krishna\",prop = Row(hair = \"white\",eye='black')),\n       Row(name = \"mani\",prop = Row(hair = \"black\",eye = 'red'))\n       ]\nDF6 =  spark.createDataFrame(data= data)\nDF6.printSchema()\nDF6.show(truncate= False)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"e1caaead-f849-464d-bbcf-60a660d22589"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"root\n |-- name: string (nullable = true)\n |-- prop: struct (nullable = true)\n |    |-- hair: string (nullable = true)\n |    |-- eye: string (nullable = true)\n\n+-------+--------------+\n|name   |prop          |\n+-------+--------------+\n|krishna|{white, black}|\n|mani   |{black, red}  |\n+-------+--------------+\n\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["root\n |-- name: string (nullable = true)\n |-- prop: struct (nullable = true)\n |    |-- hair: string (nullable = true)\n |    |-- eye: string (nullable = true)\n\n+-------+--------------+\n|name   |prop          |\n+-------+--------------+\n|krishna|{white, black}|\n|mani   |{black, red}  |\n+-------+--------------+\n\n"]}}],"execution_count":0},{"cell_type":"code","source":["PySpark Column Class | Operators & Functions\npyspark.sql.Column class provides several functions to work with DataFrame to manipulate the Column values, evaluate the boolean expression to filter rows, retrieve a value or part of a value from a DataFrame column, and to work with list, map & struct columns.\n\nIn this article, I will cover how to create Column object, access them to perform operations, and finally most used PySpark Column Functions with Examples.\n1. Create Column Class Object\nOne of the simplest ways to create a Column class object is by using PySpark lit() SQL function, this takes a literal value and returns a Column object.\n\nfrom pyspark.sql.functions import lit\ncolObj = lit(\"sparkbyexamples.com\")\n\n\ndata=[(\"James\",23),(\"Ann\",40)]\ndf=spark.createDataFrame(data).toDF(\"name.fname\",\"gender\")\ndf.printSchema()\n#root\n# |-- name.fname: string (nullable = true)\n# |-- gender: long (nullable = true)\n\n# Using DataFrame object (df)\ndf.select(df.gender).show()\ndf.select(df[\"gender\"]).show()\n#Accessing column name with dot (with backticks)\ndf.select(df[\"`name.fname`\"]).show()\n\n#Using SQL col() function\nfrom pyspark.sql.functions import col\ndf.select(col(\"gender\")).show()\n#Accessing column name with dot (with backticks)\ndf.select(col(\"`name.fname`\")).show()\n\n"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"727e7f5a-cea0-4bcb-a6e9-9864c9d537f1"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["import pyspark\nfrom pyspark.sql.functions import lit,col\n\ncolObj = lit(\"sparkbyexamples.com\")\n\ndata = [(\"james\",23),(\"Ann\",40)]\ndf7 = spark.createDataFrame(data).toDF(\"name.fname\",\"age\")\ndf7.printSchema()\ndf7.select(df7[\"age\"]).show()\ndf7.select(df7.age).show()\ndf7.select(df7[\"`name.fname`\"]).show()\n#Using SQL col() function\ndf7.select(col(\"age\")).show()\ndf7.select(col(\"`name.fname`\")).show()\n"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"24160419-55d0-4bf9-9b4a-1f94d766b610"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"root\n |-- name.fname: string (nullable = true)\n |-- age: long (nullable = true)\n\n+---+\n|age|\n+---+\n| 23|\n| 40|\n+---+\n\n+---+\n|age|\n+---+\n| 23|\n| 40|\n+---+\n\n+----------+\n|name.fname|\n+----------+\n|     james|\n|       Ann|\n+----------+\n\n+---+\n|age|\n+---+\n| 23|\n| 40|\n+---+\n\n+----------+\n|name.fname|\n+----------+\n|     james|\n|       Ann|\n+----------+\n\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["root\n |-- name.fname: string (nullable = true)\n |-- age: long (nullable = true)\n\n+---+\n|age|\n+---+\n| 23|\n| 40|\n+---+\n\n+---+\n|age|\n+---+\n| 23|\n| 40|\n+---+\n\n+----------+\n|name.fname|\n+----------+\n|     james|\n|       Ann|\n+----------+\n\n+---+\n|age|\n+---+\n| 23|\n| 40|\n+---+\n\n+----------+\n|name.fname|\n+----------+\n|     james|\n|       Ann|\n+----------+\n\n"]}}],"execution_count":0},{"cell_type":"code","source":["Below example demonstrates accessing struct type columns. Here I have use PySpark Row class to create a struct type. Alternatively you can also create it by using PySpark StructType & StructField classes\n\n\n#Create DataFrame with struct using Row class\nfrom pyspark.sql import Row\ndata=[Row(name=\"James\",prop=Row(hair=\"black\",eye=\"blue\")),\n      Row(name=\"Ann\",prop=Row(hair=\"grey\",eye=\"black\"))]\ndf=spark.createDataFrame(data)\ndf.printSchema()\n#root\n# |-- name: string (nullable = true)\n# |-- prop: struct (nullable = true)\n# |    |-- hair: string (nullable = true)\n# |    |-- eye: string (nullable = true)\n\n#Access struct column\ndf.select(df.prop.hair).show()\ndf.select(df[\"prop.hair\"]).show()\ndf.select(col(\"prop.hair\")).show()\n\n#Access all columns from struct\ndf.select(col(\"prop.*\")).show()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"43482c4f-3205-4739-beca-18f3ade54dfd"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["from pyspark.sql import Row\ndata1 = [Row(name = \"james\",prop = Row(hair = \"black\",eye=\"black\")),\n        Row(name = \"anna\",prop = Row(hair= 'white',eye = \"red\"))\n        ]\ndf8 = spark.createDataFrame(data = data1)\ndf8.printSchema()\n#df8.select(df8.prop.hair).show()\n#df8.select(df8.prop.eye).show()\ndf8.select(df8[\"prop.hair\"]).show()\ndf8.select(col(\"prop.hair\")).show()\ndf8.select(col(\"prop.*\")).show()\ndf8.select(col(\"prop.*\")).show()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"4be4ba05-8425-49fd-8493-7e1d866abd1c"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"root\n |-- name: string (nullable = true)\n |-- prop: struct (nullable = true)\n |    |-- hair: string (nullable = true)\n |    |-- eye: string (nullable = true)\n\n+-----+\n| hair|\n+-----+\n|black|\n|white|\n+-----+\n\n+-----+\n| hair|\n+-----+\n|black|\n|white|\n+-----+\n\n+-----+-----+\n| hair|  eye|\n+-----+-----+\n|black|black|\n|white|  red|\n+-----+-----+\n\n+-----+-----+\n| hair|  eye|\n+-----+-----+\n|black|black|\n|white|  red|\n+-----+-----+\n\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["root\n |-- name: string (nullable = true)\n |-- prop: struct (nullable = true)\n |    |-- hair: string (nullable = true)\n |    |-- eye: string (nullable = true)\n\n+-----+\n| hair|\n+-----+\n|black|\n|white|\n+-----+\n\n+-----+\n| hair|\n+-----+\n|black|\n|white|\n+-----+\n\n+-----+-----+\n| hair|  eye|\n+-----+-----+\n|black|black|\n|white|  red|\n+-----+-----+\n\n+-----+-----+\n| hair|  eye|\n+-----+-----+\n|black|black|\n|white|  red|\n+-----+-----+\n\n"]}}],"execution_count":0},{"cell_type":"code","source":["2. PySpark Column Operators\nPySpark column also provides a way to do arithmetic operations on columns using operators.\n\ndata=[(100,2,1),(200,3,4),(300,4,4)]\ndf=spark.createDataFrame(data).toDF(\"col1\",\"col2\",\"col3\")\n\n#Arthmetic operations\ndf.select(df.col1 + df.col2).show()\ndf.select(df.col1 - df.col2).show() \ndf.select(df.col1 * df.col2).show()\ndf.select(df.col1 / df.col2).show()\ndf.select(df.col1 % df.col2).show()\n\ndf.select(df.col2 > df.col3).show()\ndf.select(df.col2 < df.col3).show()\ndf.select(df.col2 == df.col3).show()\n"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"a3b3b0e0-35ae-4fdf-9727-27a9ddcb1e38"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["data2 = [(100,2,1),(200,3,4),(300,4,4)]\ndf9 = spark.createDataFrame(data = data2).toDF(\"col1\",\"col2\",\"col3\")\ndf9.printSchema()\ndf9.select(df9.col1 + df9.col2).show()\n#df9.select(col(\"col1\") + col(\"col2\")).show()\n#df9.select(df9.col1 - df9.col2).show()\n#df9.select(col(\"col1\")-col(\"col2\")).show()\ndf9.select(df9.col1 * df9.col2).show()\ndf9.select(df9.col1>df9.col2).show()\ndf9.select(col(\"col1\") == col(\"col2\")).show()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"bc22f7ec-9c5b-4da1-aefd-e1d677735e46"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"root\n |-- col1: long (nullable = true)\n |-- col2: long (nullable = true)\n |-- col3: long (nullable = true)\n\n+-------------+\n|(col1 + col2)|\n+-------------+\n|          102|\n|          203|\n|          304|\n+-------------+\n\n+-------------+\n|(col1 * col2)|\n+-------------+\n|          200|\n|          600|\n|         1200|\n+-------------+\n\n+-------------+\n|(col1 > col2)|\n+-------------+\n|         true|\n|         true|\n|         true|\n+-------------+\n\n+-------------+\n|(col1 = col2)|\n+-------------+\n|        false|\n|        false|\n|        false|\n+-------------+\n\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["root\n |-- col1: long (nullable = true)\n |-- col2: long (nullable = true)\n |-- col3: long (nullable = true)\n\n+-------------+\n|(col1 + col2)|\n+-------------+\n|          102|\n|          203|\n|          304|\n+-------------+\n\n+-------------+\n|(col1 * col2)|\n+-------------+\n|          200|\n|          600|\n|         1200|\n+-------------+\n\n+-------------+\n|(col1 > col2)|\n+-------------+\n|         true|\n|         true|\n|         true|\n+-------------+\n\n+-------------+\n|(col1 = col2)|\n+-------------+\n|        false|\n|        false|\n|        false|\n+-------------+\n\n"]}}],"execution_count":0},{"cell_type":"code","source":["4.1 alias() – Set’s name to Column\nOn below example df.fname refers to Column object and alias() is a function of the Column to give alternate name. Here, fname column has been changed to first_name & lname to last_name.\n\nOn second example I have use PySpark expr() function to concatenate columns and named column as fullName.\ndata=[(\"James\",\"Bond\",\"100\",None),\n      (\"Ann\",\"Varsa\",\"200\",'F'),\n      (\"Tom Cruise\",\"XXX\",\"400\",''),\n      (\"Tom Brand\",None,\"400\",'M')] \ncolumns=[\"fname\",\"lname\",\"id\",\"gender\"]\ndf=spark.createDataFrame(data,columns)\n#alias\nfrom pyspark.sql.functions import expr\ndf.select(df.fname.alias(\"first_name\"), \\\n          df.lname.alias(\"last_name\")\n   ).show()\n\n#Another example\ndf.select(expr(\" fname ||','|| lname\").alias(\"fullName\") \\\n   ).show()\n"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"8cb76d4c-9b27-48cb-93f5-83c3052c5127"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["from pyspark.sql.functions import expr\ndata10=[(\"James\",\"Bond\",\"100\",None),\n      (\"Ann\",\"Varsa\",\"200\",'F'),\n      (\"Tom Cruise\",\"XXX\",\"400\",''),\n      (\"Tom Brand\",None,\"400\",'M')]\ndf11 = spark.createDataFrame(data = data10).toDF(\"fname\",\"lname\",\"id\",\"gender\")\n#df11.printSchema()\n#df11.show()\ndf11.select(df11.fname.alias(\"first_name\"),\\\n           df11.lname.alias(\"last_name\")\n           ).show()\ndf11.select(expr(\"fname ||','|| lname\").alias(\"fullname\")).show()\ndf11.select(col(\"fname\").alias(\"first_name\")\n,col(\"fname\").alias(\"lastname\")).show()\n"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"68adac7c-166c-4967-a404-e6efb85b4783"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"+----------+---------+\n|first_name|last_name|\n+----------+---------+\n|     James|     Bond|\n|       Ann|    Varsa|\n|Tom Cruise|      XXX|\n| Tom Brand|     null|\n+----------+---------+\n\n+--------------+\n|      fullname|\n+--------------+\n|    James,Bond|\n|     Ann,Varsa|\n|Tom Cruise,XXX|\n|          null|\n+--------------+\n\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["+----------+---------+\n|first_name|last_name|\n+----------+---------+\n|     James|     Bond|\n|       Ann|    Varsa|\n|Tom Cruise|      XXX|\n| Tom Brand|     null|\n+----------+---------+\n\n+--------------+\n|      fullname|\n+--------------+\n|    James,Bond|\n|     Ann,Varsa|\n|Tom Cruise,XXX|\n|          null|\n+--------------+\n\n"]}},{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)\n\u001B[0;32m<command-3543374590422074>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m     11\u001B[0m            ).show()\n\u001B[1;32m     12\u001B[0m \u001B[0mdf11\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mselect\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mexpr\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"fname ||','|| lname\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0malias\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"fullname\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mshow\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 13\u001B[0;31m df11.select(col(\"fname\").alias(\"first_name\")\n\u001B[0m\u001B[1;32m     14\u001B[0m ,col(\"fname\").alias(\"lastname\")).show()\n\n\u001B[0;31mNameError\u001B[0m: name 'col' is not defined","errorSummary":"<span class='ansi-red-fg'>NameError</span>: name 'col' is not defined","metadata":{},"errorTraceType":"ansi","type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/plain":["\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)\n\u001B[0;32m<command-3543374590422074>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m     11\u001B[0m            ).show()\n\u001B[1;32m     12\u001B[0m \u001B[0mdf11\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mselect\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mexpr\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"fname ||','|| lname\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0malias\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"fullname\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mshow\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 13\u001B[0;31m df11.select(col(\"fname\").alias(\"first_name\")\n\u001B[0m\u001B[1;32m     14\u001B[0m ,col(\"fname\").alias(\"lastname\")).show()\n\n\u001B[0;31mNameError\u001B[0m: name 'col' is not defined"]}}],"execution_count":0},{"cell_type":"code","source":["4.2 asc() & desc() – Sort the DataFrame columns by Ascending or Descending order.\n\n#asc, desc to sort ascending and descending order repsectively.\ndf.sort(df.fname.asc()).show()\ndf.sort(df.fname.desc()).show()\n"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"83996fde-ae15-4653-8ac7-d1e752a40b60"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["df11.sort(df11.fname.asc()).show()\ndf11.sort(df11.lname.asc()).show()\ndf11.sort(df11.id.desc()).show()\ndf11.sort(df11.gender.desc()).show()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"a073c4a3-65e8-4629-b54d-c81fe6eb4a60"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"+----------+-----+---+------+\n|     fname|lname| id|gender|\n+----------+-----+---+------+\n|       Ann|Varsa|200|     F|\n|     James| Bond|100|  null|\n| Tom Brand| null|400|     M|\n|Tom Cruise|  XXX|400|      |\n+----------+-----+---+------+\n\n+----------+-----+---+------+\n|     fname|lname| id|gender|\n+----------+-----+---+------+\n| Tom Brand| null|400|     M|\n|     James| Bond|100|  null|\n|       Ann|Varsa|200|     F|\n|Tom Cruise|  XXX|400|      |\n+----------+-----+---+------+\n\n+----------+-----+---+------+\n|     fname|lname| id|gender|\n+----------+-----+---+------+\n| Tom Brand| null|400|     M|\n|Tom Cruise|  XXX|400|      |\n|       Ann|Varsa|200|     F|\n|     James| Bond|100|  null|\n+----------+-----+---+------+\n\n+----------+-----+---+------+\n|     fname|lname| id|gender|\n+----------+-----+---+------+\n| Tom Brand| null|400|     M|\n|       Ann|Varsa|200|     F|\n|Tom Cruise|  XXX|400|      |\n|     James| Bond|100|  null|\n+----------+-----+---+------+\n\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["+----------+-----+---+------+\n|     fname|lname| id|gender|\n+----------+-----+---+------+\n|       Ann|Varsa|200|     F|\n|     James| Bond|100|  null|\n| Tom Brand| null|400|     M|\n|Tom Cruise|  XXX|400|      |\n+----------+-----+---+------+\n\n+----------+-----+---+------+\n|     fname|lname| id|gender|\n+----------+-----+---+------+\n| Tom Brand| null|400|     M|\n|     James| Bond|100|  null|\n|       Ann|Varsa|200|     F|\n|Tom Cruise|  XXX|400|      |\n+----------+-----+---+------+\n\n+----------+-----+---+------+\n|     fname|lname| id|gender|\n+----------+-----+---+------+\n| Tom Brand| null|400|     M|\n|Tom Cruise|  XXX|400|      |\n|       Ann|Varsa|200|     F|\n|     James| Bond|100|  null|\n+----------+-----+---+------+\n\n+----------+-----+---+------+\n|     fname|lname| id|gender|\n+----------+-----+---+------+\n| Tom Brand| null|400|     M|\n|       Ann|Varsa|200|     F|\n|Tom Cruise|  XXX|400|      |\n|     James| Bond|100|  null|\n+----------+-----+---+------+\n\n"]}}],"execution_count":0},{"cell_type":"code","source":["4.3 cast() & astype() – Used to convert the data Type.\n\n#cast\ndf.select(df.fname,df.id.cast(\"int\")).printSchema()\n4.4 between() – Returns a Boolean expression when a column values in between lower and upper bound.\n\n#between\ndf.filter(df.id.between(100,300)).show()\n4.5 contains() – Checks if a DataFrame column value contains a a value specified in this function.\n\n#contains\ndf.filter(df.fname.contains(\"Cruise\")).show()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"dc744c53-26d8-4b28-8a46-e4903454de48"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["#f11.select(df11.fname,df11.id.cast(\"int\")).show()\ndf11.select(df11.lname,df11.id.cast(\"int\")).printSchema()\ndf11.select(df11.gender,df11.id.astype(\"string\")).printSchema()\n#between\ndf11.select(df11.id.between(100,300)).show()\ndf11.select(df11.id,df11.id.between(100,300)).show()\n#contains\ndf11.select(df11.fname.contains(\"Ann\")).show()\n#f11.select(df11.lname.conatins(\"Cruise\")).show()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"c12a35c5-c474-44d0-84c4-a235e081c492"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"root\n |-- lname: string (nullable = true)\n |-- id: integer (nullable = true)\n\nroot\n |-- gender: string (nullable = true)\n |-- id: string (nullable = true)\n\n+-----------------------------+\n|((id >= 100) AND (id <= 300))|\n+-----------------------------+\n|                         true|\n|                         true|\n|                        false|\n|                        false|\n+-----------------------------+\n\n+---+-----------------------------+\n| id|((id >= 100) AND (id <= 300))|\n+---+-----------------------------+\n|100|                         true|\n|200|                         true|\n|400|                        false|\n|400|                        false|\n+---+-----------------------------+\n\n+--------------------+\n|contains(fname, Ann)|\n+--------------------+\n|               false|\n|                true|\n|               false|\n|               false|\n+--------------------+\n\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["root\n |-- lname: string (nullable = true)\n |-- id: integer (nullable = true)\n\nroot\n |-- gender: string (nullable = true)\n |-- id: string (nullable = true)\n\n+-----------------------------+\n|((id >= 100) AND (id <= 300))|\n+-----------------------------+\n|                         true|\n|                         true|\n|                        false|\n|                        false|\n+-----------------------------+\n\n+---+-----------------------------+\n| id|((id >= 100) AND (id <= 300))|\n+---+-----------------------------+\n|100|                         true|\n|200|                         true|\n|400|                        false|\n|400|                        false|\n+---+-----------------------------+\n\n+--------------------+\n|contains(fname, Ann)|\n+--------------------+\n|               false|\n|                true|\n|               false|\n|               false|\n+--------------------+\n\n"]}}],"execution_count":0},{"cell_type":"code","source":["4.6 startswith() & endswith() – Checks if the value of the DataFrame Column starts and ends with a String respectively.\n\n#startswith, endswith()\ndf.filter(df.fname.startswith(\"T\")).show()\ndf.filter(df.fname.endswith(\"Cruise\")).show()\n4.7 eqNullSafe() –\n\n\n4.8 isNull & isNotNull() – Checks if the DataFrame column has NULL or non NULL values.\nRefer to\n\n\n#isNull & isNotNull\ndf.filter(df.lname.isNull()).show()\ndf.filter(df.lname.isNotNull()).show()\n4.9 like() & rlike() – Similar to SQL LIKE expression\n\n#like , rlike\ndf.select(df.fname,df.lname,df.id) \\\n  .filter(df.fname.like(\"%om\")) \n4.10 substr() – Returns a Column after getting sub string from the Column\n\ndf.select(df.fname.substr(1,2).alias(\"substr\")).show()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"a6ad4203-dd56-4606-926b-9355465f68ea"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["df11.filter(df11.fname.startswith(\"T\")).show()\ndf11.filter(df11.lname.contains(\"XXX\")).show()\ndf11.filter(df11.fname.endswith(\"e\")).show()\ndf11.filter(df11.gender.isNull()).show()\ndf11.filter(df11.gender.isNotNull()).show()\ndf11.select(df11.fname,df11.lname,df11.gender).filter(df11.lname.like(\"%sa\")).show()\ndf11.select(df11.fname,df11.id).filter(df11.fname.like(\"%om%\")).show()\ndf11.filter(df11.fname.rlike(\"^om\")).show()\ndf11.select(df11.fname.substr(1,3).alias(\"substr\")).show()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"54602d35-6173-4fe1-bd7a-849a53772b32"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"+----------+-----+---+------+\n|     fname|lname| id|gender|\n+----------+-----+---+------+\n|Tom Cruise|  XXX|400|      |\n| Tom Brand| null|400|     M|\n+----------+-----+---+------+\n\n+----------+-----+---+------+\n|     fname|lname| id|gender|\n+----------+-----+---+------+\n|Tom Cruise|  XXX|400|      |\n+----------+-----+---+------+\n\n+----------+-----+---+------+\n|     fname|lname| id|gender|\n+----------+-----+---+------+\n|Tom Cruise|  XXX|400|      |\n+----------+-----+---+------+\n\n+-----+-----+---+------+\n|fname|lname| id|gender|\n+-----+-----+---+------+\n|James| Bond|100|  null|\n+-----+-----+---+------+\n\n+----------+-----+---+------+\n|     fname|lname| id|gender|\n+----------+-----+---+------+\n|       Ann|Varsa|200|     F|\n|Tom Cruise|  XXX|400|      |\n| Tom Brand| null|400|     M|\n+----------+-----+---+------+\n\n+-----+-----+------+\n|fname|lname|gender|\n+-----+-----+------+\n|  Ann|Varsa|     F|\n+-----+-----+------+\n\n+----------+---+\n|     fname| id|\n+----------+---+\n|Tom Cruise|400|\n| Tom Brand|400|\n+----------+---+\n\n+-----+-----+---+------+\n|fname|lname| id|gender|\n+-----+-----+---+------+\n+-----+-----+---+------+\n\n+------+\n|substr|\n+------+\n|   Jam|\n|   Ann|\n|   Tom|\n|   Tom|\n+------+\n\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["+----------+-----+---+------+\n|     fname|lname| id|gender|\n+----------+-----+---+------+\n|Tom Cruise|  XXX|400|      |\n| Tom Brand| null|400|     M|\n+----------+-----+---+------+\n\n+----------+-----+---+------+\n|     fname|lname| id|gender|\n+----------+-----+---+------+\n|Tom Cruise|  XXX|400|      |\n+----------+-----+---+------+\n\n+----------+-----+---+------+\n|     fname|lname| id|gender|\n+----------+-----+---+------+\n|Tom Cruise|  XXX|400|      |\n+----------+-----+---+------+\n\n+-----+-----+---+------+\n|fname|lname| id|gender|\n+-----+-----+---+------+\n|James| Bond|100|  null|\n+-----+-----+---+------+\n\n+----------+-----+---+------+\n|     fname|lname| id|gender|\n+----------+-----+---+------+\n|       Ann|Varsa|200|     F|\n|Tom Cruise|  XXX|400|      |\n| Tom Brand| null|400|     M|\n+----------+-----+---+------+\n\n+-----+-----+------+\n|fname|lname|gender|\n+-----+-----+------+\n|  Ann|Varsa|     F|\n+-----+-----+------+\n\n+----------+---+\n|     fname| id|\n+----------+---+\n|Tom Cruise|400|\n| Tom Brand|400|\n+----------+---+\n\n+-----+-----+---+------+\n|fname|lname| id|gender|\n+-----+-----+---+------+\n+-----+-----+---+------+\n\n+------+\n|substr|\n+------+\n|   Jam|\n|   Ann|\n|   Tom|\n|   Tom|\n+------+\n\n"]}}],"execution_count":0},{"cell_type":"code","source":["4.11 when() & otherwise() – It is similar to SQL Case When, executes sequence of expressions until it matches the condition and returns a value when match.\n\n#when & otherwise\nfrom pyspark.sql.functions import when\ndf.select(df.fname,df.lname,when(df.gender==\"M\",\"Male\") \\\n              .when(df.gender==\"F\",\"Female\") \\\n              .when(df.gender==None ,\"\") \\\n              .otherwise(df.gender).alias(\"new_gender\") \\\n    ).show()\n4.12 isin() – Check if value presents in a List.\n\n#isin\nli=[\"100\",\"200\"]\ndf.select(df.fname,df.lname,df.id) \\\n  .filter(df.id.isin(li)) \\\n  .show()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"a14a08c6-043b-4298-bd27-60283b69b191"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["from pyspark.sql.functions import when\ndf11.select(df11.fname,df11.lname,when(df11.gender == \"M\",\"Male\")\\\n                                    .when(df11.gender == \"F\",\"Female\") \\\n                                    .when(df11.gender == \"\",\"empty\") \\\n                                    .otherwise(df11.gender).alias(\"new_gender\")\n           \n           ).show()\n\n#isin function same like in operator\nIn = [100,200]\ndf11.select(df11.id.isin(In)).show()\ndf11.select(df11.id,df11.fname).filter(df11.id.isin(In)).show()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"6d2908be-e883-4560-8d72-b46448b7403b"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)\n\u001B[0;32m<command-1929897425368937>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[0;32mfrom\u001B[0m \u001B[0mpyspark\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msql\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mfunctions\u001B[0m \u001B[0;32mimport\u001B[0m \u001B[0mwhen\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 2\u001B[0;31m df11.select(df11.fname,df11.lname,when(df11.gender == \"M\",\"Male\")\\\n\u001B[0m\u001B[1;32m      3\u001B[0m                                     \u001B[0;34m.\u001B[0m\u001B[0mwhen\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mdf11\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mgender\u001B[0m \u001B[0;34m==\u001B[0m \u001B[0;34m\"F\"\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\"Female\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;31m \u001B[0m\u001B[0;31m\\\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      4\u001B[0m                                     \u001B[0;34m.\u001B[0m\u001B[0mwhen\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mdf11\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mgender\u001B[0m \u001B[0;34m==\u001B[0m \u001B[0;34m\"\"\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\"empty\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;31m \u001B[0m\u001B[0;31m\\\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      5\u001B[0m                                     \u001B[0;34m.\u001B[0m\u001B[0motherwise\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mdf11\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mgender\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0malias\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"new_gender\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;31mNameError\u001B[0m: name 'df11' is not defined","errorSummary":"<span class='ansi-red-fg'>NameError</span>: name 'df11' is not defined","metadata":{},"errorTraceType":"ansi","type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/plain":["\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)\n\u001B[0;32m<command-1929897425368937>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[0;32mfrom\u001B[0m \u001B[0mpyspark\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msql\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mfunctions\u001B[0m \u001B[0;32mimport\u001B[0m \u001B[0mwhen\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 2\u001B[0;31m df11.select(df11.fname,df11.lname,when(df11.gender == \"M\",\"Male\")\\\n\u001B[0m\u001B[1;32m      3\u001B[0m                                     \u001B[0;34m.\u001B[0m\u001B[0mwhen\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mdf11\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mgender\u001B[0m \u001B[0;34m==\u001B[0m \u001B[0;34m\"F\"\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\"Female\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;31m \u001B[0m\u001B[0;31m\\\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      4\u001B[0m                                     \u001B[0;34m.\u001B[0m\u001B[0mwhen\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mdf11\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mgender\u001B[0m \u001B[0;34m==\u001B[0m \u001B[0;34m\"\"\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\"empty\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;31m \u001B[0m\u001B[0;31m\\\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      5\u001B[0m                                     \u001B[0;34m.\u001B[0m\u001B[0motherwise\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mdf11\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mgender\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0malias\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"new_gender\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;31mNameError\u001B[0m: name 'df11' is not defined"]}}],"execution_count":0},{"cell_type":"code","source":["4.13 getField() – To get the value by key from MapType column and by stuct child name from StructType column\nRest of the below functions operates on List, Map & Struct data structures hence to demonstrate these I will use another DataFrame with list, map and struct columns. For more explanation how to use Arrays refer to PySpark ArrayType Column on DataFrame Examples & for map refer to PySpark MapType Examples\n\n\n#Create DataFrame with struct, array & map\nfrom pyspark.sql.types import StructType,StructField,StringType,ArrayType,MapType\ndata=[((\"James\",\"Bond\"),[\"Java\",\"C#\"],{'hair':'black','eye':'brown'}),\n      ((\"Ann\",\"Varsa\"),[\".NET\",\"Python\"],{'hair':'brown','eye':'black'}),\n      ((\"Tom Cruise\",\"\"),[\"Python\",\"Scala\"],{'hair':'red','eye':'grey'}),\n      ((\"Tom Brand\",None),[\"Perl\",\"Ruby\"],{'hair':'black','eye':'blue'})]\n\nschema = StructType([\n        StructField('name', StructType([\n            StructField('fname', StringType(), True),\n            StructField('lname', StringType(), True)])),\n        StructField('languages', ArrayType(StringType()),True),\n        StructField('properties', MapType(StringType(),StringType()),True)\n     ])\ndf=spark.createDataFrame(data,schema)\ndf.printSchema()\n\n#Display's to console\nroot\n |-- name: struct (nullable = true)\n |    |-- fname: string (nullable = true)\n |    |-- lname: string (nullable = true)\n |-- languages: array (nullable = true)\n |    |-- element: string (containsNull = true)\n |-- properties: map (nullable = true)\n |    |-- key: string\n |    |-- value: string (valueContainsNull = true)\ngetField Example\n\n\n#getField from MapType\ndf.select(df.properties.getField(\"hair\")).show()\n\n#getField from Struct\ndf.select(df.name.getField(\"fname\")).show()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"b9b4176a-ebe9-469f-84cf-05828aadd865"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["import pyspark\nfrom pyspark.sql.types import StructType,StructField,StringType,IntegerType,DataType,ArrayType,MapType\nfrom pyspark.sql.functions import col\ndata12 = [((\"james\",\"bond\"),[\"java\",\"C#\"],{'hair':'black','eye':'red'}),\n          ((\"krishna\",\"reddy\"),[\".net\",\"python\"],{'hair':'white','eye':'black'}),\n          ((\"mani\",\"gali\"),[\"sql\",\"C\"],{'hair':'grey','eye':'black'})         \n         ]\nData_Struct = StructType([ StructField('name',StructType([StructField('fname',StringType(),True),\n                                                          StructField('lname',StringType(),True)])\n                                      ),\n                          StructField('language',ArrayType(StringType()),True),\n                          StructField('properties',MapType(StringType(),StringType()),True)\n                         ])\n\ndf22= spark.createDataFrame(data = data12,schema = Data_Struct)\ndf22.printSchema()\n#df22.show(truncate = True)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"2ef707d2-1a2d-48f4-ab1d-26b0aee73737"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"root\n |-- name: struct (nullable = true)\n |    |-- fname: string (nullable = true)\n |    |-- lname: string (nullable = true)\n |-- language: array (nullable = true)\n |    |-- element: string (containsNull = true)\n |-- properties: map (nullable = true)\n |    |-- key: string\n |    |-- value: string (valueContainsNull = true)\n\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["root\n |-- name: struct (nullable = true)\n |    |-- fname: string (nullable = true)\n |    |-- lname: string (nullable = true)\n |-- language: array (nullable = true)\n |    |-- element: string (containsNull = true)\n |-- properties: map (nullable = true)\n |    |-- key: string\n |    |-- value: string (valueContainsNull = true)\n\n"]}}],"execution_count":0},{"cell_type":"code","source":["df22.select(df22.properties.getField(\"hair\")).show()\ndf22.select(df22.name.getField(\"fname\")).show()\ndf22.select(df22.language).show()\ndf22.select(df22.language.getItem(0)).show()\ndf22.select(df22.language.getItem(1)).show()# datatype is Array type\ndf22.select(df22.properties.getItem(\"eye\")).show()# datatype is Maptype\n"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"a3401051-0445-47b3-b008-0baa14361485"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"+----------------+\n|properties[hair]|\n+----------------+\n|           black|\n|           white|\n|            grey|\n+----------------+\n\n+----------+\n|name.fname|\n+----------+\n|     james|\n|   krishna|\n|      mani|\n+----------+\n\n+--------------+\n|      language|\n+--------------+\n|    [java, C#]|\n|[.net, python]|\n|      [sql, C]|\n+--------------+\n\n+-----------+\n|language[0]|\n+-----------+\n|       java|\n|       .net|\n|        sql|\n+-----------+\n\n+-----------+\n|language[1]|\n+-----------+\n|         C#|\n|     python|\n|          C|\n+-----------+\n\n+---------------+\n|properties[eye]|\n+---------------+\n|            red|\n|          black|\n|          black|\n+---------------+\n\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["+----------------+\n|properties[hair]|\n+----------------+\n|           black|\n|           white|\n|            grey|\n+----------------+\n\n+----------+\n|name.fname|\n+----------+\n|     james|\n|   krishna|\n|      mani|\n+----------+\n\n+--------------+\n|      language|\n+--------------+\n|    [java, C#]|\n|[.net, python]|\n|      [sql, C]|\n+--------------+\n\n+-----------+\n|language[0]|\n+-----------+\n|       java|\n|       .net|\n|        sql|\n+-----------+\n\n+-----------+\n|language[1]|\n+-----------+\n|         C#|\n|     python|\n|          C|\n+-----------+\n\n+---------------+\n|properties[eye]|\n+---------------+\n|            red|\n|          black|\n|          black|\n+---------------+\n\n"]}}],"execution_count":0},{"cell_type":"code","source":["1. Select Single & Multiple Columns From PySpark\n2. Select All Columns From List\n3. Select Columns by Index\n4. Select Nested Struct Columns from PySpark"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"5187462a-d31a-4864-8194-f5f094a63837"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["#Select Nested Struct Columns from PySpark\n#df22.select(\"name\").show()\n#df22.select(\"name.fname\",\"name.lname\").show()\n#df22.select(\"name.*\").show()\n#df22.select(df22.name).show()\n#df22.select(df22.name.fname).show()\ndf22.select(df22[\"name\"]).show()\nfrom pyspark.sql.functions import col\n#df22.select(col(\"name.fname\"),col(\"name.lname\")).show()\n#df22.select(col(\"language\")).show()\n#df22.select(col(\"properties\")).show()\n#df22.select(df22.language).show()\n#df22.select(df22.properties).show()\n#df22.select(df22.language,df22.properties.hair).show()\n#df22.select(df22[\"language\"],df22[\"properties.hair\"]).show()\ndf22.select(\"language\",\"properties.hair\").show()\ndf22.select(\"*\").show()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"b7c7bf78-6c4e-4a84-852c-01b602b7d7ed"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"+----------------+\n|            name|\n+----------------+\n|   {james, bond}|\n|{krishna, reddy}|\n|    {mani, gali}|\n+----------------+\n\n+--------------+-----+\n|      language| hair|\n+--------------+-----+\n|    [java, C#]|black|\n|[.net, python]|white|\n|      [sql, C]| grey|\n+--------------+-----+\n\n+----------------+--------------+--------------------+\n|            name|      language|          properties|\n+----------------+--------------+--------------------+\n|   {james, bond}|    [java, C#]|{eye -> red, hair...|\n|{krishna, reddy}|[.net, python]|{eye -> black, ha...|\n|    {mani, gali}|      [sql, C]|{eye -> black, ha...|\n+----------------+--------------+--------------------+\n\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["+----------------+\n|            name|\n+----------------+\n|   {james, bond}|\n|{krishna, reddy}|\n|    {mani, gali}|\n+----------------+\n\n+--------------+-----+\n|      language| hair|\n+--------------+-----+\n|    [java, C#]|black|\n|[.net, python]|white|\n|      [sql, C]| grey|\n+--------------+-----+\n\n+----------------+--------------+--------------------+\n|            name|      language|          properties|\n+----------------+--------------+--------------------+\n|   {james, bond}|    [java, C#]|{eye -> red, hair...|\n|{krishna, reddy}|[.net, python]|{eye -> black, ha...|\n|    {mani, gali}|      [sql, C]|{eye -> black, ha...|\n+----------------+--------------+--------------------+\n\n"]}}],"execution_count":0},{"cell_type":"code","source":["#2. Select All Columns From List\ndf22.select(\"*\").show(2)\ndf22.select([col for col in df22.Data_struct]).show()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"9b74c666-7067-4aea-a5f0-ab617363b116"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"+----------------+--------------+--------------------+\n|            name|      language|          properties|\n+----------------+--------------+--------------------+\n|   {james, bond}|    [java, C#]|{eye -> red, hair...|\n|{krishna, reddy}|[.net, python]|{eye -> black, ha...|\n+----------------+--------------+--------------------+\nonly showing top 2 rows\n\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["+----------------+--------------+--------------------+\n|            name|      language|          properties|\n+----------------+--------------+--------------------+\n|   {james, bond}|    [java, C#]|{eye -> red, hair...|\n|{krishna, reddy}|[.net, python]|{eye -> black, ha...|\n+----------------+--------------+--------------------+\nonly showing top 2 rows\n\n"]}},{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n\u001B[0;31mAttributeError\u001B[0m                            Traceback (most recent call last)\n\u001B[0;32m<command-991280237107375>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[0;31m#2. Select All Columns From List\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      2\u001B[0m \u001B[0mdf22\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mselect\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"*\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mshow\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;36m2\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 3\u001B[0;31m \u001B[0mdf22\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mselect\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mcol\u001B[0m \u001B[0;32mfor\u001B[0m \u001B[0mcol\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mdf22\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mData_struct\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mshow\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\n\u001B[0;32m/databricks/spark/python/pyspark/sql/dataframe.py\u001B[0m in \u001B[0;36m__getattr__\u001B[0;34m(self, name)\u001B[0m\n\u001B[1;32m   1798\u001B[0m         \"\"\"\n\u001B[1;32m   1799\u001B[0m         \u001B[0;32mif\u001B[0m \u001B[0mname\u001B[0m \u001B[0;32mnot\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mcolumns\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1800\u001B[0;31m             raise AttributeError(\n\u001B[0m\u001B[1;32m   1801\u001B[0m                 \"'%s' object has no attribute '%s'\" % (self.__class__.__name__, name))\n\u001B[1;32m   1802\u001B[0m         \u001B[0mjc\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_jdf\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mapply\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mname\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;31mAttributeError\u001B[0m: 'DataFrame' object has no attribute 'Data_struct'","errorSummary":"<span class='ansi-red-fg'>AttributeError</span>: 'DataFrame' object has no attribute 'Data_struct'","metadata":{},"errorTraceType":"ansi","type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/plain":["\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n\u001B[0;31mAttributeError\u001B[0m                            Traceback (most recent call last)\n\u001B[0;32m<command-991280237107375>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[0;31m#2. Select All Columns From List\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      2\u001B[0m \u001B[0mdf22\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mselect\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"*\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mshow\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;36m2\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 3\u001B[0;31m \u001B[0mdf22\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mselect\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mcol\u001B[0m \u001B[0;32mfor\u001B[0m \u001B[0mcol\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mdf22\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mData_struct\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mshow\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\n\u001B[0;32m/databricks/spark/python/pyspark/sql/dataframe.py\u001B[0m in \u001B[0;36m__getattr__\u001B[0;34m(self, name)\u001B[0m\n\u001B[1;32m   1798\u001B[0m         \"\"\"\n\u001B[1;32m   1799\u001B[0m         \u001B[0;32mif\u001B[0m \u001B[0mname\u001B[0m \u001B[0;32mnot\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mcolumns\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1800\u001B[0;31m             raise AttributeError(\n\u001B[0m\u001B[1;32m   1801\u001B[0m                 \"'%s' object has no attribute '%s'\" % (self.__class__.__name__, name))\n\u001B[1;32m   1802\u001B[0m         \u001B[0mjc\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_jdf\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mapply\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mname\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;31mAttributeError\u001B[0m: 'DataFrame' object has no attribute 'Data_struct'"]}}],"execution_count":0},{"cell_type":"code","source":["#PySpark Collect() – Retrieve data from DataFrame\nimport pyspark\nfrom pyspark.sql import SparkSession\n\nspark = SparkSession.builder.appName('SparkByExamples.com').getOrCreate()\n\ndept = [(\"Finance\",10), \\\n    (\"Marketing\",20), \\\n    (\"Sales\",30), \\\n    (\"IT\",40) \\\n  ]\ndeptColumns = [\"dept_name\",\"dept_id\"]\ndeptDF = spark.createDataFrame(data=dept, schema = deptColumns)\ndeptDF.show()\n"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"b0f145bd-0402-4cc8-bbdd-6e75b86bda75"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"+---------+-------+\n|dept_name|dept_id|\n+---------+-------+\n|  Finance|     10|\n|Marketing|     20|\n|    Sales|     30|\n|       IT|     40|\n+---------+-------+\n\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["+---------+-------+\n|dept_name|dept_id|\n+---------+-------+\n|  Finance|     10|\n|Marketing|     20|\n|    Sales|     30|\n|       IT|     40|\n+---------+-------+\n\n"]}}],"execution_count":0},{"cell_type":"code","source":["data_collect = deptDF.collect()\nprint(data_collect)\n\nfor row in data_collect:\n    print(row[\"dept_name\"] + ',' + str(row[\"dept_id\"]))\ndata_collect1 = deptDF.collect()\nprint(data_collect1)\n\nfor row in data_collect1 : print(row[\"dept_name\"] +\",\"+ str(row[\"dept_id\"]))"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"9f4568e0-5640-4874-a298-76c56399fe61"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"[Row(dept_name='Finance', dept_id=10), Row(dept_name='Marketing', dept_id=20), Row(dept_name='Sales', dept_id=30), Row(dept_name='IT', dept_id=40)]\nFinance,10\nMarketing,20\nSales,30\nIT,40\n[Row(dept_name='Finance', dept_id=10), Row(dept_name='Marketing', dept_id=20), Row(dept_name='Sales', dept_id=30), Row(dept_name='IT', dept_id=40)]\nFinance,10\nMarketing,20\nSales,30\nIT,40\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["[Row(dept_name='Finance', dept_id=10), Row(dept_name='Marketing', dept_id=20), Row(dept_name='Sales', dept_id=30), Row(dept_name='IT', dept_id=40)]\nFinance,10\nMarketing,20\nSales,30\nIT,40\n[Row(dept_name='Finance', dept_id=10), Row(dept_name='Marketing', dept_id=20), Row(dept_name='Sales', dept_id=30), Row(dept_name='IT', dept_id=40)]\nFinance,10\nMarketing,20\nSales,30\nIT,40\n"]}}],"execution_count":0},{"cell_type":"code","source":["#deptDF.collect()[1][1]\ndeptDF.collect()[1]"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"ab0c8b61-f420-430f-8feb-c8a7f05b15e6"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"Out[11]: Row(dept_name='Marketing', dept_id=20)","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["Out[11]: Row(dept_name='Marketing', dept_id=20)"]}}],"execution_count":0},{"cell_type":"code","source":["data_collect2 = deptDF.select(\"dept_name\").collect()\nprint(data_collect2)\ndata_collect = deptDF.collect()\nprint(data_collect)\ndata_collect2=deptDF.select(\"dept_id\").collect()\nprint(data_collect2)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"b5a834fa-4eb6-4a33-aa8b-c9fead62f905"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"[Row(dept_name='Finance'), Row(dept_name='Marketing'), Row(dept_name='Sales'), Row(dept_name='IT')]\n[Row(dept_name='Finance', dept_id=10), Row(dept_name='Marketing', dept_id=20), Row(dept_name='Sales', dept_id=30), Row(dept_name='IT', dept_id=40)]\n[Row(dept_id=10), Row(dept_id=20), Row(dept_id=30), Row(dept_id=40)]\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["[Row(dept_name='Finance'), Row(dept_name='Marketing'), Row(dept_name='Sales'), Row(dept_name='IT')]\n[Row(dept_name='Finance', dept_id=10), Row(dept_name='Marketing', dept_id=20), Row(dept_name='Sales', dept_id=30), Row(dept_name='IT', dept_id=40)]\n[Row(dept_id=10), Row(dept_id=20), Row(dept_id=30), Row(dept_id=40)]\n"]}}],"execution_count":0},{"cell_type":"code","source":["#PySpark withColumn() Usage with Examples\n#PySpark withColumn() is a transformation function of DataFrame which is used to change the value, convert the datatype of an existing column, create a new column, and many more. In this post, I will walk you through commonly used PySpark DataFrame column operations using withColumn() examples.\ndata100 = [('james','','smith','1991-04-01','M',3000),\n           ('Michael','Rose','','2000-05-19','M',4000),\n           ('Robert','','Williams','1978-09-05','M',4000),\n          ('Maria','Anne','Jones','1967-12-01','F',4000),\n          ('Jen','Mary','Brown','1980-02-17','F',-1)\n           ]"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"b999cbd3-d36b-4089-8ffe-1db26d872590"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["import pyspark\nfrom pyspark.sql import SparkSession\nspark = SparkSession.builder.appName('spark by example.com').getOrCreate()\n\ndata100 = [('james','','smith','1991-04-01','M',3000),\n           ('Michael','Rose','','2000-05-19','M',4000),\n           ('Robert','','Williams','1978-09-05','M',4000),\n          ('Maria','Anne','Jones','1967-12-01','F',4000),\n          ('Jen','Mary','Brown','1980-02-17','F',-1)\n           ]\ncolumns1 = [\"first_name\",\"middel_name\",\"last_name\",\"DOB\",\"gender\",\"salary\"]\ndataframe2 = spark.createDataFrame(data = data100,schema = columns1)\ndataframe2.printSchema()\ndataframe2.show()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"bc60c9dc-2c97-4fcd-a1ea-48f27576ee16"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"root\n |-- first_name: string (nullable = true)\n |-- middel_name: string (nullable = true)\n |-- last_name: string (nullable = true)\n |-- DOB: string (nullable = true)\n |-- gender: string (nullable = true)\n |-- salary: long (nullable = true)\n\n+----------+-----------+---------+----------+------+------+\n|first_name|middel_name|last_name|       DOB|gender|salary|\n+----------+-----------+---------+----------+------+------+\n|     james|           |    smith|1991-04-01|     M|  3000|\n|   Michael|       Rose|         |2000-05-19|     M|  4000|\n|    Robert|           | Williams|1978-09-05|     M|  4000|\n|     Maria|       Anne|    Jones|1967-12-01|     F|  4000|\n|       Jen|       Mary|    Brown|1980-02-17|     F|    -1|\n+----------+-----------+---------+----------+------+------+\n\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["root\n |-- first_name: string (nullable = true)\n |-- middel_name: string (nullable = true)\n |-- last_name: string (nullable = true)\n |-- DOB: string (nullable = true)\n |-- gender: string (nullable = true)\n |-- salary: long (nullable = true)\n\n+----------+-----------+---------+----------+------+------+\n|first_name|middel_name|last_name|       DOB|gender|salary|\n+----------+-----------+---------+----------+------+------+\n|     james|           |    smith|1991-04-01|     M|  3000|\n|   Michael|       Rose|         |2000-05-19|     M|  4000|\n|    Robert|           | Williams|1978-09-05|     M|  4000|\n|     Maria|       Anne|    Jones|1967-12-01|     F|  4000|\n|       Jen|       Mary|    Brown|1980-02-17|     F|    -1|\n+----------+-----------+---------+----------+------+------+\n\n"]}}],"execution_count":0},{"cell_type":"code","source":["1. Change DataType using PySpark withColumn()\nBy using PySpark withColumn() on a DataFrame, we can cast or change the data type of a column. In order to change data type, you would also need to use cast() function along with withColumn(). The below statement changes the datatype from String to Integer for the salary column.\ndf.withColumn(\"salary\",col(\"salary\").cast(\"Integer\")).show()\n2. Update The Value of an Existing Column\nPySpark withColumn() function of DataFrame can also be used to change the value of an existing column. In order to change the value, pass an existing column name as a first argument and a value to be assigned as a second argument to the withColumn() function. Note that the second argument should be Column type . Also, see Different Ways to Update PySpark DataFrame Column.\n\ndf.withColumn(\"salary\",col(\"salary\")*100).show()\n3. Create a Column from an Existing\nTo add/create a new column, specify the first argument with a name you want your new column to be and use the second argument to assign a value by applying an operation on an existing column. Also, see Different Ways to Add New Column to PySpark DataFrame.\n\ndf.withColumn(\"CopiedColumn\",col(\"salary\")* -1).show()\n4. Add a New Column using withColumn()\nIn order to create a new column, pass the column name you wanted to the first argument of withColumn() transformation function. Make sure this new column not already present on DataFrame, if it presents it updates the value of that column.\n\nOn below snippet, PySpark lit() function is used to add a constant value to a DataFrame column. We can also chain in order to add multiple columns.\n\n\ndf.withColumn(\"Country\", lit(\"USA\")).show()\ndf.withColumn(\"Country\", lit(\"USA\")) \\\n  .withColumn(\"anotherColumn\",lit(\"anotherValue\")) \\\n  .show()\n5. Rename Column Name\nThough you cannot rename a column using withColumn, still I wanted to cover this as renaming is one of the common operations we perform on DataFrame. To rename an existing column use withColumnRenamed() function on DataFrame.\n\ndf.withColumnRenamed(\"gender\",\"sex\") \\\n  .show(truncate=False) \n6. Drop Column From PySpark DataFrame\nUse “drop” function to drop a specific column from the DataFrame.\ndf.drop(\"salary\") \\\n  .show() "],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"be27a1cd-a189-4ae0-812e-0f1ce9330a06"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["from pyspark.sql.functions import col\n#dataframe2.withColumn(\"salary\",col(\"salary\").cast(\"Integer\")).printSchema()\n#dataframe2.withColumn(\"salary\",col(\"salary\").cast(\"Integer\")).show()\ndataframe2.withColumn(\"NewSalary\",col(\"salary\")-1000).show()\n"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"d50660b7-599f-4ecf-8075-71b7b99ac0dc"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"+----------+-----------+---------+----------+------+------+---------+\n|first_name|middel_name|last_name|       DOB|gender|salary|NewSalary|\n+----------+-----------+---------+----------+------+------+---------+\n|     james|           |    smith|1991-04-01|     M|  3000|     2000|\n|   Michael|       Rose|         |2000-05-19|     M|  4000|     3000|\n|    Robert|           | Williams|1978-09-05|     M|  4000|     3000|\n|     Maria|       Anne|    Jones|1967-12-01|     F|  4000|     3000|\n|       Jen|       Mary|    Brown|1980-02-17|     F|    -1|    -1001|\n+----------+-----------+---------+----------+------+------+---------+\n\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["+----------+-----------+---------+----------+------+------+---------+\n|first_name|middel_name|last_name|       DOB|gender|salary|NewSalary|\n+----------+-----------+---------+----------+------+------+---------+\n|     james|           |    smith|1991-04-01|     M|  3000|     2000|\n|   Michael|       Rose|         |2000-05-19|     M|  4000|     3000|\n|    Robert|           | Williams|1978-09-05|     M|  4000|     3000|\n|     Maria|       Anne|    Jones|1967-12-01|     F|  4000|     3000|\n|       Jen|       Mary|    Brown|1980-02-17|     F|    -1|    -1001|\n+----------+-----------+---------+----------+------+------+---------+\n\n"]}}],"execution_count":0},{"cell_type":"code","source":["from pyspark.sql.functions import lit\n#dataframe2.withColumn(\"salary\",col(\"salary\")*100).show()\n#dataframe2.withColumn(\"DOB\",col(\"DOB\").cast(\"Date\")).show()\n#dataframe2.withColumn(\"salary\",col(\"salary\")+1000).show()\ndataframe2.withColumn(\"country\",lit(\"USA\")).show()\ndataframe2.withColumn(\"state\",lit(\"\"))\\\n            .withColumn(\"anothercolumn\",lit(\"\")).show()\ndataframe2.withColumnRenamed(\"gender\",\"sex\").show(truncate = False)\ndataframe2.withColumnRenamed(\"First_name\",\"Fname\")\\\n            .withColumnRenamed(\"salary\",\"Paid\").show()\ndataframe2.drop(\"middel_name\").show()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"7abcb25d-d5ae-41bd-b20e-ebf28353593c"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"+----------+-----------+---------+----------+------+------+-------+\n|first_name|middel_name|last_name|       DOB|gender|salary|country|\n+----------+-----------+---------+----------+------+------+-------+\n|     james|           |    smith|1991-04-01|     M|  3000|    USA|\n|   Michael|       Rose|         |2000-05-19|     M|  4000|    USA|\n|    Robert|           | Williams|1978-09-05|     M|  4000|    USA|\n|     Maria|       Anne|    Jones|1967-12-01|     F|  4000|    USA|\n|       Jen|       Mary|    Brown|1980-02-17|     F|    -1|    USA|\n+----------+-----------+---------+----------+------+------+-------+\n\n+----------+-----------+---------+----------+------+------+-----+-------------+\n|first_name|middel_name|last_name|       DOB|gender|salary|state|anothercolumn|\n+----------+-----------+---------+----------+------+------+-----+-------------+\n|     james|           |    smith|1991-04-01|     M|  3000|     |             |\n|   Michael|       Rose|         |2000-05-19|     M|  4000|     |             |\n|    Robert|           | Williams|1978-09-05|     M|  4000|     |             |\n|     Maria|       Anne|    Jones|1967-12-01|     F|  4000|     |             |\n|       Jen|       Mary|    Brown|1980-02-17|     F|    -1|     |             |\n+----------+-----------+---------+----------+------+------+-----+-------------+\n\n+----------+-----------+---------+----------+---+------+\n|first_name|middel_name|last_name|DOB       |sex|salary|\n+----------+-----------+---------+----------+---+------+\n|james     |           |smith    |1991-04-01|M  |3000  |\n|Michael   |Rose       |         |2000-05-19|M  |4000  |\n|Robert    |           |Williams |1978-09-05|M  |4000  |\n|Maria     |Anne       |Jones    |1967-12-01|F  |4000  |\n|Jen       |Mary       |Brown    |1980-02-17|F  |-1    |\n+----------+-----------+---------+----------+---+------+\n\n+-------+-----------+---------+----------+------+----+\n|  Fname|middel_name|last_name|       DOB|gender|Paid|\n+-------+-----------+---------+----------+------+----+\n|  james|           |    smith|1991-04-01|     M|3000|\n|Michael|       Rose|         |2000-05-19|     M|4000|\n| Robert|           | Williams|1978-09-05|     M|4000|\n|  Maria|       Anne|    Jones|1967-12-01|     F|4000|\n|    Jen|       Mary|    Brown|1980-02-17|     F|  -1|\n+-------+-----------+---------+----------+------+----+\n\n+----------+---------+----------+------+------+\n|first_name|last_name|       DOB|gender|salary|\n+----------+---------+----------+------+------+\n|     james|    smith|1991-04-01|     M|  3000|\n|   Michael|         |2000-05-19|     M|  4000|\n|    Robert| Williams|1978-09-05|     M|  4000|\n|     Maria|    Jones|1967-12-01|     F|  4000|\n|       Jen|    Brown|1980-02-17|     F|    -1|\n+----------+---------+----------+------+------+\n\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["+----------+-----------+---------+----------+------+------+-------+\n|first_name|middel_name|last_name|       DOB|gender|salary|country|\n+----------+-----------+---------+----------+------+------+-------+\n|     james|           |    smith|1991-04-01|     M|  3000|    USA|\n|   Michael|       Rose|         |2000-05-19|     M|  4000|    USA|\n|    Robert|           | Williams|1978-09-05|     M|  4000|    USA|\n|     Maria|       Anne|    Jones|1967-12-01|     F|  4000|    USA|\n|       Jen|       Mary|    Brown|1980-02-17|     F|    -1|    USA|\n+----------+-----------+---------+----------+------+------+-------+\n\n+----------+-----------+---------+----------+------+------+-----+-------------+\n|first_name|middel_name|last_name|       DOB|gender|salary|state|anothercolumn|\n+----------+-----------+---------+----------+------+------+-----+-------------+\n|     james|           |    smith|1991-04-01|     M|  3000|     |             |\n|   Michael|       Rose|         |2000-05-19|     M|  4000|     |             |\n|    Robert|           | Williams|1978-09-05|     M|  4000|     |             |\n|     Maria|       Anne|    Jones|1967-12-01|     F|  4000|     |             |\n|       Jen|       Mary|    Brown|1980-02-17|     F|    -1|     |             |\n+----------+-----------+---------+----------+------+------+-----+-------------+\n\n+----------+-----------+---------+----------+---+------+\n|first_name|middel_name|last_name|DOB       |sex|salary|\n+----------+-----------+---------+----------+---+------+\n|james     |           |smith    |1991-04-01|M  |3000  |\n|Michael   |Rose       |         |2000-05-19|M  |4000  |\n|Robert    |           |Williams |1978-09-05|M  |4000  |\n|Maria     |Anne       |Jones    |1967-12-01|F  |4000  |\n|Jen       |Mary       |Brown    |1980-02-17|F  |-1    |\n+----------+-----------+---------+----------+---+------+\n\n+-------+-----------+---------+----------+------+----+\n|  Fname|middel_name|last_name|       DOB|gender|Paid|\n+-------+-----------+---------+----------+------+----+\n|  james|           |    smith|1991-04-01|     M|3000|\n|Michael|       Rose|         |2000-05-19|     M|4000|\n| Robert|           | Williams|1978-09-05|     M|4000|\n|  Maria|       Anne|    Jones|1967-12-01|     F|4000|\n|    Jen|       Mary|    Brown|1980-02-17|     F|  -1|\n+-------+-----------+---------+----------+------+----+\n\n+----------+---------+----------+------+------+\n|first_name|last_name|       DOB|gender|salary|\n+----------+---------+----------+------+------+\n|     james|    smith|1991-04-01|     M|  3000|\n|   Michael|         |2000-05-19|     M|  4000|\n|    Robert| Williams|1978-09-05|     M|  4000|\n|     Maria|    Jones|1967-12-01|     F|  4000|\n|       Jen|    Brown|1980-02-17|     F|    -1|\n+----------+---------+----------+------+------+\n\n"]}}],"execution_count":0},{"cell_type":"code","source":["PySpark Where Filter Function | Multiple Conditions\nPySpark filter() function is used to filter the rows from RDD/DataFrame based on the given condition or SQL expression, you can also use where() clause instead of the filter() if you are coming from an SQL background, both these functions operate exactly the same.\n\nIn this PySpark article, you will learn how to apply a filter on DataFrame columns of string, arrays, struct types by using single and multiple conditions and also applying filter using isin() with PySpark (Python Spark) examples.\n1. PySpark DataFrame filter() Syntax\nBelow is syntax of the filter function. condition would be an expression you wanted to filter.\n\n\nfilter(condition)\nBefore we start with examples, first let’s create a DataFrame. Here, I am using a DataFrame with StructType and ArrayType columns as I will also be covering examples with struct and array types as-well.\n\n\nfrom pyspark.sql.types import StructType,StructField \nfrom pyspark.sql.types import StringType, IntegerType, ArrayType\ndata = [\n    ((\"James\",\"\",\"Smith\"),[\"Java\",\"Scala\",\"C++\"],\"OH\",\"M\"),\n    ((\"Anna\",\"Rose\",\"\"),[\"Spark\",\"Java\",\"C++\"],\"NY\",\"F\"),\n    ((\"Julia\",\"\",\"Williams\"),[\"CSharp\",\"VB\"],\"OH\",\"F\"),\n    ((\"Maria\",\"Anne\",\"Jones\"),[\"CSharp\",\"VB\"],\"NY\",\"M\"),\n    ((\"Jen\",\"Mary\",\"Brown\"),[\"CSharp\",\"VB\"],\"NY\",\"M\"),\n    ((\"Mike\",\"Mary\",\"Williams\"),[\"Python\",\"VB\"],\"OH\",\"M\")\n ]"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"0253d45c-ae38-4cda-baf4-2230f26263d7"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["from pyspark.sql.types import StructType,StructField,StringType,IntegerType,ArrayType,MapType\ndata200 = [((\"james\",\"\",\"smith\"),[\"java\",\"scala\",\"c++\"],\"OH\",\"M\"),\n        ((\"Anna\",\"Rose\",\"\"),[\"Spark\",\"Java\",\"C++\"],\"NY\",\"F\"),\n    ((\"Julia\",\"\",\"Williams\"),[\"CSharp\",\"VB\"],\"OH\",\"F\"),\n    ((\"Maria\",\"Anne\",\"Jones\"),[\"CSharp\",\"VB\"],\"NY\",\"M\"),\n    ((\"Jen\",\"Mary\",\"Brown\"),[\"CSharp\",\"VB\"],\"NY\",\"M\"),\n    ((\"Mike\",\"Mary\",\"Williams\"),[\"Python\",\"VB\"],\"OH\",\"M\")    \n]\ndata200_schema = StructType([StructField('name',StructType([StructField('fname',StringType(),True),\n                                                StructField('mname',StringType(),True),\n                                                StructField('lname',StringType(),True)])\n                                        ),\n                             StructField('language',ArrayType(StringType()),True),\n                             StructField('state',StringType(),True),\n                             StructField('gender',StringType(),True) ])\ndataframe200 = spark.createDataFrame(data = data200,schema=data200_schema)\ndataframe200.printSchema()\ndataframe200.show()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"ed2e7054-b97f-4264-bfdf-6045bfff7e6d"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"root\n |-- name: struct (nullable = true)\n |    |-- fname: string (nullable = true)\n |    |-- mname: string (nullable = true)\n |    |-- lname: string (nullable = true)\n |-- language: array (nullable = true)\n |    |-- element: string (containsNull = true)\n |-- state: string (nullable = true)\n |-- gender: string (nullable = true)\n\n+--------------------+------------------+-----+------+\n|                name|          language|state|gender|\n+--------------------+------------------+-----+------+\n|    {james, , smith}|[java, scala, c++]|   OH|     M|\n|      {Anna, Rose, }|[Spark, Java, C++]|   NY|     F|\n| {Julia, , Williams}|      [CSharp, VB]|   OH|     F|\n|{Maria, Anne, Jones}|      [CSharp, VB]|   NY|     M|\n|  {Jen, Mary, Brown}|      [CSharp, VB]|   NY|     M|\n|{Mike, Mary, Will...|      [Python, VB]|   OH|     M|\n+--------------------+------------------+-----+------+\n\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["root\n |-- name: struct (nullable = true)\n |    |-- fname: string (nullable = true)\n |    |-- mname: string (nullable = true)\n |    |-- lname: string (nullable = true)\n |-- language: array (nullable = true)\n |    |-- element: string (containsNull = true)\n |-- state: string (nullable = true)\n |-- gender: string (nullable = true)\n\n+--------------------+------------------+-----+------+\n|                name|          language|state|gender|\n+--------------------+------------------+-----+------+\n|    {james, , smith}|[java, scala, c++]|   OH|     M|\n|      {Anna, Rose, }|[Spark, Java, C++]|   NY|     F|\n| {Julia, , Williams}|      [CSharp, VB]|   OH|     F|\n|{Maria, Anne, Jones}|      [CSharp, VB]|   NY|     M|\n|  {Jen, Mary, Brown}|      [CSharp, VB]|   NY|     M|\n|{Mike, Mary, Will...|      [Python, VB]|   OH|     M|\n+--------------------+------------------+-----+------+\n\n"]}}],"execution_count":0},{"cell_type":"code","source":["2. DataFrame filter() with Column Condition\nUse Column with the condition to filter the rows from DataFrame, using this you can express complex condition by referring column names using dfObject.colname"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"89323bda-19a6-4c2f-9e14-79b790f3d838"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["import  pyspark\nfrom pyspark.sql.functions import col\n#dataframe200.filter(dataframe200.state == \"OH\").show()\n#dataframe200.filter(dataframe200.state != \"OH\").show()\n#dataframe200.filter(col(\"state\")==\"OH\").show()\ndataframe200.filter(col(\"gender\")==\"M\").show()\ndataframe200.filter(\"gender == 'F'\").show()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"598fb007-5aac-4de4-b652-8f4fcf63221a"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"+--------------------+------------------+-----+------+\n|                name|          language|state|gender|\n+--------------------+------------------+-----+------+\n|    {james, , smith}|[java, scala, c++]|   OH|     M|\n|{Maria, Anne, Jones}|      [CSharp, VB]|   NY|     M|\n|  {Jen, Mary, Brown}|      [CSharp, VB]|   NY|     M|\n|{Mike, Mary, Will...|      [Python, VB]|   OH|     M|\n+--------------------+------------------+-----+------+\n\n+-------------------+------------------+-----+------+\n|               name|          language|state|gender|\n+-------------------+------------------+-----+------+\n|     {Anna, Rose, }|[Spark, Java, C++]|   NY|     F|\n|{Julia, , Williams}|      [CSharp, VB]|   OH|     F|\n+-------------------+------------------+-----+------+\n\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["+--------------------+------------------+-----+------+\n|                name|          language|state|gender|\n+--------------------+------------------+-----+------+\n|    {james, , smith}|[java, scala, c++]|   OH|     M|\n|{Maria, Anne, Jones}|      [CSharp, VB]|   NY|     M|\n|  {Jen, Mary, Brown}|      [CSharp, VB]|   NY|     M|\n|{Mike, Mary, Will...|      [Python, VB]|   OH|     M|\n+--------------------+------------------+-----+------+\n\n+-------------------+------------------+-----+------+\n|               name|          language|state|gender|\n+-------------------+------------------+-----+------+\n|     {Anna, Rose, }|[Spark, Java, C++]|   NY|     F|\n|{Julia, , Williams}|      [CSharp, VB]|   OH|     F|\n+-------------------+------------------+-----+------+\n\n"]}}],"execution_count":0},{"cell_type":"code","source":["3. DataFrame filter() with SQL Expression\nIf you are coming from SQL background, you can use that knowledge in PySpark to filter DataFrame rows with SQL expressions.\n#Using SQL Expression\ndf.filter(\"gender == 'M'\").show()\n#For not equal\ndf.filter(\"gender != 'M'\").show()\ndf.filter(\"gender <> 'M'\").show()\n4. PySpark Filter with Multiple Conditions\nIn PySpark, to filter() rows on DataFrame based on multiple conditions, you case use either Column with a condition or SQL expression. Below is just a simple example using AND (&) condition, you can extend this with OR(|), and NOT(!) conditional expressions as needed.\n\n\n//Filter multiple condition\ndf.filter( (df.state  == \"OH\") & (df.gender  == \"M\") ) \\\n    .show(truncate=False) \n5. Filter Based on List Values\nIf you have a list of elements and you wanted to filter that is not in the list or in the list, use isin() function of Column class and it doesn’t have isnotin() function but you do the same using not operator (~)\n\n\n#Filter IS IN List values\nli=[\"OH\",\"CA\",\"DE\"]\ndf.filter(df.state.isin(li)).show()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"d8f1cf46-9c4d-4575-9a73-18a63077f70e"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["dataframe200.filter((dataframe200.state == 'OH') & (dataframe200.gender == 'M')).show()\ndataframe200.filter((col(\"state\")=='OH') & (col(\"gender\") =='M')).show()\ndataframe200.filter((\"state =='OH'\")&(\"gender=='M'\")).show()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"20c96cee-db38-40fb-a5d6-8771120c3da5"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"+--------------------+------------------+-----+------+\n|                name|          language|state|gender|\n+--------------------+------------------+-----+------+\n|    {james, , smith}|[java, scala, c++]|   OH|     M|\n|{Mike, Mary, Will...|      [Python, VB]|   OH|     M|\n+--------------------+------------------+-----+------+\n\n+--------------------+------------------+-----+------+\n|                name|          language|state|gender|\n+--------------------+------------------+-----+------+\n|    {james, , smith}|[java, scala, c++]|   OH|     M|\n|{Mike, Mary, Will...|      [Python, VB]|   OH|     M|\n+--------------------+------------------+-----+------+\n\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["+--------------------+------------------+-----+------+\n|                name|          language|state|gender|\n+--------------------+------------------+-----+------+\n|    {james, , smith}|[java, scala, c++]|   OH|     M|\n|{Mike, Mary, Will...|      [Python, VB]|   OH|     M|\n+--------------------+------------------+-----+------+\n\n+--------------------+------------------+-----+------+\n|                name|          language|state|gender|\n+--------------------+------------------+-----+------+\n|    {james, , smith}|[java, scala, c++]|   OH|     M|\n|{Mike, Mary, Will...|      [Python, VB]|   OH|     M|\n+--------------------+------------------+-----+------+\n\n"]}},{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n\u001B[0;31mTypeError\u001B[0m                                 Traceback (most recent call last)\n\u001B[0;32m<command-2768786762561516>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[0mdataframe200\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mfilter\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mdataframe200\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mstate\u001B[0m \u001B[0;34m==\u001B[0m \u001B[0;34m'OH'\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;34m&\u001B[0m \u001B[0;34m(\u001B[0m\u001B[0mdataframe200\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mgender\u001B[0m \u001B[0;34m==\u001B[0m \u001B[0;34m'M'\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mshow\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      2\u001B[0m \u001B[0mdataframe200\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mfilter\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mcol\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"state\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m==\u001B[0m\u001B[0;34m'OH'\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;34m&\u001B[0m \u001B[0;34m(\u001B[0m\u001B[0mcol\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"gender\"\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;34m==\u001B[0m\u001B[0;34m'M'\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mshow\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 3\u001B[0;31m \u001B[0mdataframe200\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mfilter\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"state =='OH'\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m&\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"gender=='M'\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mshow\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\n\u001B[0;31mTypeError\u001B[0m: unsupported operand type(s) for &: 'str' and 'str'","errorSummary":"<span class='ansi-red-fg'>TypeError</span>: unsupported operand type(s) for &: 'str' and 'str'","metadata":{},"errorTraceType":"ansi","type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/plain":["\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n\u001B[0;31mTypeError\u001B[0m                                 Traceback (most recent call last)\n\u001B[0;32m<command-2768786762561516>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[0mdataframe200\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mfilter\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mdataframe200\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mstate\u001B[0m \u001B[0;34m==\u001B[0m \u001B[0;34m'OH'\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;34m&\u001B[0m \u001B[0;34m(\u001B[0m\u001B[0mdataframe200\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mgender\u001B[0m \u001B[0;34m==\u001B[0m \u001B[0;34m'M'\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mshow\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      2\u001B[0m \u001B[0mdataframe200\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mfilter\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mcol\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"state\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m==\u001B[0m\u001B[0;34m'OH'\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;34m&\u001B[0m \u001B[0;34m(\u001B[0m\u001B[0mcol\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"gender\"\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;34m==\u001B[0m\u001B[0;34m'M'\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mshow\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 3\u001B[0;31m \u001B[0mdataframe200\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mfilter\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"state =='OH'\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m&\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"gender=='M'\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mshow\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\n\u001B[0;31mTypeError\u001B[0m: unsupported operand type(s) for &: 'str' and 'str'"]}}],"execution_count":0},{"cell_type":"code","source":["6. Filter Based on Starts With, Ends With, Contains\nYou can also filter DataFrame rows by using startswith(), endswith() and contains() methods of Column class. For more examples on Column class, refer to PySpark Column Functions.\n# Using startswith\ndf.filter(df.state.startswith(\"N\")).show()\n#using endswith\ndf.filter(df.state.endswith(\"H\")).show()\n#contains\ndf.filter(df.state.contains(\"H\")).show()\n7. PySpark Filter like and rlike\nIf you have SQL background you must be familiar with like and rlike (regex like), PySpark also provides similar methods in Column class to filter similar values using wildcard characters. You can use rlike() to filter by checking values case insensitive.\n\n\ndata2 = [(2,\"Michael Rose\"),(3,\"Robert Williams\"),\n     (4,\"Rames Rose\"),(5,\"Rames rose\")\n  ]\ndf2 = spark.createDataFrame(data = data2, schema = [\"id\",\"name\"])\n\n# like - SQL LIKE pattern\ndf2.filter(df2.name.like(\"%rose%\")).show()\n+---+----------+\n| id|      name|\n+---+----------+\n|  5|Rames rose|\n+---+----------+\n\n# rlike - SQL RLIKE pattern (LIKE with Regex)\n#This check case insensitive\ndf2.filter(df2.name.rlike(\"(?i)^*rose$\")).show()\n+---+------------+\n| id|        name|\n+---+------------+\n|  2|Michael Rose|\n|  4|  Rames Rose|\n|  5|  Rames rose|"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"98442a84-4d3f-49f9-a614-135da2e7f8ff"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["dataframe200.filter(dataframe200.state.startswith(\"O\")).show()\ndataframe200.filter(dataframe200.name.fname.startswith(\"M\")).show()\ndataframe200.filter(dataframe200.name.fname.endswith(\"a\")).show()\ndataframe200.filter(dataframe200.name.mname.contains(\"nn\")).show()\ndataframe200.filter(dataframe200.name.lname.contains(\"ll\")).show()\ndataframe200.filter(dataframe200.name.lname.like(\"W%\")).show()\n#dataframe200.filter(dataframe200.name.lname.rlike(\"(?i)^*s$\").show()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"92b9aab8-c30d-48f6-899e-ceeeb2e91062"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"\u001B[0;36m  File \u001B[0;32m\"<command-32121595053186>\"\u001B[0;36m, line \u001B[0;32m7\u001B[0m\n\u001B[0;31m    dataframe200.filter(dataframe200.name.lname.rlike(\"(?i)^*s$\").show()\u001B[0m\n\u001B[0m                                                                        ^\u001B[0m\n\u001B[0;31mSyntaxError\u001B[0m\u001B[0;31m:\u001B[0m unexpected EOF while parsing\n","errorSummary":"<span class='ansi-red-fg'>SyntaxError</span>: unexpected EOF while parsing (<command-32121595053186>, line 7)","metadata":{},"errorTraceType":"ansi","type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/plain":["\u001B[0;36m  File \u001B[0;32m\"<command-32121595053186>\"\u001B[0;36m, line \u001B[0;32m7\u001B[0m\n\u001B[0;31m    dataframe200.filter(dataframe200.name.lname.rlike(\"(?i)^*s$\").show()\u001B[0m\n\u001B[0m                                                                        ^\u001B[0m\n\u001B[0;31mSyntaxError\u001B[0m\u001B[0;31m:\u001B[0m unexpected EOF while parsing\n"]}}],"execution_count":0},{"cell_type":"code","source":["8. Filter on an Array column\nWhen you want to filter rows from DataFrame based on value present in an array collection column, you can use the first syntax. The below example uses array_contains() from Pyspark SQL functions which checks if a value contains in an array if present it returns true otherwise false.\n\n\nfrom pyspark.sql.functions import array_contains\ndf.filter(array_contains(df.languages,\"Java\")) \\\n    .show(truncate=False) \n9. Filtering on Nested Struct columns\nIf your DataFrame consists of nested struct columns, you can use any of the above syntaxes to filter the rows based on the nested column.\n\n\n  //Struct condition\ndf.filter(df.name.lastname == \"Williams\") \\\n    .show(truncate=False) "],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"44e79896-e4c1-4242-83ea-c2835e68b557"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["from pyspark.sql.functions import array_contains\ndataframe200.filter(array_contains(dataframe200.language,\"Java\")).show()\ndataframe200.filter(array_contains(dataframe200.language,\"Python\")).show()\ndataframe200.filter(dataframe200.name.lname == \"Williams\").show()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"ca203ba7-f327-4f89-9856-cc715de40526"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"+--------------+------------------+-----+------+\n|          name|          language|state|gender|\n+--------------+------------------+-----+------+\n|{Anna, Rose, }|[Spark, Java, C++]|   NY|     F|\n+--------------+------------------+-----+------+\n\n+--------------------+------------+-----+------+\n|                name|    language|state|gender|\n+--------------------+------------+-----+------+\n|{Mike, Mary, Will...|[Python, VB]|   OH|     M|\n+--------------------+------------+-----+------+\n\n+--------------------+------------+-----+------+\n|                name|    language|state|gender|\n+--------------------+------------+-----+------+\n| {Julia, , Williams}|[CSharp, VB]|   OH|     F|\n|{Mike, Mary, Will...|[Python, VB]|   OH|     M|\n+--------------------+------------+-----+------+\n\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["+--------------+------------------+-----+------+\n|          name|          language|state|gender|\n+--------------+------------------+-----+------+\n|{Anna, Rose, }|[Spark, Java, C++]|   NY|     F|\n+--------------+------------------+-----+------+\n\n+--------------------+------------+-----+------+\n|                name|    language|state|gender|\n+--------------------+------------+-----+------+\n|{Mike, Mary, Will...|[Python, VB]|   OH|     M|\n+--------------------+------------+-----+------+\n\n+--------------------+------------+-----+------+\n|                name|    language|state|gender|\n+--------------------+------------+-----+------+\n| {Julia, , Williams}|[CSharp, VB]|   OH|     F|\n|{Mike, Mary, Will...|[Python, VB]|   OH|     M|\n+--------------------+------------+-----+------+\n\n"]}}],"execution_count":0},{"cell_type":"code","source":["PySpark Distinct to Drop Duplicate Rows\nPySpark distinct() function is used to drop/remove the duplicate rows (all columns) from DataFrame and dropDuplicates() is used to drop rows based on selected (one or multiple) columns. In this article, you will learn how to use distinct() and dropDuplicates() functions with PySpark example.\n\nBefore we start, first let’s create a DataFrame with some duplicate rows and values on a few columns. We use this DataFrame to demonstrate how to get distinct multiple columns.\n# Prepare Data\ndata = [(\"James\", \"Sales\", 3000), \\\n    (\"Michael\", \"Sales\", 4600), \\\n    (\"Robert\", \"Sales\", 4100), \\\n    (\"Maria\", \"Finance\", 3000), \\\n    (\"James\", \"Sales\", 3000), \\\n    (\"Scott\", \"Finance\", 3300), \\\n    (\"Jen\", \"Finance\", 3900), \\\n    (\"Jeff\", \"Marketing\", 3000), \\\n    (\"Kumar\", \"Marketing\", 2000), \\\n    (\"Saif\", \"Sales\", 4100) \\\n  ]\n\n# Create DataFrame\ncolumns= [\"employee_name\", \"department\", \"salary\"]\ndf = spark.createDataFrame(data = data, schema = columns)\ndf.printSchema()\ndf.show(truncate=False)\n\n1. Get Distinct Rows (By Comparing All Columns)\nOn the above DataFrame, we have a total of 10 rows with 2 rows having all values duplicated, performing distinct on this DataFrame should get us 9 after removing 1 duplicate row.\n\n\ndistinctDF = df.distinct()\nprint(\"Distinct count: \"+str(distinctDF.count()))\ndistinctDF.show(truncate=False)\ndistinct() function on DataFrame returns a new DataFrame after removing the duplicate records. This example yields the below output.\n\nAlternatively, you can also run dropDuplicates() function which returns a new DataFrame after removing duplicate rows.\n\n2. PySpark Distinct of Selected Multiple Columns\nPySpark doesn’t have a distinct method that takes columns that should run distinct on (drop duplicate rows on selected multiple columns) however, it provides another signature of dropDuplicates() function which takes multiple columns to eliminate duplicates.\n\nNote that calling dropDuplicates() on DataFrame returns a new DataFrame with duplicate rows removed.\n\n\ndropDisDF = df.dropDuplicates([\"department\",\"salary\"])\nprint(\"Distinct count of department & salary : \"+str(dropDisDF.count()))\ndropDisDF.show(truncate=False)\nYields below output. If you notice the output, It dropped 2 records that are duplicates."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"a30b105d-e10b-4331-896f-8c1c0cb9da6d"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["data_1 = [(\"James\", \"Sales\", 3000), \\\n    (\"Michael\", \"Sales\", 4600), \\\n    (\"Robert\", \"Sales\", 4100), \\\n    (\"Maria\", \"Finance\", 3000), \\\n    (\"James\", \"Sales\", 3000), \\\n    (\"Scott\", \"Finance\", 3300), \\\n    (\"Jen\", \"Finance\", 3900), \\\n    (\"Jeff\", \"Marketing\", 3000), \\\n    (\"Kumar\", \"Marketing\", 2000), \\\n    (\"Saif\", \"Sales\", 4100) \\\n  ]\ncolumns_1 = [\"employee_name\", \"department\", \"salary\"]\ndf100=spark.createDataFrame(data=data_1,schema= columns_1)\ndf100.printSchema()\ndf100.show(truncate = False)\nprint(\"total records:\"+str(df100.count()))"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"fae235be-cd5c-455c-953b-c79134cf090e"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"root\n |-- employee_name: string (nullable = true)\n |-- department: string (nullable = true)\n |-- salary: long (nullable = true)\n\n+-------------+----------+------+\n|employee_name|department|salary|\n+-------------+----------+------+\n|James        |Sales     |3000  |\n|Michael      |Sales     |4600  |\n|Robert       |Sales     |4100  |\n|Maria        |Finance   |3000  |\n|James        |Sales     |3000  |\n|Scott        |Finance   |3300  |\n|Jen          |Finance   |3900  |\n|Jeff         |Marketing |3000  |\n|Kumar        |Marketing |2000  |\n|Saif         |Sales     |4100  |\n+-------------+----------+------+\n\ntotal records:10\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["root\n |-- employee_name: string (nullable = true)\n |-- department: string (nullable = true)\n |-- salary: long (nullable = true)\n\n+-------------+----------+------+\n|employee_name|department|salary|\n+-------------+----------+------+\n|James        |Sales     |3000  |\n|Michael      |Sales     |4600  |\n|Robert       |Sales     |4100  |\n|Maria        |Finance   |3000  |\n|James        |Sales     |3000  |\n|Scott        |Finance   |3300  |\n|Jen          |Finance   |3900  |\n|Jeff         |Marketing |3000  |\n|Kumar        |Marketing |2000  |\n|Saif         |Sales     |4100  |\n+-------------+----------+------+\n\ntotal records:10\n"]}}],"execution_count":0},{"cell_type":"code","source":["from pyspark.sql.functions import expr\ndistinctDF = df100.distinct()\ndistinctDF.show()\nprint(\"distinct count:\"+str(distinctDF.count()))\n\n"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"c2388de6-15e9-4bb6-a9d0-52e4b82e6680"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)\n\u001B[0;32m<command-32121595053191>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[0;32mfrom\u001B[0m \u001B[0mpyspark\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msql\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mfunctions\u001B[0m \u001B[0;32mimport\u001B[0m \u001B[0mexpr\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 2\u001B[0;31m \u001B[0mdistinctDF\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mdf100\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mdistinct\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      3\u001B[0m \u001B[0mdistinctDF\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mshow\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      4\u001B[0m \u001B[0mprint\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"distinct count:\"\u001B[0m\u001B[0;34m+\u001B[0m\u001B[0mstr\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mdistinctDF\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mcount\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      5\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;31mNameError\u001B[0m: name 'df100' is not defined","errorSummary":"<span class='ansi-red-fg'>NameError</span>: name 'df100' is not defined","metadata":{},"errorTraceType":"ansi","type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/plain":["\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)\n\u001B[0;32m<command-32121595053191>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[0;32mfrom\u001B[0m \u001B[0mpyspark\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msql\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mfunctions\u001B[0m \u001B[0;32mimport\u001B[0m \u001B[0mexpr\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 2\u001B[0;31m \u001B[0mdistinctDF\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mdf100\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mdistinct\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      3\u001B[0m \u001B[0mdistinctDF\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mshow\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      4\u001B[0m \u001B[0mprint\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"distinct count:\"\u001B[0m\u001B[0;34m+\u001B[0m\u001B[0mstr\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mdistinctDF\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mcount\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      5\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;31mNameError\u001B[0m: name 'df100' is not defined"]}}],"execution_count":0},{"cell_type":"code","source":["dropdistDF = df100.dropDuplicates([\"department\",\"salary\"])\ndropdistDF.show()\nprint(\"distinct count of dept & salary:\"+str(dropdistDF.count()))"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"aff97d4a-0ca5-4de2-939c-bb86be908288"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"+-------------+----------+------+\n|employee_name|department|salary|\n+-------------+----------+------+\n|        Maria|   Finance|  3000|\n|        Scott|   Finance|  3300|\n|          Jen|   Finance|  3900|\n|        Kumar| Marketing|  2000|\n|         Jeff| Marketing|  3000|\n|        James|     Sales|  3000|\n|       Robert|     Sales|  4100|\n|      Michael|     Sales|  4600|\n+-------------+----------+------+\n\ndistinct count of dept & salary:8\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["+-------------+----------+------+\n|employee_name|department|salary|\n+-------------+----------+------+\n|        Maria|   Finance|  3000|\n|        Scott|   Finance|  3300|\n|          Jen|   Finance|  3900|\n|        Kumar| Marketing|  2000|\n|         Jeff| Marketing|  3000|\n|        James|     Sales|  3000|\n|       Robert|     Sales|  4100|\n|      Michael|     Sales|  4600|\n+-------------+----------+------+\n\ndistinct count of dept & salary:8\n"]}}],"execution_count":0},{"cell_type":"code","source":["PySpark orderBy() and sort() explained\nYou can use either sort() or orderBy() function of PySpark DataFrame to sort DataFrame by ascending or descending order based on single or multiple columns, you can also do sorting using PySpark SQL sorting functions, In this article, I will explain all these different ways using PySpark examples.\n\nUsing sort() function\nUsing orderBy() function\nAscending order\nDescending order\nSQL Sort functions\nsimpleData = [(\"James\",\"Sales\",\"NY\",90000,34,10000), \\\n    (\"Michael\",\"Sales\",\"NY\",86000,56,20000), \\\n    (\"Robert\",\"Sales\",\"CA\",81000,30,23000), \\\n    (\"Maria\",\"Finance\",\"CA\",90000,24,23000), \\\n    (\"Raman\",\"Finance\",\"CA\",99000,40,24000), \\\n    (\"Scott\",\"Finance\",\"NY\",83000,36,19000), \\\n    (\"Jen\",\"Finance\",\"NY\",79000,53,15000), \\\n    (\"Jeff\",\"Marketing\",\"CA\",80000,25,18000), \\\n    (\"Kumar\",\"Marketing\",\"NY\",91000,50,21000) \\\n  ]\ncolumns= [\"employee_name\",\"department\",\"state\",\"salary\",\"age\",\"bonus\"]\ndf = spark.createDataFrame(data = simpleData, schema = columns)\ndf.printSchema()\ndf.show(truncate=False)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"8c61c6bf-7f22-4a9e-85d1-a1970ad04462"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["import pyspark\n#from pyspark.sql.functions \n#from pyspark.sql.items\nsimpleData = [(\"James\",\"Sales\",\"NY\",90000,34,10000), \\\n    (\"Michael\",\"Sales\",\"NY\",86000,56,20000), \\\n    (\"Robert\",\"Sales\",\"CA\",81000,30,23000), \\\n    (\"Maria\",\"Finance\",\"CA\",90000,24,23000), \\\n    (\"Raman\",\"Finance\",\"CA\",99000,40,24000), \\\n    (\"Scott\",\"Finance\",\"NY\",83000,36,19000), \\\n    (\"Jen\",\"Finance\",\"NY\",79000,53,15000), \\\n    (\"Jeff\",\"Marketing\",\"CA\",80000,25,18000), \\\n    (\"Kumar\",\"Marketing\",\"NY\",91000,50,21000) \\\n  ]\ncolumns_names = [\"employee_name\",\"department\",\"state\",\"salary\",\"age\",\"bonus\"]\ndf200 = spark.createDataFrame(data = simpleData ,schema = columns_names)\ndf200.printSchema()\ndf200.show()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"b1ea4265-8147-4480-8d46-3ae251c2fa6d"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"root\n |-- employee_name: string (nullable = true)\n |-- department: string (nullable = true)\n |-- state: string (nullable = true)\n |-- salary: long (nullable = true)\n |-- age: long (nullable = true)\n |-- bonus: long (nullable = true)\n\n+-------------+----------+-----+------+---+-----+\n|employee_name|department|state|salary|age|bonus|\n+-------------+----------+-----+------+---+-----+\n|        James|     Sales|   NY| 90000| 34|10000|\n|      Michael|     Sales|   NY| 86000| 56|20000|\n|       Robert|     Sales|   CA| 81000| 30|23000|\n|        Maria|   Finance|   CA| 90000| 24|23000|\n|        Raman|   Finance|   CA| 99000| 40|24000|\n|        Scott|   Finance|   NY| 83000| 36|19000|\n|          Jen|   Finance|   NY| 79000| 53|15000|\n|         Jeff| Marketing|   CA| 80000| 25|18000|\n|        Kumar| Marketing|   NY| 91000| 50|21000|\n+-------------+----------+-----+------+---+-----+\n\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["root\n |-- employee_name: string (nullable = true)\n |-- department: string (nullable = true)\n |-- state: string (nullable = true)\n |-- salary: long (nullable = true)\n |-- age: long (nullable = true)\n |-- bonus: long (nullable = true)\n\n+-------------+----------+-----+------+---+-----+\n|employee_name|department|state|salary|age|bonus|\n+-------------+----------+-----+------+---+-----+\n|        James|     Sales|   NY| 90000| 34|10000|\n|      Michael|     Sales|   NY| 86000| 56|20000|\n|       Robert|     Sales|   CA| 81000| 30|23000|\n|        Maria|   Finance|   CA| 90000| 24|23000|\n|        Raman|   Finance|   CA| 99000| 40|24000|\n|        Scott|   Finance|   NY| 83000| 36|19000|\n|          Jen|   Finance|   NY| 79000| 53|15000|\n|         Jeff| Marketing|   CA| 80000| 25|18000|\n|        Kumar| Marketing|   NY| 91000| 50|21000|\n+-------------+----------+-----+------+---+-----+\n\n"]}}],"execution_count":0},{"cell_type":"code","source":["DataFrame sorting using the sort() function\nPySpark DataFrame class provides sort() function to sort on one or more columns. By default, it sorts by ascending order.\n\nSyntax\nsort(self, *cols, **kwargs):\n    df.sort(\"department\",\"state\").show(truncate=False)\ndf.sort(col(\"department\"),col(\"state\")).show(truncate=False)\nDataFrame sorting using orderBy() function\nPySpark DataFrame also provides orderBy() function to sort on one or more columns. By default, it orders by ascending.\n\ndf.orderBy(\"department\",\"state\").show(truncate=False)\ndf.orderBy(col(\"department\"),col(\"state\")).show(truncate=False)\nThis returns the same output as the previous section.\n\nSort by Ascending (ASC)\nIf you wanted to specify the ascending order/sort explicitly on DataFrame, you can use the asc method of the Column function. for example\n\n\ndf.sort(df.department.asc(),df.state.asc()).show(truncate=False)\ndf.sort(col(\"department\").asc(),col(\"state\").asc()).show(truncate=False)\ndf.orderBy(col(\"department\").asc(),col(\"state\").asc()).show(truncate=False)\n\nSort by Descending (DESC)\nIf you wanted to specify the sorting by descending order on DataFrame, you can use the desc method of the Column function. for example. From our example, let’s use desc on the state column.\n\n\ndf.sort(df.department.asc(),df.state.desc()).show(truncate=False)\ndf.sort(col(\"department\").asc(),col(\"state\").desc()).show(truncate=False)\ndf.orderBy(col(\"department\").asc(),col(\"state\").desc()).show(truncate=False)\n\nUsing Raw SQL\nBelow is an example of how to sort DataFrame using raw SQL syntax.\ndf.createOrReplaceTempView(\"EMP\")\nspark.sql(\"select employee_name,department,state,salary,age,bonus from EMP ORDER BY department asc\").show(truncate=False)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"96c9fbbc-0d42-4d7c-9b50-124c20d841f0"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["from pyspark.sql.functions import col\ndf200.sort(\"department\",\"state\").show()\ndf200.sort(col(\"department\"),col(\"state\")).show()\ndf200.sort(col(\"department\").asc(),col(\"state\").asc()).show()\ndf200.sort(df200.department.desc()).show()\ndf200.createOrReplaceTempView(\"EMP\")\nspark.sql(\"select * from emp ORDER BY department asc\").show()\nspark.sql(\"select employee_name,bonus from emp where department = 'Finance' ORDER BY employee_name\").show()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"32c1fd43-9a44-4235-9a61-eadc739876f2"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"+-------------+----------+-----+------+---+-----+\n|employee_name|department|state|salary|age|bonus|\n+-------------+----------+-----+------+---+-----+\n|        Maria|   Finance|   CA| 90000| 24|23000|\n|        Raman|   Finance|   CA| 99000| 40|24000|\n|        Scott|   Finance|   NY| 83000| 36|19000|\n|          Jen|   Finance|   NY| 79000| 53|15000|\n|         Jeff| Marketing|   CA| 80000| 25|18000|\n|        Kumar| Marketing|   NY| 91000| 50|21000|\n|       Robert|     Sales|   CA| 81000| 30|23000|\n|        James|     Sales|   NY| 90000| 34|10000|\n|      Michael|     Sales|   NY| 86000| 56|20000|\n+-------------+----------+-----+------+---+-----+\n\n+-------------+----------+-----+------+---+-----+\n|employee_name|department|state|salary|age|bonus|\n+-------------+----------+-----+------+---+-----+\n|        Raman|   Finance|   CA| 99000| 40|24000|\n|        Maria|   Finance|   CA| 90000| 24|23000|\n|        Scott|   Finance|   NY| 83000| 36|19000|\n|          Jen|   Finance|   NY| 79000| 53|15000|\n|         Jeff| Marketing|   CA| 80000| 25|18000|\n|        Kumar| Marketing|   NY| 91000| 50|21000|\n|       Robert|     Sales|   CA| 81000| 30|23000|\n|        James|     Sales|   NY| 90000| 34|10000|\n|      Michael|     Sales|   NY| 86000| 56|20000|\n+-------------+----------+-----+------+---+-----+\n\n+-------------+----------+-----+------+---+-----+\n|employee_name|department|state|salary|age|bonus|\n+-------------+----------+-----+------+---+-----+\n|        Maria|   Finance|   CA| 90000| 24|23000|\n|        Raman|   Finance|   CA| 99000| 40|24000|\n|          Jen|   Finance|   NY| 79000| 53|15000|\n|        Scott|   Finance|   NY| 83000| 36|19000|\n|         Jeff| Marketing|   CA| 80000| 25|18000|\n|        Kumar| Marketing|   NY| 91000| 50|21000|\n|       Robert|     Sales|   CA| 81000| 30|23000|\n|      Michael|     Sales|   NY| 86000| 56|20000|\n|        James|     Sales|   NY| 90000| 34|10000|\n+-------------+----------+-----+------+---+-----+\n\n+-------------+----------+-----+------+---+-----+\n|employee_name|department|state|salary|age|bonus|\n+-------------+----------+-----+------+---+-----+\n|        James|     Sales|   NY| 90000| 34|10000|\n|      Michael|     Sales|   NY| 86000| 56|20000|\n|       Robert|     Sales|   CA| 81000| 30|23000|\n|         Jeff| Marketing|   CA| 80000| 25|18000|\n|        Kumar| Marketing|   NY| 91000| 50|21000|\n|        Raman|   Finance|   CA| 99000| 40|24000|\n|          Jen|   Finance|   NY| 79000| 53|15000|\n|        Scott|   Finance|   NY| 83000| 36|19000|\n|        Maria|   Finance|   CA| 90000| 24|23000|\n+-------------+----------+-----+------+---+-----+\n\n+-------------+----------+-----+------+---+-----+\n|employee_name|department|state|salary|age|bonus|\n+-------------+----------+-----+------+---+-----+\n|          Jen|   Finance|   NY| 79000| 53|15000|\n|        Maria|   Finance|   CA| 90000| 24|23000|\n|        Scott|   Finance|   NY| 83000| 36|19000|\n|        Raman|   Finance|   CA| 99000| 40|24000|\n|         Jeff| Marketing|   CA| 80000| 25|18000|\n|        Kumar| Marketing|   NY| 91000| 50|21000|\n|       Robert|     Sales|   CA| 81000| 30|23000|\n|      Michael|     Sales|   NY| 86000| 56|20000|\n|        James|     Sales|   NY| 90000| 34|10000|\n+-------------+----------+-----+------+---+-----+\n\n+-------------+-----+\n|employee_name|bonus|\n+-------------+-----+\n|          Jen|15000|\n|        Maria|23000|\n|        Raman|24000|\n|        Scott|19000|\n+-------------+-----+\n\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["+-------------+----------+-----+------+---+-----+\n|employee_name|department|state|salary|age|bonus|\n+-------------+----------+-----+------+---+-----+\n|        Maria|   Finance|   CA| 90000| 24|23000|\n|        Raman|   Finance|   CA| 99000| 40|24000|\n|        Scott|   Finance|   NY| 83000| 36|19000|\n|          Jen|   Finance|   NY| 79000| 53|15000|\n|         Jeff| Marketing|   CA| 80000| 25|18000|\n|        Kumar| Marketing|   NY| 91000| 50|21000|\n|       Robert|     Sales|   CA| 81000| 30|23000|\n|        James|     Sales|   NY| 90000| 34|10000|\n|      Michael|     Sales|   NY| 86000| 56|20000|\n+-------------+----------+-----+------+---+-----+\n\n+-------------+----------+-----+------+---+-----+\n|employee_name|department|state|salary|age|bonus|\n+-------------+----------+-----+------+---+-----+\n|        Raman|   Finance|   CA| 99000| 40|24000|\n|        Maria|   Finance|   CA| 90000| 24|23000|\n|        Scott|   Finance|   NY| 83000| 36|19000|\n|          Jen|   Finance|   NY| 79000| 53|15000|\n|         Jeff| Marketing|   CA| 80000| 25|18000|\n|        Kumar| Marketing|   NY| 91000| 50|21000|\n|       Robert|     Sales|   CA| 81000| 30|23000|\n|        James|     Sales|   NY| 90000| 34|10000|\n|      Michael|     Sales|   NY| 86000| 56|20000|\n+-------------+----------+-----+------+---+-----+\n\n+-------------+----------+-----+------+---+-----+\n|employee_name|department|state|salary|age|bonus|\n+-------------+----------+-----+------+---+-----+\n|        Maria|   Finance|   CA| 90000| 24|23000|\n|        Raman|   Finance|   CA| 99000| 40|24000|\n|          Jen|   Finance|   NY| 79000| 53|15000|\n|        Scott|   Finance|   NY| 83000| 36|19000|\n|         Jeff| Marketing|   CA| 80000| 25|18000|\n|        Kumar| Marketing|   NY| 91000| 50|21000|\n|       Robert|     Sales|   CA| 81000| 30|23000|\n|      Michael|     Sales|   NY| 86000| 56|20000|\n|        James|     Sales|   NY| 90000| 34|10000|\n+-------------+----------+-----+------+---+-----+\n\n+-------------+----------+-----+------+---+-----+\n|employee_name|department|state|salary|age|bonus|\n+-------------+----------+-----+------+---+-----+\n|        James|     Sales|   NY| 90000| 34|10000|\n|      Michael|     Sales|   NY| 86000| 56|20000|\n|       Robert|     Sales|   CA| 81000| 30|23000|\n|         Jeff| Marketing|   CA| 80000| 25|18000|\n|        Kumar| Marketing|   NY| 91000| 50|21000|\n|        Raman|   Finance|   CA| 99000| 40|24000|\n|          Jen|   Finance|   NY| 79000| 53|15000|\n|        Scott|   Finance|   NY| 83000| 36|19000|\n|        Maria|   Finance|   CA| 90000| 24|23000|\n+-------------+----------+-----+------+---+-----+\n\n+-------------+----------+-----+------+---+-----+\n|employee_name|department|state|salary|age|bonus|\n+-------------+----------+-----+------+---+-----+\n|          Jen|   Finance|   NY| 79000| 53|15000|\n|        Maria|   Finance|   CA| 90000| 24|23000|\n|        Scott|   Finance|   NY| 83000| 36|19000|\n|        Raman|   Finance|   CA| 99000| 40|24000|\n|         Jeff| Marketing|   CA| 80000| 25|18000|\n|        Kumar| Marketing|   NY| 91000| 50|21000|\n|       Robert|     Sales|   CA| 81000| 30|23000|\n|      Michael|     Sales|   NY| 86000| 56|20000|\n|        James|     Sales|   NY| 90000| 34|10000|\n+-------------+----------+-----+------+---+-----+\n\n+-------------+-----+\n|employee_name|bonus|\n+-------------+-----+\n|          Jen|15000|\n|        Maria|23000|\n|        Raman|24000|\n|        Scott|19000|\n+-------------+-----+\n\n"]}}],"execution_count":0},{"cell_type":"code","source":[""],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"abe077e8-1b8f-431a-8b96-fb053ab74e0e"}},"outputs":[],"execution_count":0}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"Test_Notebook (2)","dashboards":[],"notebookMetadata":{"pythonIndentUnit":4},"language":"python","widgets":{},"notebookOrigID":2385036558321592}},"nbformat":4,"nbformat_minor":0}
