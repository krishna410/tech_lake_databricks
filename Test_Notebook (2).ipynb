{"cells":[{"cell_type":"code","source":["columns = [\"language\",\"user_count\"]\ndata = [(\"java\",\"20000\"),(\"python\",\"10000\"),(\"scala\",\"3000\")]"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"799dc7b1-77ed-479d-81a0-415f2011bb35"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["first, letâ€™s create a Spark RDD from a collection List by calling parallelize() function from SparkContext . We would need this rdd object for all our examples below.\nspark = SparkSession.builder.appName('SparkByExamples.com').getOrCreate()\nrdd = spark.sparkContext.parallelize(data)\n"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"125935b7-4878-4636-abc7-74c0dd87633c"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["import pyspark\nfrom pyspark.sql import SparkSession\nspark = SparkSession.builder.appName('SparkByExamples.com').getOrCreate()\nrdd = spark.sparkContext.parallelize(data)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"989a6f68-cc4c-4a41-b58d-aee3bdb22a24"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["dfFromRDD1= rdd.toDF()\ndfFromRDD1.printSchema()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"e77cee2b-68d1-4090-be40-1789e9469889"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"root\n |-- _1: string (nullable = true)\n |-- _2: string (nullable = true)\n\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["root\n |-- _1: string (nullable = true)\n |-- _2: string (nullable = true)\n\n"]}}],"execution_count":0},{"cell_type":"code","source":["columns = [\"language\",\"user_count\"]\ndfFromrRDD = rdd.toDF(columns)\ndfFromrRDD.printSchema()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"6862abdd-4a53-450c-ae41-387920a39c39"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"root\n |-- language: string (nullable = true)\n |-- user_count: string (nullable = true)\n\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["root\n |-- language: string (nullable = true)\n |-- user_count: string (nullable = true)\n\n"]}}],"execution_count":0},{"cell_type":"code","source":["\ndfFromRDD2 = spark.createDataFrame(rdd).toDF(*columns)\n"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"34707d93-dad1-4384-97f7-ef7b74910d65"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["\ndfFromData2 = spark.createDataFrame(data).toDF(*columns)\n"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"36ff40e6-72e6-42ff-ba2f-f98c95f6755f"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["import pyspark\nfrom pyspark.sql import Row\nrowData = map(lambda x: Row(*x), data) \ndfFromData3 = spark.createDataFrame(rowData,columns)\n"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"5063797c-fdd0-488a-a63b-38b64b8a6a8c"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["\nfrom pyspark.sql.types import StructType,StructField, StringType, IntegerType\ndata2 = [(\"James\",\"\",\"Smith\",\"36636\",\"M\",3000),\n    (\"Michael\",\"Rose\",\"\",\"40288\",\"M\",4000),\n    (\"Robert\",\"\",\"Williams\",\"42114\",\"M\",4000),\n    (\"Maria\",\"Anne\",\"Jones\",\"39192\",\"F\",4000),\n    (\"Jen\",\"Mary\",\"Brown\",\"\",\"F\",-1)\n  ]\n\nschema = StructType([ \\\n    StructField(\"firstname\",StringType(),True), \\\n    StructField(\"middlename\",StringType(),True), \\\n    StructField(\"lastname\",StringType(),True), \\\n    StructField(\"id\", StringType(), True), \\\n    StructField(\"gender\", StringType(), True), \\\n    StructField(\"salary\", IntegerType(), True) \\\n  ])\n \ndf = spark.createDataFrame(data=data2,schema=schema)\ndf.printSchema()\ndf.show(truncate=False)\n"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"8b80ccf7-aff3-4da3-b08c-5df96c3ceaa6"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["import pyspark\nfrom pyspark.sql.types import StructType,StructField,StringType,IntegerType\ndata2 = [(\"James\",\"\",\"Smith\",36636,\"M\",3000),\n    (\"Michael\",\"Rose\",\"\",40288,\"M\",4000),\n    (\"Robert\",\"\",\"Williams\",42114,\"M\",4000),\n    (\"Maria\",\"Anne\",\"Jones\",39192,\"F\",4000),\n    (\"Jen\",\"Mary\",\"Brown\",1234,\"F\",-1)]\n\nschema = StructType([\\\n         StructField(\"firstname\",StringType(),True), \\\n         StructField(\"middlename\",StringType(),True), \\\n         StructField(\"lastname\",StringType(),True), \\\n         StructField(\"id\",IntegerType(),True), \\\n         StructField(\"gender\",StringType(),True), \\\n         StructField(\"salary\",IntegerType(),True) \\\n                 ])\ndf = spark.createDataFrame(data = data2,schema=schema)\ndf.printSchema()\ndf.show(truncate=False)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"0067fa84-9bbd-4521-a90f-5e64b21587c2"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"root\n |-- firstname: string (nullable = true)\n |-- middlename: string (nullable = true)\n |-- lastname: string (nullable = true)\n |-- id: integer (nullable = true)\n |-- gender: string (nullable = true)\n |-- salary: integer (nullable = true)\n\n+---------+----------+--------+-----+------+------+\n|firstname|middlename|lastname|id   |gender|salary|\n+---------+----------+--------+-----+------+------+\n|James    |          |Smith   |36636|M     |3000  |\n|Michael  |Rose      |        |40288|M     |4000  |\n|Robert   |          |Williams|42114|M     |4000  |\n|Maria    |Anne      |Jones   |39192|F     |4000  |\n|Jen      |Mary      |Brown   |1234 |F     |-1    |\n+---------+----------+--------+-----+------+------+\n\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["root\n |-- firstname: string (nullable = true)\n |-- middlename: string (nullable = true)\n |-- lastname: string (nullable = true)\n |-- id: integer (nullable = true)\n |-- gender: string (nullable = true)\n |-- salary: integer (nullable = true)\n\n+---------+----------+--------+-----+------+------+\n|firstname|middlename|lastname|id   |gender|salary|\n+---------+----------+--------+-----+------+------+\n|James    |          |Smith   |36636|M     |3000  |\n|Michael  |Rose      |        |40288|M     |4000  |\n|Robert   |          |Williams|42114|M     |4000  |\n|Maria    |Anne      |Jones   |39192|F     |4000  |\n|Jen      |Mary      |Brown   |1234 |F     |-1    |\n+---------+----------+--------+-----+------+------+\n\n"]}}],"execution_count":0},{"cell_type":"code","source":["1. Create Empty RDD in PySpark\nCreate an empty RDD by using emptyRDD() of SparkContext for example spark.sparkContext.emptyRDD()."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"ce2bf3e2-ebc8-48e3-9701-6d25c9aef974"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["\nfrom pyspark.sql import SparkSession\nspark = SparkSession.builder.appName('SparkByExamples.com').getOrCreate()\n\n#Creates Empty RDD\nemptyRDD = spark.sparkContext.emptyRDD()\nprint(emptyRDD)\n\n#Diplays\n#EmptyRDD[188] at emptyRDD\n\n#Creates Empty RDD using parallelize\nrdd2= spark.sparkContext.parallelize([])\nprint(rdd2)\n\n#EmptyRDD[205] at emptyRDD at NativeMethodAccessorImpl.java:0\n#ParallelCollectionRDD[206] at readRDDFromFile at PythonRDD.scala:262\n\n"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"a4386a48-fd37-4b4a-bebc-d61c0011fcb2"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["import pyspark\nfrom pyspark.sql import SparkSession\nspark = SparkSession.builder.appName('SparkByExamples.com').getOrCreate()\n#create empty RDD\nemptyRDD = spark.sparkContext.emptyRDD()\nprint(emptyRDD)\n"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"a7577f7d-3547-4f29-a62d-07d9133ae382"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"EmptyRDD[2] at emptyRDD at NativeMethodAccessorImpl.java:0\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["EmptyRDD[2] at emptyRDD at NativeMethodAccessorImpl.java:0\n"]}}],"execution_count":0},{"cell_type":"code","source":["emptyRRD2 = spark.sparkContext.Parallelize([])\nprint(emptyRDD2)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"f557abc9-2748-4df1-bd00-91c4a0e047f7"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n\u001B[0;31mAttributeError\u001B[0m                            Traceback (most recent call last)\n\u001B[0;32m<command-392957826950563>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[0;32m----> 1\u001B[0;31m \u001B[0memptyRRD2\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mspark\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msparkContext\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mParallelize\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      2\u001B[0m \u001B[0mprint\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0memptyRDD2\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;31mAttributeError\u001B[0m: 'RemoteContext' object has no attribute 'Parallelize'","errorSummary":"<span class='ansi-red-fg'>AttributeError</span>: 'RemoteContext' object has no attribute 'Parallelize'","metadata":{},"errorTraceType":"ansi","type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/plain":["\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n\u001B[0;31mAttributeError\u001B[0m                            Traceback (most recent call last)\n\u001B[0;32m<command-392957826950563>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[0;32m----> 1\u001B[0;31m \u001B[0memptyRRD2\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mspark\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msparkContext\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mParallelize\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      2\u001B[0m \u001B[0mprint\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0memptyRDD2\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;31mAttributeError\u001B[0m: 'RemoteContext' object has no attribute 'Parallelize'"]}}],"execution_count":0},{"cell_type":"code","source":["2. Create Empty DataFrame with Schema (StructType)\nIn order to create an empty PySpark DataFrame manually with schema ( column names & data types) first, Create a schema using StructType and StructField ."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"4ec1599f-9c06-4030-a940-6709bf9eb018"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["from pyspark.sql.types import StructType,StructField,StringType,IntegerType,DateType\nschema = StructType([\n    StructField('fisstname',StringType(), True),\n    StructField('Lastname',StringType(),True),\n    StructField('Middlename',StringType(),True),\n    StructField('Id',IntegerType(),True),\n    StructField('Date',DateType(),True)\n])\n#Now use the empty RDD created above and pass it to createDataFrame() of SparkSession along with the schema for column names & data types.\n#create empty RRD\ndf= spark.createDataFrame(emptyRDD,schema)\ndf.printSchema()\n"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"712a7b31-c7a3-445c-a6f0-7387f248c5e6"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"root\n |-- fisstname: string (nullable = true)\n |-- Lastname: string (nullable = true)\n |-- Middlename: string (nullable = true)\n |-- Id: integer (nullable = true)\n |-- Date: date (nullable = true)\n\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["root\n |-- fisstname: string (nullable = true)\n |-- Lastname: string (nullable = true)\n |-- Middlename: string (nullable = true)\n |-- Id: integer (nullable = true)\n |-- Date: date (nullable = true)\n\n"]}}],"execution_count":0},{"cell_type":"code","source":["3. Convert Empty RDD to DataFrame\nYou can also create empty DataFrame by converting empty RDD to DataFrame using toDF()."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"e51bc929-5cf6-4987-9bfe-1eb06cf19fbc"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["#convert empty RDD to dataframe DF()\ndf1 = emptyRDD.toDF(schema)\ndf1.printSchema()\n"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"227ad40d-e0cb-489e-9318-7bcc24280f7f"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"root\n |-- fisstname: string (nullable = true)\n |-- Lastname: string (nullable = true)\n |-- Middlename: string (nullable = true)\n |-- Id: integer (nullable = true)\n |-- Date: date (nullable = true)\n\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["root\n |-- fisstname: string (nullable = true)\n |-- Lastname: string (nullable = true)\n |-- Middlename: string (nullable = true)\n |-- Id: integer (nullable = true)\n |-- Date: date (nullable = true)\n\n"]}}],"execution_count":0},{"cell_type":"code","source":["4. Create Empty DataFrame with Schema.\nSo far I have covered creating an empty DataFrame from RDD, but here will create it manually with schema and without RDD."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"847f45ad-59be-4ad7-9a21-162499d18a2f"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["df2 = spark.createDataFrame([],schema)\ndf2.printSchema()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"dfac87dd-d64c-4715-a06d-ea4fcbec27b9"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"root\n |-- fisstname: string (nullable = true)\n |-- Lastname: string (nullable = true)\n |-- Middlename: string (nullable = true)\n |-- Id: integer (nullable = true)\n |-- Date: date (nullable = true)\n\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["root\n |-- fisstname: string (nullable = true)\n |-- Lastname: string (nullable = true)\n |-- Middlename: string (nullable = true)\n |-- Id: integer (nullable = true)\n |-- Date: date (nullable = true)\n\n"]}}],"execution_count":0},{"cell_type":"code","source":["#5. Create Empty DataFrame without Schema (no columns)\n#To create empty DataFrame with out schema (no columns) just create a empty schema and use it while creating PySpark DataFrame."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"6df178d5-4e58-423f-8c42-59addda895a9"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["#create empty dataframe woth no schema(no columns)\ndf3= spark.createDataFrame([],StructType([]))\ndf3.printSchema()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"1c5223ac-5feb-4c62-8f20-5337bfac21b0"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"root\n\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["root\n\n"]}}],"execution_count":0},{"cell_type":"code","source":["#Convert PySpark RDD to DataFrame\n#1. Create PySpark RDD\n#First, letâ€™s create an RDD by passing Python list object to sparkContext.parallelize() function. We would need this rdd object for all our examples below.\n\n#In PySpark, when you have data in a list meaning you have a collection of data in a PySpark driver memory when you create an RDD, this collection is going to be parallelized.\n\n\nfrom pyspark.sql import SparkSession\nspark = SparkSession.builder.appName('SparkByExamples.com').getOrCreate()\ndept = [(\"Finance\",10),(\"Marketing\",20),(\"Sales\",30),(\"IT\",40)]\nrdd = spark.sparkContext.parallelize(dept)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"80f0e116-1593-4cde-8a66-2cb0030477d1"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["import pyspark\nfrom pyspark.sql import SparkSession\nspark= SparkSession.builder.appName('SparkByExamples.com').getOrCreate()\ndept = [(\"finanace\",10),(\"Marketing\",20),(\"sales\",30),(\"IT\",40)]\nrdd = spark.sparkContext.parallelize(dept)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"7edc36b2-1338-4e21-857e-0dd15b92a190"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["2. Convert PySpark RDD to DataFrame\nConverting PySpark RDD to DataFrame can be done using toDF(), createDataFrame(). In this section, I will explain these two methods.\n2.1 Using rdd.toDF() function\nPySpark provides toDF() function in RDD which can be used to convert RDD into Dataframe\n\n\ndf = rdd.toDF()\ndf.printSchema()\ndf.show(truncate=False)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"4181ec8a-dd3c-44fd-bc2d-99f136845529"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["df = rdd.toDF()\ndf.printSchema()\ndf.show(truncate=False)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"4ee8f0a7-8df6-47d5-943e-36c742d1daea"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"root\n |-- _1: string (nullable = true)\n |-- _2: long (nullable = true)\n\n+---------+---+\n|_1       |_2 |\n+---------+---+\n|finanace |10 |\n|Marketing|20 |\n|sales    |30 |\n|IT       |40 |\n+---------+---+\n\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["root\n |-- _1: string (nullable = true)\n |-- _2: long (nullable = true)\n\n+---------+---+\n|_1       |_2 |\n+---------+---+\n|finanace |10 |\n|Marketing|20 |\n|sales    |30 |\n|IT       |40 |\n+---------+---+\n\n"]}}],"execution_count":0},{"cell_type":"code","source":["depColumns = [(\"dept_name\"),(\"dept_id\")]\ndf2 = rdd.toDF(depColumns)\ndf2.printSchema()\ndf2.show(truncate = False)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"28b43a85-2d35-433c-bc38-0f5581bfe130"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"root\n |-- dept_name: string (nullable = true)\n |-- dept_id: long (nullable = true)\n\n+---------+-------+\n|dept_name|dept_id|\n+---------+-------+\n|finanace |10     |\n|Marketing|20     |\n|sales    |30     |\n|IT       |40     |\n+---------+-------+\n\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["root\n |-- dept_name: string (nullable = true)\n |-- dept_id: long (nullable = true)\n\n+---------+-------+\n|dept_name|dept_id|\n+---------+-------+\n|finanace |10     |\n|Marketing|20     |\n|sales    |30     |\n|IT       |40     |\n+---------+-------+\n\n"]}}],"execution_count":0},{"cell_type":"code","source":["2.2 Using PySpark createDataFrame() function\nSparkSession class provides createDataFrame() method to create DataFrame and it takes rdd object as an argument.\n\n\ndeptDF = spark.createDataFrame(rdd, schema = deptColumns)\ndeptDF.printSchema()\ndeptDF.show(truncate=False)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":true,"inputWidgets":{},"nuid":"00527605-2cc4-481e-ad83-0c8c5dd11594"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"\u001B[0;36m  File \u001B[0;32m\"<command-392957826950577>\"\u001B[0;36m, line \u001B[0;32m1\u001B[0m\n\u001B[0;31m    2.2 Using PySpark createDataFrame() function\u001B[0m\n\u001B[0m        ^\u001B[0m\n\u001B[0;31mSyntaxError\u001B[0m\u001B[0;31m:\u001B[0m invalid syntax\n","errorSummary":"<span class='ansi-red-fg'>SyntaxError</span>: invalid syntax (<command-392957826950577>, line 1)","metadata":{},"errorTraceType":"ansi","type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/plain":["\u001B[0;36m  File \u001B[0;32m\"<command-392957826950577>\"\u001B[0;36m, line \u001B[0;32m1\u001B[0m\n\u001B[0;31m    2.2 Using PySpark createDataFrame() function\u001B[0m\n\u001B[0m        ^\u001B[0m\n\u001B[0;31mSyntaxError\u001B[0m\u001B[0;31m:\u001B[0m invalid syntax\n"]}}],"execution_count":0},{"cell_type":"code","source":["\ndeptDF = spark.createDataFrame(rdd,schema= depColumns)\ndeptDF.printSchema()\ndeptDF.show(truncate = False)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"2d2952d6-402d-46dd-9c00-5c85ef794e32"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"root\n |-- dept_name: string (nullable = true)\n |-- dept_id: long (nullable = true)\n\n+---------+-------+\n|dept_name|dept_id|\n+---------+-------+\n|finanace |10     |\n|Marketing|20     |\n|sales    |30     |\n|IT       |40     |\n+---------+-------+\n\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["root\n |-- dept_name: string (nullable = true)\n |-- dept_id: long (nullable = true)\n\n+---------+-------+\n|dept_name|dept_id|\n+---------+-------+\n|finanace |10     |\n|Marketing|20     |\n|sales    |30     |\n|IT       |40     |\n+---------+-------+\n\n"]}}],"execution_count":0},{"cell_type":"code","source":["import pyspark\nfrom pyspark.sql.types  import StructType,StructField,StringType,IntegerType, DateType\ndeptschema = StructType([\n    StructField(\"dept_name\", StringType(),True),\n    StructField(\"Dept_id\",IntegerType(),True)\n])\n\nDf4 = spark.createDataFrame(rdd , schema = deptschema)\nDf4.printSchema()\nDf4.show(truncate = False)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"867582b6-0257-4679-bfa6-1fffcb3cc1ba"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"root\n |-- dept_name: string (nullable = true)\n |-- Dept_id: integer (nullable = true)\n\n+---------+-------+\n|dept_name|Dept_id|\n+---------+-------+\n|finanace |10     |\n|Marketing|20     |\n|sales    |30     |\n|IT       |40     |\n+---------+-------+\n\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["root\n |-- dept_name: string (nullable = true)\n |-- Dept_id: integer (nullable = true)\n\n+---------+-------+\n|dept_name|Dept_id|\n+---------+-------+\n|finanace |10     |\n|Marketing|20     |\n|sales    |30     |\n|IT       |40     |\n+---------+-------+\n\n"]}}],"execution_count":0},{"cell_type":"code","source":["\nConvert PySpark DataFrame to Pandas\n\nimport pyspark\nfrom pyspark.sql import SparkSession\n\nspark = SparkSession.builder.appName('SparkByExamples.com').getOrCreate()\n\ndata = [(\"James\",\"\",\"Smith\",\"36636\",\"M\",60000),\n        (\"Michael\",\"Rose\",\"\",\"40288\",\"M\",70000),\n        (\"Robert\",\"\",\"Williams\",\"42114\",\"\",400000),\n        (\"Maria\",\"Anne\",\"Jones\",\"39192\",\"F\",500000),\n        (\"Jen\",\"Mary\",\"Brown\",\"\",\"F\",0)]\n\ncolumns = [\"first_name\",\"middle_name\",\"last_name\",\"dob\",\"gender\",\"salary\"]\npysparkDF = spark.createDataFrame(data = data, schema = columns)\npysparkDF.printSchema()\npysparkDF.show(truncate=False)\n"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"6da92770-7cd7-4b59-8bd4-38a9abea15eb"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["import pyspark\nfrom pyspark.sql import SparkSession\nspark = SparkSession.builder.appName('SparkByExamples.com').getOrCreate()\ndata = [(\"James\",\"\",\"Smith\",\"36636\",\"M\",60000),\n        (\"Michael\",\"Rose\",\"\",\"40288\",\"M\",70000),\n        (\"Robert\",\"\",\"Williams\",\"42114\",\"\",400000),\n        (\"Maria\",\"Anne\",\"Jones\",\"39192\",\"F\",500000),\n        (\"Jen\",\"Mary\",\"Brown\",\"\",\"F\",0)\n       ]\ncolumns = [\"first_name\",\"middle_name\",\"last_name\",\"dob\",\"gender\",\"salary\"]\npysparkDF = spark.createDataFrame(data = data , schema = columns)\npysparkDF.printSchema()\npysparkDF.show(truncate = False)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"dc43c63b-a204-4b47-8753-aa076d6b4b23"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"root\n |-- first_name: string (nullable = true)\n |-- middle_name: string (nullable = true)\n |-- last_name: string (nullable = true)\n |-- dob: string (nullable = true)\n |-- gender: string (nullable = true)\n |-- salary: long (nullable = true)\n\n+----------+-----------+---------+-----+------+------+\n|first_name|middle_name|last_name|dob  |gender|salary|\n+----------+-----------+---------+-----+------+------+\n|James     |           |Smith    |36636|M     |60000 |\n|Michael   |Rose       |         |40288|M     |70000 |\n|Robert    |           |Williams |42114|      |400000|\n|Maria     |Anne       |Jones    |39192|F     |500000|\n|Jen       |Mary       |Brown    |     |F     |0     |\n+----------+-----------+---------+-----+------+------+\n\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["root\n |-- first_name: string (nullable = true)\n |-- middle_name: string (nullable = true)\n |-- last_name: string (nullable = true)\n |-- dob: string (nullable = true)\n |-- gender: string (nullable = true)\n |-- salary: long (nullable = true)\n\n+----------+-----------+---------+-----+------+------+\n|first_name|middle_name|last_name|dob  |gender|salary|\n+----------+-----------+---------+-----+------+------+\n|James     |           |Smith    |36636|M     |60000 |\n|Michael   |Rose       |         |40288|M     |70000 |\n|Robert    |           |Williams |42114|      |400000|\n|Maria     |Anne       |Jones    |39192|F     |500000|\n|Jen       |Mary       |Brown    |     |F     |0     |\n+----------+-----------+---------+-----+------+------+\n\n"]}}],"execution_count":0},{"cell_type":"code","source":["columnsdattypes = StructType([StructField(\"first_name\", StringType(), True),\n                              StructField(\"middle_name\",StringType(),True),\n                              StructField(\"last_name\",StringType(),True),\n                              StructField(\"dob\",StringType(),True),\n                              StructField(\"gender\",StringType(), True),\n                              StructField(\"salary\",IntegerType(),True)\n                             ])\npysparkDf = spark.createDataFrame(data = data , schema = columnsdattypes)\npysparkDf.printSchema()\npysparkDf.show(truncate = False)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"081a1998-cd51-4034-ad6d-d4d8fcbda219"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"root\n |-- first_name: string (nullable = true)\n |-- middle_name: string (nullable = true)\n |-- last_name: string (nullable = true)\n |-- dob: string (nullable = true)\n |-- gender: string (nullable = true)\n |-- salary: integer (nullable = true)\n\n+----------+-----------+---------+-----+------+------+\n|first_name|middle_name|last_name|dob  |gender|salary|\n+----------+-----------+---------+-----+------+------+\n|James     |           |Smith    |36636|M     |60000 |\n|Michael   |Rose       |         |40288|M     |70000 |\n|Robert    |           |Williams |42114|      |400000|\n|Maria     |Anne       |Jones    |39192|F     |500000|\n|Jen       |Mary       |Brown    |     |F     |0     |\n+----------+-----------+---------+-----+------+------+\n\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["root\n |-- first_name: string (nullable = true)\n |-- middle_name: string (nullable = true)\n |-- last_name: string (nullable = true)\n |-- dob: string (nullable = true)\n |-- gender: string (nullable = true)\n |-- salary: integer (nullable = true)\n\n+----------+-----------+---------+-----+------+------+\n|first_name|middle_name|last_name|dob  |gender|salary|\n+----------+-----------+---------+-----+------+------+\n|James     |           |Smith    |36636|M     |60000 |\n|Michael   |Rose       |         |40288|M     |70000 |\n|Robert    |           |Williams |42114|      |400000|\n|Maria     |Anne       |Jones    |39192|F     |500000|\n|Jen       |Mary       |Brown    |     |F     |0     |\n+----------+-----------+---------+-----+------+------+\n\n"]}}],"execution_count":0},{"cell_type":"code","source":["Convert PySpark Dataframe to Pandas DataFrame\nPySpark DataFrame provides a method toPandas() to convert it to Python Pandas DataFrame."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"0ab2d7a2-6470-4594-b9e6-d9e3c589fdd9"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["pandasDF = pysparkDf.toPandas()\nprint(pandasDF)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"a806cdec-83af-4a92-aeea-7782bba74e4a"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"  first_name middle_name last_name    dob gender  salary\n0      James                 Smith  36636      M   60000\n1    Michael        Rose            40288      M   70000\n2     Robert              Williams  42114         400000\n3      Maria        Anne     Jones  39192      F  500000\n4        Jen        Mary     Brown             F       0\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["  first_name middle_name last_name    dob gender  salary\n0      James                 Smith  36636      M   60000\n1    Michael        Rose            40288      M   70000\n2     Robert              Williams  42114         400000\n3      Maria        Anne     Jones  39192      F  500000\n4        Jen        Mary     Brown             F       0\n"]}}],"execution_count":0},{"cell_type":"code","source":["import pyspark\nfrom pyspark.sql import SparkSession\nspark = SparkSession.builder.appName('SparkByExamples.com').getOrCreate()\ndata = [((\"James\",\"\",\"Smith\"),\"36636\",\"M\",60000),\n        ((\"Michael\",\"Rose\",\"\"),\"40288\",\"M\",70000),\n        ((\"Robert\",\"\",\"Williams\"),\"42114\",\"\",400000),\n        ((\"Maria\",\"Anne\",\"Jones\"),\"39192\",\"F\",500000),\n        ((\"Jen\",\"Mary\",\"Brown\"),\"\",\"F\",0)\n       ]\ncolumns = [\"first_name\",\"middle_name\",\"last_name\",\"dob\",\"gender\",\"salary\"]\n\npysparkDF = spark.createDataFrame(data = data , schema = columns)\npysparkDF.printSchema()\npysparkDF.show(truncate = False)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"8b75e277-7f85-4fa6-a0e6-e774772b896d"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n\u001B[0;31mIndexError\u001B[0m                                Traceback (most recent call last)\n\u001B[0;32m<command-1603522030302347>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m      9\u001B[0m        ]\n\u001B[1;32m     10\u001B[0m \u001B[0;31m#columns = [\"first_name\",\"middle_name\",\"last_name\",\"dob\",\"gender\",\"salary\"]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 11\u001B[0;31m \u001B[0mpysparkDF\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mspark\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mcreateDataFrame\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mdata\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mdata\u001B[0m \u001B[0;34m,\u001B[0m \u001B[0mschema\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mcolumns\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     12\u001B[0m \u001B[0mpysparkDF\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mprintSchema\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     13\u001B[0m \u001B[0mpysparkDF\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mshow\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mtruncate\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;32mFalse\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;32m/databricks/spark/python/pyspark/sql/session.py\u001B[0m in \u001B[0;36mcreateDataFrame\u001B[0;34m(self, data, schema, samplingRatio, verifySchema)\u001B[0m\n\u001B[1;32m    720\u001B[0m             return super(SparkSession, self).createDataFrame(\n\u001B[1;32m    721\u001B[0m                 data, schema, samplingRatio, verifySchema)\n\u001B[0;32m--> 722\u001B[0;31m         \u001B[0;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_create_dataframe\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mdata\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mschema\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0msamplingRatio\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mverifySchema\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    723\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    724\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0m_create_dataframe\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mdata\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mschema\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0msamplingRatio\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mverifySchema\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;32m/databricks/spark/python/pyspark/sql/session.py\u001B[0m in \u001B[0;36m_create_dataframe\u001B[0;34m(self, data, schema, samplingRatio, verifySchema)\u001B[0m\n\u001B[1;32m    752\u001B[0m                 \u001B[0mrdd\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mschema\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_createFromRDD\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mdata\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mmap\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mprepare\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mschema\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0msamplingRatio\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    753\u001B[0m             \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 754\u001B[0;31m                 \u001B[0mrdd\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mschema\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_createFromLocal\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mmap\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mprepare\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mdata\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mschema\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    755\u001B[0m             \u001B[0mjrdd\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_jvm\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mSerDeUtil\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mtoJavaArray\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mrdd\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_to_java_object_rdd\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    756\u001B[0m             \u001B[0mjdf\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_jsparkSession\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mapplySchemaToPythonRDD\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mjrdd\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mrdd\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mschema\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mjson\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;32m/databricks/spark/python/pyspark/sql/session.py\u001B[0m in \u001B[0;36m_createFromLocal\u001B[0;34m(self, data, schema)\u001B[0m\n\u001B[1;32m    528\u001B[0m         \u001B[0mwrite\u001B[0m \u001B[0mtemp\u001B[0m \u001B[0mfiles\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    529\u001B[0m         \"\"\"\n\u001B[0;32m--> 530\u001B[0;31m         \u001B[0mdata\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mschema\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_wrap_data_schema\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mdata\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mschema\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    531\u001B[0m         \u001B[0;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_sc\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mparallelize\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mdata\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mschema\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    532\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;32m/databricks/spark/python/pyspark/sql/session.py\u001B[0m in \u001B[0;36m_wrap_data_schema\u001B[0;34m(self, data, schema)\u001B[0m\n\u001B[1;32m    512\u001B[0m             \u001B[0;32mif\u001B[0m \u001B[0misinstance\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mschema\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m(\u001B[0m\u001B[0mlist\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mtuple\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    513\u001B[0m                 \u001B[0;32mfor\u001B[0m \u001B[0mi\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mname\u001B[0m \u001B[0;32min\u001B[0m \u001B[0menumerate\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mschema\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 514\u001B[0;31m                     \u001B[0mstruct\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mfields\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mi\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mname\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mname\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    515\u001B[0m                     \u001B[0mstruct\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mnames\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mi\u001B[0m\u001B[0;34m]\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mname\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    516\u001B[0m             \u001B[0mschema\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mstruct\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;31mIndexError\u001B[0m: list index out of range","errorSummary":"<span class='ansi-red-fg'>IndexError</span>: list index out of range","metadata":{},"errorTraceType":"ansi","type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/plain":["\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n\u001B[0;31mIndexError\u001B[0m                                Traceback (most recent call last)\n\u001B[0;32m<command-1603522030302347>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m      9\u001B[0m        ]\n\u001B[1;32m     10\u001B[0m \u001B[0;31m#columns = [\"first_name\",\"middle_name\",\"last_name\",\"dob\",\"gender\",\"salary\"]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 11\u001B[0;31m \u001B[0mpysparkDF\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mspark\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mcreateDataFrame\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mdata\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mdata\u001B[0m \u001B[0;34m,\u001B[0m \u001B[0mschema\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mcolumns\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     12\u001B[0m \u001B[0mpysparkDF\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mprintSchema\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     13\u001B[0m \u001B[0mpysparkDF\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mshow\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mtruncate\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;32mFalse\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;32m/databricks/spark/python/pyspark/sql/session.py\u001B[0m in \u001B[0;36mcreateDataFrame\u001B[0;34m(self, data, schema, samplingRatio, verifySchema)\u001B[0m\n\u001B[1;32m    720\u001B[0m             return super(SparkSession, self).createDataFrame(\n\u001B[1;32m    721\u001B[0m                 data, schema, samplingRatio, verifySchema)\n\u001B[0;32m--> 722\u001B[0;31m         \u001B[0;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_create_dataframe\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mdata\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mschema\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0msamplingRatio\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mverifySchema\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    723\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    724\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0m_create_dataframe\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mdata\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mschema\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0msamplingRatio\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mverifySchema\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;32m/databricks/spark/python/pyspark/sql/session.py\u001B[0m in \u001B[0;36m_create_dataframe\u001B[0;34m(self, data, schema, samplingRatio, verifySchema)\u001B[0m\n\u001B[1;32m    752\u001B[0m                 \u001B[0mrdd\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mschema\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_createFromRDD\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mdata\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mmap\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mprepare\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mschema\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0msamplingRatio\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    753\u001B[0m             \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 754\u001B[0;31m                 \u001B[0mrdd\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mschema\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_createFromLocal\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mmap\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mprepare\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mdata\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mschema\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    755\u001B[0m             \u001B[0mjrdd\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_jvm\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mSerDeUtil\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mtoJavaArray\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mrdd\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_to_java_object_rdd\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    756\u001B[0m             \u001B[0mjdf\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_jsparkSession\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mapplySchemaToPythonRDD\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mjrdd\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mrdd\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mschema\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mjson\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;32m/databricks/spark/python/pyspark/sql/session.py\u001B[0m in \u001B[0;36m_createFromLocal\u001B[0;34m(self, data, schema)\u001B[0m\n\u001B[1;32m    528\u001B[0m         \u001B[0mwrite\u001B[0m \u001B[0mtemp\u001B[0m \u001B[0mfiles\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    529\u001B[0m         \"\"\"\n\u001B[0;32m--> 530\u001B[0;31m         \u001B[0mdata\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mschema\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_wrap_data_schema\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mdata\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mschema\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    531\u001B[0m         \u001B[0;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_sc\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mparallelize\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mdata\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mschema\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    532\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;32m/databricks/spark/python/pyspark/sql/session.py\u001B[0m in \u001B[0;36m_wrap_data_schema\u001B[0;34m(self, data, schema)\u001B[0m\n\u001B[1;32m    512\u001B[0m             \u001B[0;32mif\u001B[0m \u001B[0misinstance\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mschema\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m(\u001B[0m\u001B[0mlist\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mtuple\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    513\u001B[0m                 \u001B[0;32mfor\u001B[0m \u001B[0mi\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mname\u001B[0m \u001B[0;32min\u001B[0m \u001B[0menumerate\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mschema\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 514\u001B[0;31m                     \u001B[0mstruct\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mfields\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mi\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mname\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mname\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    515\u001B[0m                     \u001B[0mstruct\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mnames\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mi\u001B[0m\u001B[0;34m]\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mname\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    516\u001B[0m             \u001B[0mschema\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mstruct\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;31mIndexError\u001B[0m: list index out of range"]}}],"execution_count":0},{"cell_type":"code","source":["data1 = [((\"James\",\"\",\"Smith\"),\"36636\",\"M\",60000),\\\n        ((\"Michael\",\"Rose\",\"\"),\"40288\",\"M\",70000),\\\n        ((\"Robert\",\"\",\"Williams\"),\"42114\",\"\",400000),\\\n        ((\"Maria\",\"Anne\",\"Jones\"),\"39192\",\"F\",500000),\\\n        ((\"Jen\",\"Mary\",\"Brown\"),\"\",\"F\",0)\\\n]\n\ndatastruct = StructType([\n    StructField(\"name\",StructType([StructField('first_name',StringType(),True),\n                                   StructField('middle_name',StringType(),True),\n                                   StructField('last_name',StringType(),True)])),\n     StructField('dob', StringType(), True),\n         StructField('gender', StringType(), True),\n         StructField('salary', IntegerType(), True)\n])\n\ndataDF = spark.createDataFrame(data = data1 , schema = datastruct)\ndataDF.printSchema()\npandaDF = dataDF.toPandas()\nprint(pandaDF)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"174e1848-a75b-46c2-bd39-37031ad0af46"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"root\n |-- name: struct (nullable = true)\n |    |-- first_name: string (nullable = true)\n |    |-- middle_name: string (nullable = true)\n |    |-- last_name: string (nullable = true)\n |-- dob: string (nullable = true)\n |-- gender: string (nullable = true)\n |-- salary: integer (nullable = true)\n\n                                                name    dob gender  salary\n0  {'first_name': 'James', 'middle_name': '', 'la...  36636      M   60000\n1  {'first_name': 'Michael', 'middle_name': 'Rose...  40288      M   70000\n2  {'first_name': 'Robert', 'middle_name': '', 'l...  42114         400000\n3  {'first_name': 'Maria', 'middle_name': 'Anne',...  39192      F  500000\n4  {'first_name': 'Jen', 'middle_name': 'Mary', '...             F       0\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["root\n |-- name: struct (nullable = true)\n |    |-- first_name: string (nullable = true)\n |    |-- middle_name: string (nullable = true)\n |    |-- last_name: string (nullable = true)\n |-- dob: string (nullable = true)\n |-- gender: string (nullable = true)\n |-- salary: integer (nullable = true)\n\n                                                name    dob gender  salary\n0  {'first_name': 'James', 'middle_name': '', 'la...  36636      M   60000\n1  {'first_name': 'Michael', 'middle_name': 'Rose...  40288      M   70000\n2  {'first_name': 'Robert', 'middle_name': '', 'l...  42114         400000\n3  {'first_name': 'Maria', 'middle_name': 'Anne',...  39192      F  500000\n4  {'first_name': 'Jen', 'middle_name': 'Mary', '...             F       0\n"]}}],"execution_count":0},{"cell_type":"code","source":["PySpark show() â€“ Display DataFrame Contents in Table"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"37d46a3f-f728-4088-a927-5cd030ee8612"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["import pyspark\nfrom pyspark.sql import SparkSession\nspark = SparkSession.builder.appName('Sparkbyexample.com').getOrCreate()\ncolumns = [\"Seqno\",\"Quote\"]\ndata = [(\"1\", \"Be the change that you wish to see in the world\"),\n    (\"2\", \"Everyone thinks of changing the world, but no one thinks of changing himself.\"),\n    (\"3\", \"The purpose of our lives is to be happy.\"),\n    (\"4\", \"Be cool.\")]\ndataFrame = spark.createDataFrame(data = data , schema = columns)\ndataFrame.printSchema()\ndataframe.show(trunacte = False)# diplay full content in column.\ndataframe.show(2,trunacte = False)# diplay 2 rows and  full content in  two columns.\ndataFrame.show(truncate = False,vertical=True)# diplay columns  and  values vertically."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"72539cfa-fdeb-4d56-8f42-0d6822303996"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"root\n |-- Seqno: string (nullable = true)\n |-- Quote: string (nullable = true)\n\n-RECORD 0------------------------------------------------------------------------------\n Seqno | 1                                                                             \n Quote | Be the change that you wish to see in the world                               \n-RECORD 1------------------------------------------------------------------------------\n Seqno | 2                                                                             \n Quote | Everyone thinks of changing the world, but no one thinks of changing himself. \n-RECORD 2------------------------------------------------------------------------------\n Seqno | 3                                                                             \n Quote | The purpose of our lives is to be happy.                                      \n-RECORD 3------------------------------------------------------------------------------\n Seqno | 4                                                                             \n Quote | Be cool.                                                                      \n\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["root\n |-- Seqno: string (nullable = true)\n |-- Quote: string (nullable = true)\n\n-RECORD 0------------------------------------------------------------------------------\n Seqno | 1                                                                             \n Quote | Be the change that you wish to see in the world                               \n-RECORD 1------------------------------------------------------------------------------\n Seqno | 2                                                                             \n Quote | Everyone thinks of changing the world, but no one thinks of changing himself. \n-RECORD 2------------------------------------------------------------------------------\n Seqno | 3                                                                             \n Quote | The purpose of our lives is to be happy.                                      \n-RECORD 3------------------------------------------------------------------------------\n Seqno | 4                                                                             \n Quote | Be cool.                                                                      \n\n"]}}],"execution_count":0},{"cell_type":"code","source":["PySpark StructType & StructField Explained with Examples"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"6b23a413-b777-4710-8089-ca58b529c7e4"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["import pyspark\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.types import StructType,StructField,StringType,IntegerType,DateType\nspark = SparkSession.builder.appName('SparkByExample.com').getOrCreate()\n                                    \ndata1 = [(\"James\",\"\",\"Smith\",\"36636\",\"M\",3000),\n    (\"Michael\",\"Rose\",\"\",\"40288\",\"M\",4000),\n    (\"Robert\",\"\",\"Williams\",\"42114\",\"M\",4000),\n    (\"Maria\",\"Anne\",\"Jones\",\"39192\",\"F\",4000),\n    (\"Jen\",\"Mary\",\"Brown\",\"\",\"F\",-1)\n        ]\n       \nschemastruct = StructType([\\\n                          StructField(\"Firstname\",StringType(),True),\\\n                    StructField(\"middlename\",StringType(),True),\\\n                    StructField(\"lastname\",StringType(),True),\\\n                    StructField(\"id\",StringType(),True),\\\n                    StructField(\"gender\",StringType(),True),\\\n                    StructField(\"salary\",IntegerType(),True)\\\n                   ])\ndf = spark.createDataFrame(data = data1, schema = schemastruct)\ndf.printSchema()\ndf.show(truncate = False)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"b46666d3-0737-43f9-a2a3-595562218cae"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"root\n |-- Firstname: string (nullable = true)\n |-- middlename: string (nullable = true)\n |-- lastname: string (nullable = true)\n |-- id: string (nullable = true)\n |-- gender: string (nullable = true)\n |-- salary: integer (nullable = true)\n\n+---------+----------+--------+-----+------+------+\n|Firstname|middlename|lastname|id   |gender|salary|\n+---------+----------+--------+-----+------+------+\n|James    |          |Smith   |36636|M     |3000  |\n|Michael  |Rose      |        |40288|M     |4000  |\n|Robert   |          |Williams|42114|M     |4000  |\n|Maria    |Anne      |Jones   |39192|F     |4000  |\n|Jen      |Mary      |Brown   |     |F     |-1    |\n+---------+----------+--------+-----+------+------+\n\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["root\n |-- Firstname: string (nullable = true)\n |-- middlename: string (nullable = true)\n |-- lastname: string (nullable = true)\n |-- id: string (nullable = true)\n |-- gender: string (nullable = true)\n |-- salary: integer (nullable = true)\n\n+---------+----------+--------+-----+------+------+\n|Firstname|middlename|lastname|id   |gender|salary|\n+---------+----------+--------+-----+------+------+\n|James    |          |Smith   |36636|M     |3000  |\n|Michael  |Rose      |        |40288|M     |4000  |\n|Robert   |          |Williams|42114|M     |4000  |\n|Maria    |Anne      |Jones   |39192|F     |4000  |\n|Jen      |Mary      |Brown   |     |F     |-1    |\n+---------+----------+--------+-----+------+------+\n\n"]}}],"execution_count":0},{"cell_type":"code","source":["4. Defining Nested StructType object struct\nWhile working on DataFrame we often need to work with the nested struct column and this can be defined using StructType.\n"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"12392b75-73f2-4e8d-931a-69efcdcfb52a"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["nesteddata = [((\"James\",\"\",\"Smith\"),\"36636\",\"M\",3000),\n    ((\"Michael\",\"Rose\",\"\"),\"40288\",\"M\",4000),\n    ((\"Robert\",\"\",\"Williams\"),\"42114\",\"M\",4000),\n    ((\"Maria\",\"Anne\",\"Jones\"),\"39192\",\"F\",4000),\n    ((\"Jen\",\"Mary\",\"Brown\"),\"\",\"F\",-1)\n        ]\nnestedstruct = StructType([\n    StructField(\"name\",StructType([StructField(\"Firstname\",StringType(),True),\n                                    StructField(\"middlename\",StringType(),True),\n                                    StructField(\"lastname\",StringType(),True) \n                                  ])),\n    StructField(\"id\",StringType(),True),\n    StructField(\"gender\",StringType(),True),\n    StructField(\"salary\",IntegerType(),True)\n    \n])\n\nDF2 = spark.createDataFrame(data = nesteddata, schema = nestedstruct)\nDF2.printSchema()\nDF2.show(truncate = False)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"3cd171fd-48e2-46b8-8f9f-170247dd0eb7"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"root\n |-- name: struct (nullable = true)\n |    |-- Firstname: string (nullable = true)\n |    |-- middlename: string (nullable = true)\n |    |-- lastname: string (nullable = true)\n |-- id: string (nullable = true)\n |-- gender: string (nullable = true)\n |-- salary: integer (nullable = true)\n\n+--------------------+-----+------+------+\n|name                |id   |gender|salary|\n+--------------------+-----+------+------+\n|{James, , Smith}    |36636|M     |3000  |\n|{Michael, Rose, }   |40288|M     |4000  |\n|{Robert, , Williams}|42114|M     |4000  |\n|{Maria, Anne, Jones}|39192|F     |4000  |\n|{Jen, Mary, Brown}  |     |F     |-1    |\n+--------------------+-----+------+------+\n\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["root\n |-- name: struct (nullable = true)\n |    |-- Firstname: string (nullable = true)\n |    |-- middlename: string (nullable = true)\n |    |-- lastname: string (nullable = true)\n |-- id: string (nullable = true)\n |-- gender: string (nullable = true)\n |-- salary: integer (nullable = true)\n\n+--------------------+-----+------+------+\n|name                |id   |gender|salary|\n+--------------------+-----+------+------+\n|{James, , Smith}    |36636|M     |3000  |\n|{Michael, Rose, }   |40288|M     |4000  |\n|{Robert, , Williams}|42114|M     |4000  |\n|{Maria, Anne, Jones}|39192|F     |4000  |\n|{Jen, Mary, Brown}  |     |F     |-1    |\n+--------------------+-----+------+------+\n\n"]}}],"execution_count":0},{"cell_type":"code","source":["5. Adding & Changing struct of the DataFrame\nUsing PySpark SQL function struct(), we can change the struct of the existing DataFrame and add a new StructType to it. The below example demonstrates how to copy the columns from one structure to another and adding a new column. PySpark Column Class also provides some functions to work with the StructType column."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"a57a386f-f474-4389-988c-9be38c787c5e"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":[""],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"e6ee7a04-72a4-4c32-9290-a2edf8cdf1a1"}},"outputs":[],"execution_count":0}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"Test_Notebook","dashboards":[],"notebookMetadata":{"pythonIndentUnit":4},"language":"python","widgets":{},"notebookOrigID":2639287515226076}},"nbformat":4,"nbformat_minor":0}
