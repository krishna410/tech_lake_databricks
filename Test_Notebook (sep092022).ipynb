{"cells":[{"cell_type":"code","source":["columns = [\"language\",\"user_count\"]\ndata = [(\"java\",\"20000\"),(\"python\",\"10000\"),(\"scala\",\"3000\")]"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"799dc7b1-77ed-479d-81a0-415f2011bb35"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["first, let’s create a Spark RDD from a collection List by calling parallelize() function from SparkContext . We would need this rdd object for all our examples below.\nspark = SparkSession.builder.appName('SparkByExamples.com').getOrCreate()\nrdd = spark.sparkContext.parallelize(data)\n"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"125935b7-4878-4636-abc7-74c0dd87633c"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["import pyspark\nfrom pyspark.sql import SparkSession\nspark = SparkSession.builder.appName('SparkByExamples.com').getOrCreate()\nrdd = spark.sparkContext.parallelize(data)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"989a6f68-cc4c-4a41-b58d-aee3bdb22a24"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["dfFromRDD1= rdd.toDF()\ndfFromRDD1.printSchema()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"e77cee2b-68d1-4090-be40-1789e9469889"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"root\n |-- _1: string (nullable = true)\n |-- _2: string (nullable = true)\n\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["root\n |-- _1: string (nullable = true)\n |-- _2: string (nullable = true)\n\n"]}}],"execution_count":0},{"cell_type":"code","source":["columns = [\"language\",\"user_count\"]\ndfFromrRDD = rdd.toDF(columns)\ndfFromrRDD.printSchema()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"6862abdd-4a53-450c-ae41-387920a39c39"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"root\n |-- language: string (nullable = true)\n |-- user_count: string (nullable = true)\n\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["root\n |-- language: string (nullable = true)\n |-- user_count: string (nullable = true)\n\n"]}}],"execution_count":0},{"cell_type":"code","source":["\ndfFromRDD2 = spark.createDataFrame(rdd).toDF(*columns)\n"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"34707d93-dad1-4384-97f7-ef7b74910d65"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["\ndfFromData2 = spark.createDataFrame(data).toDF(*columns)\n"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"36ff40e6-72e6-42ff-ba2f-f98c95f6755f"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["import pyspark\nfrom pyspark.sql import Row\nrowData = map(lambda x: Row(*x), data) \ndfFromData3 = spark.createDataFrame(rowData,columns)\n"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"5063797c-fdd0-488a-a63b-38b64b8a6a8c"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["\nfrom pyspark.sql.types import StructType,StructField, StringType, IntegerType\ndata2 = [(\"James\",\"\",\"Smith\",\"36636\",\"M\",3000),\n    (\"Michael\",\"Rose\",\"\",\"40288\",\"M\",4000),\n    (\"Robert\",\"\",\"Williams\",\"42114\",\"M\",4000),\n    (\"Maria\",\"Anne\",\"Jones\",\"39192\",\"F\",4000),\n    (\"Jen\",\"Mary\",\"Brown\",\"\",\"F\",-1)\n  ]\n\nschema = StructType([ \\\n    StructField(\"firstname\",StringType(),True), \\\n    StructField(\"middlename\",StringType(),True), \\\n    StructField(\"lastname\",StringType(),True), \\\n    StructField(\"id\", StringType(), True), \\\n    StructField(\"gender\", StringType(), True), \\\n    StructField(\"salary\", IntegerType(), True) \\\n  ])\n \ndf = spark.createDataFrame(data=data2,schema=schema)\ndf.printSchema()\ndf.show(truncate=False)\n"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"8b80ccf7-aff3-4da3-b08c-5df96c3ceaa6"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["import pyspark\nfrom pyspark.sql.types import StructType,StructField,StringType,IntegerType\ndata2 = [(\"James\",\"\",\"Smith\",36636,\"M\",3000),\n    (\"Michael\",\"Rose\",\"\",40288,\"M\",4000),\n    (\"Robert\",\"\",\"Williams\",42114,\"M\",4000),\n    (\"Maria\",\"Anne\",\"Jones\",39192,\"F\",4000),\n    (\"Jen\",\"Mary\",\"Brown\",1234,\"F\",-1)]\n\nschema = StructType([\\\n         StructField(\"firstname\",StringType(),True), \\\n         StructField(\"middlename\",StringType(),True), \\\n         StructField(\"lastname\",StringType(),True), \\\n         StructField(\"id\",IntegerType(),True), \\\n         StructField(\"gender\",StringType(),True), \\\n         StructField(\"salary\",IntegerType(),True) \\\n                 ])\ndf = spark.createDataFrame(data = data2,schema=schema)\ndf.printSchema()\ndf.show(truncate=False)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"0067fa84-9bbd-4521-a90f-5e64b21587c2"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"root\n |-- firstname: string (nullable = true)\n |-- middlename: string (nullable = true)\n |-- lastname: string (nullable = true)\n |-- id: integer (nullable = true)\n |-- gender: string (nullable = true)\n |-- salary: integer (nullable = true)\n\n+---------+----------+--------+-----+------+------+\n|firstname|middlename|lastname|id   |gender|salary|\n+---------+----------+--------+-----+------+------+\n|James    |          |Smith   |36636|M     |3000  |\n|Michael  |Rose      |        |40288|M     |4000  |\n|Robert   |          |Williams|42114|M     |4000  |\n|Maria    |Anne      |Jones   |39192|F     |4000  |\n|Jen      |Mary      |Brown   |1234 |F     |-1    |\n+---------+----------+--------+-----+------+------+\n\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["root\n |-- firstname: string (nullable = true)\n |-- middlename: string (nullable = true)\n |-- lastname: string (nullable = true)\n |-- id: integer (nullable = true)\n |-- gender: string (nullable = true)\n |-- salary: integer (nullable = true)\n\n+---------+----------+--------+-----+------+------+\n|firstname|middlename|lastname|id   |gender|salary|\n+---------+----------+--------+-----+------+------+\n|James    |          |Smith   |36636|M     |3000  |\n|Michael  |Rose      |        |40288|M     |4000  |\n|Robert   |          |Williams|42114|M     |4000  |\n|Maria    |Anne      |Jones   |39192|F     |4000  |\n|Jen      |Mary      |Brown   |1234 |F     |-1    |\n+---------+----------+--------+-----+------+------+\n\n"]}}],"execution_count":0},{"cell_type":"code","source":["1. Create Empty RDD in PySpark\nCreate an empty RDD by using emptyRDD() of SparkContext for example spark.sparkContext.emptyRDD()."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"ce2bf3e2-ebc8-48e3-9701-6d25c9aef974"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["\nfrom pyspark.sql import SparkSession\nspark = SparkSession.builder.appName('SparkByExamples.com').getOrCreate()\n\n#Creates Empty RDD\nemptyRDD = spark.sparkContext.emptyRDD()\nprint(emptyRDD)\n\n#Diplays\n#EmptyRDD[188] at emptyRDD\n\n#Creates Empty RDD using parallelize\nrdd2= spark.sparkContext.parallelize([])\nprint(rdd2)\n\n#EmptyRDD[205] at emptyRDD at NativeMethodAccessorImpl.java:0\n#ParallelCollectionRDD[206] at readRDDFromFile at PythonRDD.scala:262\n\n"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"a4386a48-fd37-4b4a-bebc-d61c0011fcb2"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["import pyspark\nfrom pyspark.sql import SparkSession\nspark = SparkSession.builder.appName('SparkByExamples.com').getOrCreate()\n#create empty RDD\nemptyRDD = spark.sparkContext.emptyRDD()\nprint(emptyRDD)\n"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"a7577f7d-3547-4f29-a62d-07d9133ae382"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"EmptyRDD[2] at emptyRDD at NativeMethodAccessorImpl.java:0\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["EmptyRDD[2] at emptyRDD at NativeMethodAccessorImpl.java:0\n"]}}],"execution_count":0},{"cell_type":"code","source":["emptyRRD2 = spark.sparkContext.Parallelize([])\nprint(emptyRDD2)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"f557abc9-2748-4df1-bd00-91c4a0e047f7"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n\u001B[0;31mAttributeError\u001B[0m                            Traceback (most recent call last)\n\u001B[0;32m<command-392957826950563>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[0;32m----> 1\u001B[0;31m \u001B[0memptyRRD2\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mspark\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msparkContext\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mParallelize\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      2\u001B[0m \u001B[0mprint\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0memptyRDD2\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;31mAttributeError\u001B[0m: 'RemoteContext' object has no attribute 'Parallelize'","errorSummary":"<span class='ansi-red-fg'>AttributeError</span>: 'RemoteContext' object has no attribute 'Parallelize'","metadata":{},"errorTraceType":"ansi","type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/plain":["\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n\u001B[0;31mAttributeError\u001B[0m                            Traceback (most recent call last)\n\u001B[0;32m<command-392957826950563>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[0;32m----> 1\u001B[0;31m \u001B[0memptyRRD2\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mspark\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msparkContext\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mParallelize\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      2\u001B[0m \u001B[0mprint\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0memptyRDD2\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;31mAttributeError\u001B[0m: 'RemoteContext' object has no attribute 'Parallelize'"]}}],"execution_count":0},{"cell_type":"code","source":["2. Create Empty DataFrame with Schema (StructType)\nIn order to create an empty PySpark DataFrame manually with schema ( column names & data types) first, Create a schema using StructType and StructField ."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"4ec1599f-9c06-4030-a940-6709bf9eb018"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["from pyspark.sql.types import StructType,StructField,StringType,IntegerType,DateType\nschema = StructType([\n    StructField('fisstname',StringType(), True),\n    StructField('Lastname',StringType(),True),\n    StructField('Middlename',StringType(),True),\n    StructField('Id',IntegerType(),True),\n    StructField('Date',DateType(),True)\n])\n#Now use the empty RDD created above and pass it to createDataFrame() of SparkSession along with the schema for column names & data types.\n#create empty RRD\ndf= spark.createDataFrame(emptyRDD,schema)\ndf.printSchema()\n"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"712a7b31-c7a3-445c-a6f0-7387f248c5e6"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"root\n |-- fisstname: string (nullable = true)\n |-- Lastname: string (nullable = true)\n |-- Middlename: string (nullable = true)\n |-- Id: integer (nullable = true)\n |-- Date: date (nullable = true)\n\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["root\n |-- fisstname: string (nullable = true)\n |-- Lastname: string (nullable = true)\n |-- Middlename: string (nullable = true)\n |-- Id: integer (nullable = true)\n |-- Date: date (nullable = true)\n\n"]}}],"execution_count":0},{"cell_type":"code","source":["3. Convert Empty RDD to DataFrame\nYou can also create empty DataFrame by converting empty RDD to DataFrame using toDF()."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"e51bc929-5cf6-4987-9bfe-1eb06cf19fbc"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["#convert empty RDD to dataframe DF()\ndf1 = emptyRDD.toDF(schema)\ndf1.printSchema()\n"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"227ad40d-e0cb-489e-9318-7bcc24280f7f"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"root\n |-- fisstname: string (nullable = true)\n |-- Lastname: string (nullable = true)\n |-- Middlename: string (nullable = true)\n |-- Id: integer (nullable = true)\n |-- Date: date (nullable = true)\n\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["root\n |-- fisstname: string (nullable = true)\n |-- Lastname: string (nullable = true)\n |-- Middlename: string (nullable = true)\n |-- Id: integer (nullable = true)\n |-- Date: date (nullable = true)\n\n"]}}],"execution_count":0},{"cell_type":"code","source":["4. Create Empty DataFrame with Schema.\nSo far I have covered creating an empty DataFrame from RDD, but here will create it manually with schema and without RDD."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"847f45ad-59be-4ad7-9a21-162499d18a2f"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["df2 = spark.createDataFrame([],schema)\ndf2.printSchema()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"dfac87dd-d64c-4715-a06d-ea4fcbec27b9"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"root\n |-- fisstname: string (nullable = true)\n |-- Lastname: string (nullable = true)\n |-- Middlename: string (nullable = true)\n |-- Id: integer (nullable = true)\n |-- Date: date (nullable = true)\n\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["root\n |-- fisstname: string (nullable = true)\n |-- Lastname: string (nullable = true)\n |-- Middlename: string (nullable = true)\n |-- Id: integer (nullable = true)\n |-- Date: date (nullable = true)\n\n"]}}],"execution_count":0},{"cell_type":"code","source":["#5. Create Empty DataFrame without Schema (no columns)\n#To create empty DataFrame with out schema (no columns) just create a empty schema and use it while creating PySpark DataFrame."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"6df178d5-4e58-423f-8c42-59addda895a9"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["#create empty dataframe woth no schema(no columns)\ndf3= spark.createDataFrame([],StructType([]))\ndf3.printSchema()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"1c5223ac-5feb-4c62-8f20-5337bfac21b0"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"root\n\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["root\n\n"]}}],"execution_count":0},{"cell_type":"code","source":["#Convert PySpark RDD to DataFrame\n#1. Create PySpark RDD\n#First, let’s create an RDD by passing Python list object to sparkContext.parallelize() function. We would need this rdd object for all our examples below.\n\n#In PySpark, when you have data in a list meaning you have a collection of data in a PySpark driver memory when you create an RDD, this collection is going to be parallelized.\n\n\nfrom pyspark.sql import SparkSession\nspark = SparkSession.builder.appName('SparkByExamples.com').getOrCreate()\ndept = [(\"Finance\",10),(\"Marketing\",20),(\"Sales\",30),(\"IT\",40)]\nrdd = spark.sparkContext.parallelize(dept)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"80f0e116-1593-4cde-8a66-2cb0030477d1"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["import pyspark\nfrom pyspark.sql import SparkSession\nspark= SparkSession.builder.appName('SparkByExamples.com').getOrCreate()\ndept = [(\"finanace\",10),(\"Marketing\",20),(\"sales\",30),(\"IT\",40)]\nrdd = spark.sparkContext.parallelize(dept)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"7edc36b2-1338-4e21-857e-0dd15b92a190"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["2. Convert PySpark RDD to DataFrame\nConverting PySpark RDD to DataFrame can be done using toDF(), createDataFrame(). In this section, I will explain these two methods.\n2.1 Using rdd.toDF() function\nPySpark provides toDF() function in RDD which can be used to convert RDD into Dataframe\n\n\ndf = rdd.toDF()\ndf.printSchema()\ndf.show(truncate=False)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"4181ec8a-dd3c-44fd-bc2d-99f136845529"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["df = rdd.toDF()\ndf.printSchema()\ndf.show(truncate=False)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"4ee8f0a7-8df6-47d5-943e-36c742d1daea"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"root\n |-- _1: string (nullable = true)\n |-- _2: long (nullable = true)\n\n+---------+---+\n|_1       |_2 |\n+---------+---+\n|finanace |10 |\n|Marketing|20 |\n|sales    |30 |\n|IT       |40 |\n+---------+---+\n\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["root\n |-- _1: string (nullable = true)\n |-- _2: long (nullable = true)\n\n+---------+---+\n|_1       |_2 |\n+---------+---+\n|finanace |10 |\n|Marketing|20 |\n|sales    |30 |\n|IT       |40 |\n+---------+---+\n\n"]}}],"execution_count":0},{"cell_type":"code","source":["depColumns = [(\"dept_name\"),(\"dept_id\")]\ndf2 = rdd.toDF(depColumns)\ndf2.printSchema()\ndf2.show(truncate = False)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"28b43a85-2d35-433c-bc38-0f5581bfe130"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"root\n |-- dept_name: string (nullable = true)\n |-- dept_id: long (nullable = true)\n\n+---------+-------+\n|dept_name|dept_id|\n+---------+-------+\n|finanace |10     |\n|Marketing|20     |\n|sales    |30     |\n|IT       |40     |\n+---------+-------+\n\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["root\n |-- dept_name: string (nullable = true)\n |-- dept_id: long (nullable = true)\n\n+---------+-------+\n|dept_name|dept_id|\n+---------+-------+\n|finanace |10     |\n|Marketing|20     |\n|sales    |30     |\n|IT       |40     |\n+---------+-------+\n\n"]}}],"execution_count":0},{"cell_type":"code","source":["2.2 Using PySpark createDataFrame() function\nSparkSession class provides createDataFrame() method to create DataFrame and it takes rdd object as an argument.\n\n\ndeptDF = spark.createDataFrame(rdd, schema = deptColumns)\ndeptDF.printSchema()\ndeptDF.show(truncate=False)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":true,"inputWidgets":{},"nuid":"00527605-2cc4-481e-ad83-0c8c5dd11594"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"\u001B[0;36m  File \u001B[0;32m\"<command-392957826950577>\"\u001B[0;36m, line \u001B[0;32m1\u001B[0m\n\u001B[0;31m    2.2 Using PySpark createDataFrame() function\u001B[0m\n\u001B[0m        ^\u001B[0m\n\u001B[0;31mSyntaxError\u001B[0m\u001B[0;31m:\u001B[0m invalid syntax\n","errorSummary":"<span class='ansi-red-fg'>SyntaxError</span>: invalid syntax (<command-392957826950577>, line 1)","metadata":{},"errorTraceType":"ansi","type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/plain":["\u001B[0;36m  File \u001B[0;32m\"<command-392957826950577>\"\u001B[0;36m, line \u001B[0;32m1\u001B[0m\n\u001B[0;31m    2.2 Using PySpark createDataFrame() function\u001B[0m\n\u001B[0m        ^\u001B[0m\n\u001B[0;31mSyntaxError\u001B[0m\u001B[0;31m:\u001B[0m invalid syntax\n"]}}],"execution_count":0},{"cell_type":"code","source":["\ndeptDF = spark.createDataFrame(rdd,schema= depColumns)\ndeptDF.printSchema()\ndeptDF.show(truncate = False)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"2d2952d6-402d-46dd-9c00-5c85ef794e32"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"root\n |-- dept_name: string (nullable = true)\n |-- dept_id: long (nullable = true)\n\n+---------+-------+\n|dept_name|dept_id|\n+---------+-------+\n|finanace |10     |\n|Marketing|20     |\n|sales    |30     |\n|IT       |40     |\n+---------+-------+\n\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["root\n |-- dept_name: string (nullable = true)\n |-- dept_id: long (nullable = true)\n\n+---------+-------+\n|dept_name|dept_id|\n+---------+-------+\n|finanace |10     |\n|Marketing|20     |\n|sales    |30     |\n|IT       |40     |\n+---------+-------+\n\n"]}}],"execution_count":0},{"cell_type":"code","source":["import pyspark\nfrom pyspark.sql.types  import StructType,StructField,StringType,IntegerType, DateType\ndeptschema = StructType([\n    StructField(\"dept_name\", StringType(),True),\n    StructField(\"Dept_id\",IntegerType(),True)\n])\n\nDf4 = spark.createDataFrame(rdd , schema = deptschema)\nDf4.printSchema()\nDf4.show(truncate = False)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"867582b6-0257-4679-bfa6-1fffcb3cc1ba"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"root\n |-- dept_name: string (nullable = true)\n |-- Dept_id: integer (nullable = true)\n\n+---------+-------+\n|dept_name|Dept_id|\n+---------+-------+\n|finanace |10     |\n|Marketing|20     |\n|sales    |30     |\n|IT       |40     |\n+---------+-------+\n\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["root\n |-- dept_name: string (nullable = true)\n |-- Dept_id: integer (nullable = true)\n\n+---------+-------+\n|dept_name|Dept_id|\n+---------+-------+\n|finanace |10     |\n|Marketing|20     |\n|sales    |30     |\n|IT       |40     |\n+---------+-------+\n\n"]}}],"execution_count":0},{"cell_type":"code","source":["\nConvert PySpark DataFrame to Pandas\n\nimport pyspark\nfrom pyspark.sql import SparkSession\n\nspark = SparkSession.builder.appName('SparkByExamples.com').getOrCreate()\n\ndata = [(\"James\",\"\",\"Smith\",\"36636\",\"M\",60000),\n        (\"Michael\",\"Rose\",\"\",\"40288\",\"M\",70000),\n        (\"Robert\",\"\",\"Williams\",\"42114\",\"\",400000),\n        (\"Maria\",\"Anne\",\"Jones\",\"39192\",\"F\",500000),\n        (\"Jen\",\"Mary\",\"Brown\",\"\",\"F\",0)]\n\ncolumns = [\"first_name\",\"middle_name\",\"last_name\",\"dob\",\"gender\",\"salary\"]\npysparkDF = spark.createDataFrame(data = data, schema = columns)\npysparkDF.printSchema()\npysparkDF.show(truncate=False)\n"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"6da92770-7cd7-4b59-8bd4-38a9abea15eb"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["import pyspark\nfrom pyspark.sql import SparkSession\nspark = SparkSession.builder.appName('SparkByExamples.com').getOrCreate()\ndata = [(\"James\",\"\",\"Smith\",\"36636\",\"M\",60000),\n        (\"Michael\",\"Rose\",\"\",\"40288\",\"M\",70000),\n        (\"Robert\",\"\",\"Williams\",\"42114\",\"\",400000),\n        (\"Maria\",\"Anne\",\"Jones\",\"39192\",\"F\",500000),\n        (\"Jen\",\"Mary\",\"Brown\",\"\",\"F\",0)\n       ]\ncolumns = [\"first_name\",\"middle_name\",\"last_name\",\"dob\",\"gender\",\"salary\"]\npysparkDF = spark.createDataFrame(data = data , schema = columns)\npysparkDF.printSchema()\npysparkDF.show(truncate = False)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"dc43c63b-a204-4b47-8753-aa076d6b4b23"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"root\n |-- first_name: string (nullable = true)\n |-- middle_name: string (nullable = true)\n |-- last_name: string (nullable = true)\n |-- dob: string (nullable = true)\n |-- gender: string (nullable = true)\n |-- salary: long (nullable = true)\n\n+----------+-----------+---------+-----+------+------+\n|first_name|middle_name|last_name|dob  |gender|salary|\n+----------+-----------+---------+-----+------+------+\n|James     |           |Smith    |36636|M     |60000 |\n|Michael   |Rose       |         |40288|M     |70000 |\n|Robert    |           |Williams |42114|      |400000|\n|Maria     |Anne       |Jones    |39192|F     |500000|\n|Jen       |Mary       |Brown    |     |F     |0     |\n+----------+-----------+---------+-----+------+------+\n\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["root\n |-- first_name: string (nullable = true)\n |-- middle_name: string (nullable = true)\n |-- last_name: string (nullable = true)\n |-- dob: string (nullable = true)\n |-- gender: string (nullable = true)\n |-- salary: long (nullable = true)\n\n+----------+-----------+---------+-----+------+------+\n|first_name|middle_name|last_name|dob  |gender|salary|\n+----------+-----------+---------+-----+------+------+\n|James     |           |Smith    |36636|M     |60000 |\n|Michael   |Rose       |         |40288|M     |70000 |\n|Robert    |           |Williams |42114|      |400000|\n|Maria     |Anne       |Jones    |39192|F     |500000|\n|Jen       |Mary       |Brown    |     |F     |0     |\n+----------+-----------+---------+-----+------+------+\n\n"]}}],"execution_count":0},{"cell_type":"code","source":["columnsdattypes = StructType([StructField(\"first_name\", StringType(), True),\n                              StructField(\"middle_name\",StringType(),True),\n                              StructField(\"last_name\",StringType(),True),\n                              StructField(\"dob\",StringType(),True),\n                              StructField(\"gender\",StringType(), True),\n                              StructField(\"salary\",IntegerType(),True)\n                             ])\npysparkDf = spark.createDataFrame(data = data , schema = columnsdattypes)\npysparkDf.printSchema()\npysparkDf.show(truncate = False)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"081a1998-cd51-4034-ad6d-d4d8fcbda219"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"root\n |-- first_name: string (nullable = true)\n |-- middle_name: string (nullable = true)\n |-- last_name: string (nullable = true)\n |-- dob: string (nullable = true)\n |-- gender: string (nullable = true)\n |-- salary: integer (nullable = true)\n\n+----------+-----------+---------+-----+------+------+\n|first_name|middle_name|last_name|dob  |gender|salary|\n+----------+-----------+---------+-----+------+------+\n|James     |           |Smith    |36636|M     |60000 |\n|Michael   |Rose       |         |40288|M     |70000 |\n|Robert    |           |Williams |42114|      |400000|\n|Maria     |Anne       |Jones    |39192|F     |500000|\n|Jen       |Mary       |Brown    |     |F     |0     |\n+----------+-----------+---------+-----+------+------+\n\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["root\n |-- first_name: string (nullable = true)\n |-- middle_name: string (nullable = true)\n |-- last_name: string (nullable = true)\n |-- dob: string (nullable = true)\n |-- gender: string (nullable = true)\n |-- salary: integer (nullable = true)\n\n+----------+-----------+---------+-----+------+------+\n|first_name|middle_name|last_name|dob  |gender|salary|\n+----------+-----------+---------+-----+------+------+\n|James     |           |Smith    |36636|M     |60000 |\n|Michael   |Rose       |         |40288|M     |70000 |\n|Robert    |           |Williams |42114|      |400000|\n|Maria     |Anne       |Jones    |39192|F     |500000|\n|Jen       |Mary       |Brown    |     |F     |0     |\n+----------+-----------+---------+-----+------+------+\n\n"]}}],"execution_count":0},{"cell_type":"code","source":["Convert PySpark Dataframe to Pandas DataFrame\nPySpark DataFrame provides a method toPandas() to convert it to Python Pandas DataFrame."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"0ab2d7a2-6470-4594-b9e6-d9e3c589fdd9"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["pandasDF = pysparkDf.toPandas()\nprint(pandasDF)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"a806cdec-83af-4a92-aeea-7782bba74e4a"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"  first_name middle_name last_name    dob gender  salary\n0      James                 Smith  36636      M   60000\n1    Michael        Rose            40288      M   70000\n2     Robert              Williams  42114         400000\n3      Maria        Anne     Jones  39192      F  500000\n4        Jen        Mary     Brown             F       0\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["  first_name middle_name last_name    dob gender  salary\n0      James                 Smith  36636      M   60000\n1    Michael        Rose            40288      M   70000\n2     Robert              Williams  42114         400000\n3      Maria        Anne     Jones  39192      F  500000\n4        Jen        Mary     Brown             F       0\n"]}}],"execution_count":0},{"cell_type":"code","source":["import pyspark\nfrom pyspark.sql import SparkSession\nspark = SparkSession.builder.appName('SparkByExamples.com').getOrCreate()\ndata = [((\"James\",\"\",\"Smith\"),\"36636\",\"M\",60000),\n        ((\"Michael\",\"Rose\",\"\"),\"40288\",\"M\",70000),\n        ((\"Robert\",\"\",\"Williams\"),\"42114\",\"\",400000),\n        ((\"Maria\",\"Anne\",\"Jones\"),\"39192\",\"F\",500000),\n        ((\"Jen\",\"Mary\",\"Brown\"),\"\",\"F\",0)\n       ]\ncolumns = [\"first_name\",\"middle_name\",\"last_name\",\"dob\",\"gender\",\"salary\"]\n\npysparkDF = spark.createDataFrame(data = data , schema = columns)\npysparkDF.printSchema()\npysparkDF.show(truncate = False)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"8b75e277-7f85-4fa6-a0e6-e774772b896d"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n\u001B[0;31mIndexError\u001B[0m                                Traceback (most recent call last)\n\u001B[0;32m<command-1603522030302347>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m      9\u001B[0m        ]\n\u001B[1;32m     10\u001B[0m \u001B[0;31m#columns = [\"first_name\",\"middle_name\",\"last_name\",\"dob\",\"gender\",\"salary\"]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 11\u001B[0;31m \u001B[0mpysparkDF\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mspark\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mcreateDataFrame\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mdata\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mdata\u001B[0m \u001B[0;34m,\u001B[0m \u001B[0mschema\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mcolumns\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     12\u001B[0m \u001B[0mpysparkDF\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mprintSchema\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     13\u001B[0m \u001B[0mpysparkDF\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mshow\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mtruncate\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;32mFalse\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;32m/databricks/spark/python/pyspark/sql/session.py\u001B[0m in \u001B[0;36mcreateDataFrame\u001B[0;34m(self, data, schema, samplingRatio, verifySchema)\u001B[0m\n\u001B[1;32m    720\u001B[0m             return super(SparkSession, self).createDataFrame(\n\u001B[1;32m    721\u001B[0m                 data, schema, samplingRatio, verifySchema)\n\u001B[0;32m--> 722\u001B[0;31m         \u001B[0;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_create_dataframe\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mdata\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mschema\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0msamplingRatio\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mverifySchema\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    723\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    724\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0m_create_dataframe\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mdata\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mschema\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0msamplingRatio\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mverifySchema\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;32m/databricks/spark/python/pyspark/sql/session.py\u001B[0m in \u001B[0;36m_create_dataframe\u001B[0;34m(self, data, schema, samplingRatio, verifySchema)\u001B[0m\n\u001B[1;32m    752\u001B[0m                 \u001B[0mrdd\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mschema\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_createFromRDD\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mdata\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mmap\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mprepare\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mschema\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0msamplingRatio\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    753\u001B[0m             \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 754\u001B[0;31m                 \u001B[0mrdd\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mschema\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_createFromLocal\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mmap\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mprepare\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mdata\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mschema\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    755\u001B[0m             \u001B[0mjrdd\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_jvm\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mSerDeUtil\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mtoJavaArray\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mrdd\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_to_java_object_rdd\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    756\u001B[0m             \u001B[0mjdf\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_jsparkSession\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mapplySchemaToPythonRDD\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mjrdd\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mrdd\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mschema\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mjson\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;32m/databricks/spark/python/pyspark/sql/session.py\u001B[0m in \u001B[0;36m_createFromLocal\u001B[0;34m(self, data, schema)\u001B[0m\n\u001B[1;32m    528\u001B[0m         \u001B[0mwrite\u001B[0m \u001B[0mtemp\u001B[0m \u001B[0mfiles\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    529\u001B[0m         \"\"\"\n\u001B[0;32m--> 530\u001B[0;31m         \u001B[0mdata\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mschema\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_wrap_data_schema\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mdata\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mschema\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    531\u001B[0m         \u001B[0;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_sc\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mparallelize\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mdata\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mschema\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    532\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;32m/databricks/spark/python/pyspark/sql/session.py\u001B[0m in \u001B[0;36m_wrap_data_schema\u001B[0;34m(self, data, schema)\u001B[0m\n\u001B[1;32m    512\u001B[0m             \u001B[0;32mif\u001B[0m \u001B[0misinstance\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mschema\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m(\u001B[0m\u001B[0mlist\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mtuple\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    513\u001B[0m                 \u001B[0;32mfor\u001B[0m \u001B[0mi\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mname\u001B[0m \u001B[0;32min\u001B[0m \u001B[0menumerate\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mschema\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 514\u001B[0;31m                     \u001B[0mstruct\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mfields\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mi\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mname\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mname\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    515\u001B[0m                     \u001B[0mstruct\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mnames\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mi\u001B[0m\u001B[0;34m]\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mname\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    516\u001B[0m             \u001B[0mschema\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mstruct\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;31mIndexError\u001B[0m: list index out of range","errorSummary":"<span class='ansi-red-fg'>IndexError</span>: list index out of range","metadata":{},"errorTraceType":"ansi","type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/plain":["\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n\u001B[0;31mIndexError\u001B[0m                                Traceback (most recent call last)\n\u001B[0;32m<command-1603522030302347>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m      9\u001B[0m        ]\n\u001B[1;32m     10\u001B[0m \u001B[0;31m#columns = [\"first_name\",\"middle_name\",\"last_name\",\"dob\",\"gender\",\"salary\"]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 11\u001B[0;31m \u001B[0mpysparkDF\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mspark\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mcreateDataFrame\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mdata\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mdata\u001B[0m \u001B[0;34m,\u001B[0m \u001B[0mschema\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mcolumns\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     12\u001B[0m \u001B[0mpysparkDF\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mprintSchema\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     13\u001B[0m \u001B[0mpysparkDF\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mshow\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mtruncate\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;32mFalse\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;32m/databricks/spark/python/pyspark/sql/session.py\u001B[0m in \u001B[0;36mcreateDataFrame\u001B[0;34m(self, data, schema, samplingRatio, verifySchema)\u001B[0m\n\u001B[1;32m    720\u001B[0m             return super(SparkSession, self).createDataFrame(\n\u001B[1;32m    721\u001B[0m                 data, schema, samplingRatio, verifySchema)\n\u001B[0;32m--> 722\u001B[0;31m         \u001B[0;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_create_dataframe\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mdata\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mschema\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0msamplingRatio\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mverifySchema\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    723\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    724\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0m_create_dataframe\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mdata\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mschema\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0msamplingRatio\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mverifySchema\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;32m/databricks/spark/python/pyspark/sql/session.py\u001B[0m in \u001B[0;36m_create_dataframe\u001B[0;34m(self, data, schema, samplingRatio, verifySchema)\u001B[0m\n\u001B[1;32m    752\u001B[0m                 \u001B[0mrdd\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mschema\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_createFromRDD\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mdata\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mmap\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mprepare\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mschema\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0msamplingRatio\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    753\u001B[0m             \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 754\u001B[0;31m                 \u001B[0mrdd\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mschema\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_createFromLocal\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mmap\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mprepare\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mdata\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mschema\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    755\u001B[0m             \u001B[0mjrdd\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_jvm\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mSerDeUtil\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mtoJavaArray\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mrdd\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_to_java_object_rdd\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    756\u001B[0m             \u001B[0mjdf\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_jsparkSession\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mapplySchemaToPythonRDD\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mjrdd\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mrdd\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mschema\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mjson\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;32m/databricks/spark/python/pyspark/sql/session.py\u001B[0m in \u001B[0;36m_createFromLocal\u001B[0;34m(self, data, schema)\u001B[0m\n\u001B[1;32m    528\u001B[0m         \u001B[0mwrite\u001B[0m \u001B[0mtemp\u001B[0m \u001B[0mfiles\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    529\u001B[0m         \"\"\"\n\u001B[0;32m--> 530\u001B[0;31m         \u001B[0mdata\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mschema\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_wrap_data_schema\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mdata\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mschema\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    531\u001B[0m         \u001B[0;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_sc\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mparallelize\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mdata\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mschema\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    532\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;32m/databricks/spark/python/pyspark/sql/session.py\u001B[0m in \u001B[0;36m_wrap_data_schema\u001B[0;34m(self, data, schema)\u001B[0m\n\u001B[1;32m    512\u001B[0m             \u001B[0;32mif\u001B[0m \u001B[0misinstance\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mschema\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m(\u001B[0m\u001B[0mlist\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mtuple\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    513\u001B[0m                 \u001B[0;32mfor\u001B[0m \u001B[0mi\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mname\u001B[0m \u001B[0;32min\u001B[0m \u001B[0menumerate\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mschema\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 514\u001B[0;31m                     \u001B[0mstruct\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mfields\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mi\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mname\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mname\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    515\u001B[0m                     \u001B[0mstruct\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mnames\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mi\u001B[0m\u001B[0;34m]\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mname\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    516\u001B[0m             \u001B[0mschema\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mstruct\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;31mIndexError\u001B[0m: list index out of range"]}}],"execution_count":0},{"cell_type":"code","source":["data1 = [((\"James\",\"\",\"Smith\"),\"36636\",\"M\",60000),\\\n        ((\"Michael\",\"Rose\",\"\"),\"40288\",\"M\",70000),\\\n        ((\"Robert\",\"\",\"Williams\"),\"42114\",\"\",400000),\\\n        ((\"Maria\",\"Anne\",\"Jones\"),\"39192\",\"F\",500000),\\\n        ((\"Jen\",\"Mary\",\"Brown\"),\"\",\"F\",0)\\\n]\n\ndatastruct = StructType([\n    StructField(\"name\",StructType([StructField('first_name',StringType(),True),\n                                   StructField('middle_name',StringType(),True),\n                                   StructField('last_name',StringType(),True)])),\n     StructField('dob', StringType(), True),\n         StructField('gender', StringType(), True),\n         StructField('salary', IntegerType(), True)\n])\n\ndataDF = spark.createDataFrame(data = data1 , schema = datastruct)\ndataDF.printSchema()\npandaDF = dataDF.toPandas()\nprint(pandaDF)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"174e1848-a75b-46c2-bd39-37031ad0af46"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"root\n |-- name: struct (nullable = true)\n |    |-- first_name: string (nullable = true)\n |    |-- middle_name: string (nullable = true)\n |    |-- last_name: string (nullable = true)\n |-- dob: string (nullable = true)\n |-- gender: string (nullable = true)\n |-- salary: integer (nullable = true)\n\n                                                name    dob gender  salary\n0  {'first_name': 'James', 'middle_name': '', 'la...  36636      M   60000\n1  {'first_name': 'Michael', 'middle_name': 'Rose...  40288      M   70000\n2  {'first_name': 'Robert', 'middle_name': '', 'l...  42114         400000\n3  {'first_name': 'Maria', 'middle_name': 'Anne',...  39192      F  500000\n4  {'first_name': 'Jen', 'middle_name': 'Mary', '...             F       0\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["root\n |-- name: struct (nullable = true)\n |    |-- first_name: string (nullable = true)\n |    |-- middle_name: string (nullable = true)\n |    |-- last_name: string (nullable = true)\n |-- dob: string (nullable = true)\n |-- gender: string (nullable = true)\n |-- salary: integer (nullable = true)\n\n                                                name    dob gender  salary\n0  {'first_name': 'James', 'middle_name': '', 'la...  36636      M   60000\n1  {'first_name': 'Michael', 'middle_name': 'Rose...  40288      M   70000\n2  {'first_name': 'Robert', 'middle_name': '', 'l...  42114         400000\n3  {'first_name': 'Maria', 'middle_name': 'Anne',...  39192      F  500000\n4  {'first_name': 'Jen', 'middle_name': 'Mary', '...             F       0\n"]}}],"execution_count":0},{"cell_type":"code","source":["PySpark show() – Display DataFrame Contents in Table"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"37d46a3f-f728-4088-a927-5cd030ee8612"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["import pyspark\nfrom pyspark.sql import SparkSession\nspark = SparkSession.builder.appName('Sparkbyexample.com').getOrCreate()\ncolumns = [\"Seqno\",\"Quote\"]\ndata = [(\"1\", \"Be the change that you wish to see in the world\"),\n    (\"2\", \"Everyone thinks of changing the world, but no one thinks of changing himself.\"),\n    (\"3\", \"The purpose of our lives is to be happy.\"),\n    (\"4\", \"Be cool.\")]\ndataFrame = spark.createDataFrame(data = data , schema = columns)\ndataFrame.printSchema()\ndataframe.show(trunacte = False)# diplay full content in column.\ndataframe.show(2,trunacte = False)# diplay 2 rows and  full content in  two columns.\ndataFrame.show(truncate = False,vertical=True)# diplay columns  and  values vertically."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"72539cfa-fdeb-4d56-8f42-0d6822303996"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"root\n |-- Seqno: string (nullable = true)\n |-- Quote: string (nullable = true)\n\n-RECORD 0------------------------------------------------------------------------------\n Seqno | 1                                                                             \n Quote | Be the change that you wish to see in the world                               \n-RECORD 1------------------------------------------------------------------------------\n Seqno | 2                                                                             \n Quote | Everyone thinks of changing the world, but no one thinks of changing himself. \n-RECORD 2------------------------------------------------------------------------------\n Seqno | 3                                                                             \n Quote | The purpose of our lives is to be happy.                                      \n-RECORD 3------------------------------------------------------------------------------\n Seqno | 4                                                                             \n Quote | Be cool.                                                                      \n\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["root\n |-- Seqno: string (nullable = true)\n |-- Quote: string (nullable = true)\n\n-RECORD 0------------------------------------------------------------------------------\n Seqno | 1                                                                             \n Quote | Be the change that you wish to see in the world                               \n-RECORD 1------------------------------------------------------------------------------\n Seqno | 2                                                                             \n Quote | Everyone thinks of changing the world, but no one thinks of changing himself. \n-RECORD 2------------------------------------------------------------------------------\n Seqno | 3                                                                             \n Quote | The purpose of our lives is to be happy.                                      \n-RECORD 3------------------------------------------------------------------------------\n Seqno | 4                                                                             \n Quote | Be cool.                                                                      \n\n"]}}],"execution_count":0},{"cell_type":"code","source":["PySpark StructType & StructField Explained with Examples"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"6b23a413-b777-4710-8089-ca58b529c7e4"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["import pyspark\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.types import StructType,StructField,StringType,IntegerType,DateType\nspark = SparkSession.builder.appName('SparkByExample.com').getOrCreate()\n                                    \ndata1 = [(\"James\",\"\",\"Smith\",\"36636\",\"M\",3000),\n    (\"Michael\",\"Rose\",\"\",\"40288\",\"M\",4000),\n    (\"Robert\",\"\",\"Williams\",\"42114\",\"M\",4000),\n    (\"Maria\",\"Anne\",\"Jones\",\"39192\",\"F\",4000),\n    (\"Jen\",\"Mary\",\"Brown\",\"\",\"F\",-1)\n        ]\n       \nschemastruct = StructType([\\\n                          StructField(\"Firstname\",StringType(),True),\\\n                    StructField(\"middlename\",StringType(),True),\\\n                    StructField(\"lastname\",StringType(),True),\\\n                    StructField(\"id\",StringType(),True),\\\n                    StructField(\"gender\",StringType(),True),\\\n                    StructField(\"salary\",IntegerType(),True)\\\n                   ])\ndf = spark.createDataFrame(data = data1, schema = schemastruct)\ndf.printSchema()\ndf.show(truncate = False)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"b46666d3-0737-43f9-a2a3-595562218cae"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"root\n |-- Firstname: string (nullable = true)\n |-- middlename: string (nullable = true)\n |-- lastname: string (nullable = true)\n |-- id: string (nullable = true)\n |-- gender: string (nullable = true)\n |-- salary: integer (nullable = true)\n\n+---------+----------+--------+-----+------+------+\n|Firstname|middlename|lastname|id   |gender|salary|\n+---------+----------+--------+-----+------+------+\n|James    |          |Smith   |36636|M     |3000  |\n|Michael  |Rose      |        |40288|M     |4000  |\n|Robert   |          |Williams|42114|M     |4000  |\n|Maria    |Anne      |Jones   |39192|F     |4000  |\n|Jen      |Mary      |Brown   |     |F     |-1    |\n+---------+----------+--------+-----+------+------+\n\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["root\n |-- Firstname: string (nullable = true)\n |-- middlename: string (nullable = true)\n |-- lastname: string (nullable = true)\n |-- id: string (nullable = true)\n |-- gender: string (nullable = true)\n |-- salary: integer (nullable = true)\n\n+---------+----------+--------+-----+------+------+\n|Firstname|middlename|lastname|id   |gender|salary|\n+---------+----------+--------+-----+------+------+\n|James    |          |Smith   |36636|M     |3000  |\n|Michael  |Rose      |        |40288|M     |4000  |\n|Robert   |          |Williams|42114|M     |4000  |\n|Maria    |Anne      |Jones   |39192|F     |4000  |\n|Jen      |Mary      |Brown   |     |F     |-1    |\n+---------+----------+--------+-----+------+------+\n\n"]}}],"execution_count":0},{"cell_type":"code","source":["4. Defining Nested StructType object struct\nWhile working on DataFrame we often need to work with the nested struct column and this can be defined using StructType.\n"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"12392b75-73f2-4e8d-931a-69efcdcfb52a"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["nesteddata = [((\"James\",\"\",\"Smith\"),\"36636\",\"M\",3000),\n    ((\"Michael\",\"Rose\",\"\"),\"40288\",\"M\",4000),\n    ((\"Robert\",\"\",\"Williams\"),\"42114\",\"M\",4000),\n    ((\"Maria\",\"Anne\",\"Jones\"),\"39192\",\"F\",4000),\n    ((\"Jen\",\"Mary\",\"Brown\"),\"\",\"F\",-1)\n        ]\nnestedstruct = StructType([\n    StructField(\"name\",StructType([StructField(\"Firstname\",StringType(),True),\n                                    StructField(\"middlename\",StringType(),True),\n                                    StructField(\"lastname\",StringType(),True) \n                                  ])),\n    StructField(\"id\",StringType(),True),\n    StructField(\"gender\",StringType(),True),\n    StructField(\"salary\",IntegerType(),True)\n    \n])\n\nDF2 = spark.createDataFrame(data = nesteddata, schema = nestedstruct)\nDF2.printSchema()\nDF2.show(truncate = False)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"3cd171fd-48e2-46b8-8f9f-170247dd0eb7"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"root\n |-- name: struct (nullable = true)\n |    |-- Firstname: string (nullable = true)\n |    |-- middlename: string (nullable = true)\n |    |-- lastname: string (nullable = true)\n |-- id: string (nullable = true)\n |-- gender: string (nullable = true)\n |-- salary: integer (nullable = true)\n\n+--------------------+-----+------+------+\n|name                |id   |gender|salary|\n+--------------------+-----+------+------+\n|{James, , Smith}    |36636|M     |3000  |\n|{Michael, Rose, }   |40288|M     |4000  |\n|{Robert, , Williams}|42114|M     |4000  |\n|{Maria, Anne, Jones}|39192|F     |4000  |\n|{Jen, Mary, Brown}  |     |F     |-1    |\n+--------------------+-----+------+------+\n\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["root\n |-- name: struct (nullable = true)\n |    |-- Firstname: string (nullable = true)\n |    |-- middlename: string (nullable = true)\n |    |-- lastname: string (nullable = true)\n |-- id: string (nullable = true)\n |-- gender: string (nullable = true)\n |-- salary: integer (nullable = true)\n\n+--------------------+-----+------+------+\n|name                |id   |gender|salary|\n+--------------------+-----+------+------+\n|{James, , Smith}    |36636|M     |3000  |\n|{Michael, Rose, }   |40288|M     |4000  |\n|{Robert, , Williams}|42114|M     |4000  |\n|{Maria, Anne, Jones}|39192|F     |4000  |\n|{Jen, Mary, Brown}  |     |F     |-1    |\n+--------------------+-----+------+------+\n\n"]}}],"execution_count":0},{"cell_type":"code","source":["5. Adding & Changing struct of the DataFrame\nUsing PySpark SQL function struct(), we can change the struct of the existing DataFrame and add a new StructType to it. The below example demonstrates how to copy the columns from one structure to another and adding a new column. PySpark Column Class also provides some functions to work with the StructType column.\n\nfrom pyspark.sql.functions import col,struct,when\nupdatedDF = df2.withColumn(\"OtherInfo\", \n    struct(col(\"id\").alias(\"identifier\"),\n    col(\"gender\").alias(\"gender\"),\n    col(\"salary\").alias(\"salary\"),\n    when(col(\"salary\").cast(IntegerType()) < 2000,\"Low\")\n      .when(col(\"salary\").cast(IntegerType()) < 4000,\"Medium\")\n      .otherwise(\"High\").alias(\"Salary_Grade\")\n  )).drop(\"id\",\"gender\",\"salary\")\n\nupdatedDF.printSchema()\nupdatedDF.show(truncate=False)\n"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"a57a386f-f474-4389-988c-9be38c787c5e"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["import pyspark\nfrom pyspark.sql.functions import col,struct,when\n\nupdateDF2 = DF2.withColumn(\"otherinfo\",\n                           struct(col(\"id\").alias(\"identifier\"),col(\"gender\").alias(\"gender\"),col(\"salary\").alias(\"salary\"),\n                                  when(col(\"salary\").cast(IntegerType()) < 2000,\"Low\").when(col(\"salary\").cast(IntegerType()) < 4000,\"Medium\").otherwise(\"High\").alias(\"Salary_Grade\") ) ).drop(\"id\",\"gender\",\"salary\")\n\nupdateDF2.printSchema()\nupdateDF2.show(truncate = False)\n"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"e6ee7a04-72a4-4c32-9290-a2edf8cdf1a1"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"root\n |-- name: struct (nullable = true)\n |    |-- Firstname: string (nullable = true)\n |    |-- middlename: string (nullable = true)\n |    |-- lastname: string (nullable = true)\n |-- otherinfo: struct (nullable = false)\n |    |-- identifier: string (nullable = true)\n |    |-- gender: string (nullable = true)\n |    |-- salary: integer (nullable = true)\n |    |-- Salary_Grade: string (nullable = false)\n\n+--------------------+------------------------+\n|name                |otherinfo               |\n+--------------------+------------------------+\n|{James, , Smith}    |{36636, M, 3000, Medium}|\n|{Michael, Rose, }   |{40288, M, 4000, High}  |\n|{Robert, , Williams}|{42114, M, 4000, High}  |\n|{Maria, Anne, Jones}|{39192, F, 4000, High}  |\n|{Jen, Mary, Brown}  |{, F, -1, Low}          |\n+--------------------+------------------------+\n\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["root\n |-- name: struct (nullable = true)\n |    |-- Firstname: string (nullable = true)\n |    |-- middlename: string (nullable = true)\n |    |-- lastname: string (nullable = true)\n |-- otherinfo: struct (nullable = false)\n |    |-- identifier: string (nullable = true)\n |    |-- gender: string (nullable = true)\n |    |-- salary: integer (nullable = true)\n |    |-- Salary_Grade: string (nullable = false)\n\n+--------------------+------------------------+\n|name                |otherinfo               |\n+--------------------+------------------------+\n|{James, , Smith}    |{36636, M, 3000, Medium}|\n|{Michael, Rose, }   |{40288, M, 4000, High}  |\n|{Robert, , Williams}|{42114, M, 4000, High}  |\n|{Maria, Anne, Jones}|{39192, F, 4000, High}  |\n|{Jen, Mary, Brown}  |{, F, -1, Low}          |\n+--------------------+------------------------+\n\n"]}}],"execution_count":0},{"cell_type":"code","source":["6. Using SQL ArrayType and MapType\nSQL StructType also supports ArrayType and MapType to define the DataFrame columns for array and map collections respectively. On the below example, column hobbies defined as ArrayType(StringType) and properties defined as MapType(StringType,StringType) meaning both key and value as String."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"4f585692-e3c0-4190-8904-51f6c9cb73f6"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["import pyspark\nfrom pyspark.sql.types import ArrayType,MapType\narraystructschema = StructType([StructField(\"name\",StructType([StructField(\"firstname\",StringType(),True),\n                                                              StructField(\"middlename\",StringType(),True),\n                                                              StructField(\"lastname\",StringType(),True)])),\n                                StructField(\"hobbies\",ArrayType(StringType()),True),\n                                StructField(\"properties\",MapType(StringType(),StringType()),True)\n    \n]\n)\n\n#df3 = spark.createDataFrame(RDD ,schema = arraystructschema)\n#print(df2.schema.json)\n#df3.printSchema()\n#df3.Show(truncate= False)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"dcbc65c2-5728-4b9e-843c-4d51d783178b"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["\nimport json\nschemaFromJson = StructType.fromJson(json.loads(schema.json))\ndf3 = spark.createDataFrame(\n        spark.sparkContext.parallelize(structureData),schemaFromJson)\ndf3.printSchema()\n"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"76b416d5-4ce0-470c-a124-134a82306579"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)\n\u001B[0;32m<command-76370094242890>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[0;32mimport\u001B[0m \u001B[0mjson\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 2\u001B[0;31m \u001B[0mschemaFromJson\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mStructType\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mfromJson\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mjson\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mloads\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mschema\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mjson\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      3\u001B[0m df3 = spark.createDataFrame(\n\u001B[1;32m      4\u001B[0m         spark.sparkContext.parallelize(structureData),schemaFromJson)\n\u001B[1;32m      5\u001B[0m \u001B[0mdf3\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mprintSchema\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;31mNameError\u001B[0m: name 'schema' is not defined","errorSummary":"<span class='ansi-red-fg'>NameError</span>: name 'schema' is not defined","metadata":{},"errorTraceType":"ansi","type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/plain":["\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)\n\u001B[0;32m<command-76370094242890>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[0;32mimport\u001B[0m \u001B[0mjson\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 2\u001B[0;31m \u001B[0mschemaFromJson\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mStructType\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mfromJson\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mjson\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mloads\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mschema\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mjson\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      3\u001B[0m df3 = spark.createDataFrame(\n\u001B[1;32m      4\u001B[0m         spark.sparkContext.parallelize(structureData),schemaFromJson)\n\u001B[1;32m      5\u001B[0m \u001B[0mdf3\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mprintSchema\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;31mNameError\u001B[0m: name 'schema' is not defined"]}}],"execution_count":0},{"cell_type":"code","source":["from pyspark.sql.functions import fromDDL\n  ddlSchemaStr = \"'fullName' STRUCT<'first': STRING, 'last': STRING,'middle': STRING>,'age' INT,'gender' STRING\"\n  ddlSchema = StructType.fromDDL(ddlSchemaStr)\n  ddlSchema.printTreeString()\n"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"3f9dedff-6240-4f6c-b781-b662aa349418"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"\u001B[0;36m  File \u001B[0;32m\"<command-76370094242892>\"\u001B[0;36m, line \u001B[0;32m2\u001B[0m\n\u001B[0;31m    ddlSchemaStr = \"'fullName' STRUCT<'first': STRING, 'last': STRING,'middle': STRING>,'age' INT,'gender' STRING\"\u001B[0m\n\u001B[0m    ^\u001B[0m\n\u001B[0;31mIndentationError\u001B[0m\u001B[0;31m:\u001B[0m unexpected indent\n","errorSummary":"<span class='ansi-red-fg'>IndentationError</span>: unexpected indent (<command-76370094242892>, line 2)","metadata":{},"errorTraceType":"ansi","type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/plain":["\u001B[0;36m  File \u001B[0;32m\"<command-76370094242892>\"\u001B[0;36m, line \u001B[0;32m2\u001B[0m\n\u001B[0;31m    ddlSchemaStr = \"'fullName' STRUCT<'first': STRING, 'last': STRING,'middle': STRING>,'age' INT,'gender' STRING\"\u001B[0m\n\u001B[0m    ^\u001B[0m\n\u001B[0;31mIndentationError\u001B[0m\u001B[0;31m:\u001B[0m unexpected indent\n"]}}],"execution_count":0},{"cell_type":"code","source":["PySpark Row using on DataFrame and RDD\nIn PySpark Row class is available by importing pyspark.sql.Row which is represented as a record/row in DataFrame, one can create a Row object by using named arguments, or create a custom Row like class. In this article I will explain how to use Row class on RDD, DataFrame and its functions.\n\nBefore we start using it on RDD & DataFrame, let’s understand some basics of Row class.\nTo enable sorting by names, set the environment variable PYSPARK_ROW_FIELD_SORTING_ENABLED to true.\n1. Create a Row Object\nRow class extends the tuple hence it takes variable number of arguments, Row() is used to create the row object. Once the row object created, we can retrieve the data from Row using index similar to tuple.\n\nfrom pyspark.sql import Row\nrow=Row(\"James\",40)\nprint(row[0] +\",\"+str(row[1]))\n"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"2c4845d8-0ff4-4829-8459-a38f0d5d285b"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["from pyspark.sql import Row\nrow = Row(\"james\",40)\nprint(row[0] +\",\"+ str(row[1]))"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"3cd94ae1-3567-4563-8b95-a5cc2765b20f"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"james,40\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["james,40\n"]}}],"execution_count":0},{"cell_type":"code","source":["row = Row(name = \"james\", age = 11)\nprint(row.name,\\\n      row.age)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"38a5d0e2-73c6-4205-b135-23b09b4402be"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"james 11\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["james 11\n"]}}],"execution_count":0},{"cell_type":"code","source":["2. Create Custom Class from Row\nWe can also create a Row like class, for example “Person” and use it similar to Row object. This would be helpful when you wanted to create real time object and refer it’s properties. On below example, we have created a Person class and used similar to Row.\n\n\nPerson = Row(\"name\", \"age\")\np1=Person(\"James\", 40)\np2=Person(\"Alice\", 35)\nprint(p1.name +\",\"+p2.name)\n"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"9c2bbb05-288a-441e-9f15-d2d9aebb5a6f"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["person= Row(\"name\",\"age\")\np1 = person(\"james\",40)\np2=person(\"krishna\",38)\nprint(p1.name+\",\"+p2.name)\nprint(str(p1.age)+\",\"+str(p2.age))"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"e09d130a-5f3b-4bd5-a941-3a3e7c6a5a57"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"james,krishna\n40,38\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["james,krishna\n40,38\n"]}}],"execution_count":0},{"cell_type":"code","source":["\nPerson=Row(\"name\",\"lang\",\"state\")\ndata = [Person(\"James,,Smith\",[\"Java\",\"Scala\",\"C++\"],\"CA\"), \n    Person(\"Michael,Rose,\",[\"Spark\",\"Java\",\"C++\"],\"NJ\"),\n    Person(\"Robert,,Williams\",[\"CSharp\",\"VB\"],\"NV\")]\n"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"fde67e6d-592c-475a-95b8-122e997fb078"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)\n\u001B[0;32m<command-76370094242898>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[0;32m----> 1\u001B[0;31m \u001B[0mPerson\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mRow\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"name\"\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\"lang\"\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\"state\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      2\u001B[0m data = [Person(\"James,,Smith\",[\"Java\",\"Scala\",\"C++\"],\"CA\"), \n\u001B[1;32m      3\u001B[0m     \u001B[0mPerson\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"Michael,Rose,\"\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;34m\"Spark\"\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\"Java\"\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\"C++\"\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\"NJ\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      4\u001B[0m     Person(\"Robert,,Williams\",[\"CSharp\",\"VB\"],\"NV\")]\n\n\u001B[0;31mNameError\u001B[0m: name 'Row' is not defined","errorSummary":"<span class='ansi-red-fg'>NameError</span>: name 'Row' is not defined","metadata":{},"errorTraceType":"ansi","type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/plain":["\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)\n\u001B[0;32m<command-76370094242898>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[0;32m----> 1\u001B[0;31m \u001B[0mPerson\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mRow\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"name\"\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\"lang\"\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\"state\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      2\u001B[0m data = [Person(\"James,,Smith\",[\"Java\",\"Scala\",\"C++\"],\"CA\"), \n\u001B[1;32m      3\u001B[0m     \u001B[0mPerson\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"Michael,Rose,\"\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;34m\"Spark\"\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\"Java\"\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\"C++\"\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\"NJ\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      4\u001B[0m     Person(\"Robert,,Williams\",[\"CSharp\",\"VB\"],\"NV\")]\n\n\u001B[0;31mNameError\u001B[0m: name 'Row' is not defined"]}}],"execution_count":0},{"cell_type":"code","source":["import pyspark\nfrom pyspark.sql import Row\nPerson = Row(\"name\",\"lang\",\"state\")\ndata = [Person(\"james,,smith\",[\"java\",\"Scala\",\"C++\"],\"CA\"),\n        Person(\"Krishna,,Reddy\",[\"dotnet\",\"sql\",\"python\"],\"IND\"),\n        Person(\"Mani,,Reddy\",[\"telugu\",\"english\",\"Hindi\"],\"USA\")\n       ]\nDF4 = spark.createDataFrame(data= data)\nDF4.printSchema()\nDF4.show(truncate = False)\n\ncolumns = [\"name\",\"languagesAtSchool\",\"currentState\"]\ndf=spark.createDataFrame(data).toDF(*columns)\ndf.printSchema()\n"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"efe90234-f7ef-461d-a5b5-1babc0393239"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"root\n |-- name: string (nullable = true)\n |-- lang: array (nullable = true)\n |    |-- element: string (containsNull = true)\n |-- state: string (nullable = true)\n\n+--------------+------------------------+-----+\n|name          |lang                    |state|\n+--------------+------------------------+-----+\n|james,,smith  |[java, Scala, C++]      |CA   |\n|Krishna,,Reddy|[dotnet, sql, python]   |IND  |\n|Mani,,Reddy   |[telugu, english, Hindi]|USA  |\n+--------------+------------------------+-----+\n\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["root\n |-- name: string (nullable = true)\n |-- lang: array (nullable = true)\n |    |-- element: string (containsNull = true)\n |-- state: string (nullable = true)\n\n+--------------+------------------------+-----+\n|name          |lang                    |state|\n+--------------+------------------------+-----+\n|james,,smith  |[java, Scala, C++]      |CA   |\n|Krishna,,Reddy|[dotnet, sql, python]   |IND  |\n|Mani,,Reddy   |[telugu, english, Hindi]|USA  |\n+--------------+------------------------+-----+\n\n"]}}],"execution_count":0},{"cell_type":"code","source":["# change columns names\ncolumns = [\"name\",\"languageAtSchool\",\"currentState\"]\ndf5 = spark.createDataFrame(data).toDF(*columns)\ndf5.printSchema()\ndf5.show(truncate = False)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"23cba105-801b-41ed-8f28-fb1c34823ffb"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"root\n |-- name: string (nullable = true)\n |-- languageAtSchool: array (nullable = true)\n |    |-- element: string (containsNull = true)\n |-- currentState: string (nullable = true)\n\n+--------------+------------------------+------------+\n|name          |languageAtSchool        |currentState|\n+--------------+------------------------+------------+\n|james,,smith  |[java, Scala, C++]      |CA          |\n|Krishna,,Reddy|[dotnet, sql, python]   |IND         |\n|Mani,,Reddy   |[telugu, english, Hindi]|USA         |\n+--------------+------------------------+------------+\n\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["root\n |-- name: string (nullable = true)\n |-- languageAtSchool: array (nullable = true)\n |    |-- element: string (containsNull = true)\n |-- currentState: string (nullable = true)\n\n+--------------+------------------------+------------+\n|name          |languageAtSchool        |currentState|\n+--------------+------------------------+------------+\n|james,,smith  |[java, Scala, C++]      |CA          |\n|Krishna,,Reddy|[dotnet, sql, python]   |IND         |\n|Mani,,Reddy   |[telugu, english, Hindi]|USA         |\n+--------------+------------------------+------------+\n\n"]}}],"execution_count":0},{"cell_type":"code","source":["5. Create Nested Struct Using Row Class\nThe below example provides a way to create a struct type using the Row class. Alternatively, you can also create struct type using By Providing Schema using PySpark StructType & StructFields\n\n#Create DataFrame with struct using Row class\nfrom pyspark.sql import Row\ndata=[Row(name=\"James\",prop=Row(hair=\"black\",eye=\"blue\")),\n      Row(name=\"Ann\",prop=Row(hair=\"grey\",eye=\"black\"))]\ndf=spark.createDataFrame(data)\ndf.printSchema()\n"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"50661ecc-7739-4036-8c05-0ad2e777086f"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["data = [Row(name = \"krishna\",prop = Row(hair = \"white\",eye='black')),\n       Row(name = \"mani\",prop = Row(hair = \"black\",eye = 'red'))\n       ]\nDF6 =  spark.createDataFrame(data= data)\nDF6.printSchema()\nDF6.show(truncate= False)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"e1caaead-f849-464d-bbcf-60a660d22589"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"root\n |-- name: string (nullable = true)\n |-- prop: struct (nullable = true)\n |    |-- hair: string (nullable = true)\n |    |-- eye: string (nullable = true)\n\n+-------+--------------+\n|name   |prop          |\n+-------+--------------+\n|krishna|{white, black}|\n|mani   |{black, red}  |\n+-------+--------------+\n\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["root\n |-- name: string (nullable = true)\n |-- prop: struct (nullable = true)\n |    |-- hair: string (nullable = true)\n |    |-- eye: string (nullable = true)\n\n+-------+--------------+\n|name   |prop          |\n+-------+--------------+\n|krishna|{white, black}|\n|mani   |{black, red}  |\n+-------+--------------+\n\n"]}}],"execution_count":0},{"cell_type":"code","source":["PySpark Column Class | Operators & Functions\npyspark.sql.Column class provides several functions to work with DataFrame to manipulate the Column values, evaluate the boolean expression to filter rows, retrieve a value or part of a value from a DataFrame column, and to work with list, map & struct columns.\n\nIn this article, I will cover how to create Column object, access them to perform operations, and finally most used PySpark Column Functions with Examples.\n1. Create Column Class Object\nOne of the simplest ways to create a Column class object is by using PySpark lit() SQL function, this takes a literal value and returns a Column object.\n\nfrom pyspark.sql.functions import lit\ncolObj = lit(\"sparkbyexamples.com\")\n\n\ndata=[(\"James\",23),(\"Ann\",40)]\ndf=spark.createDataFrame(data).toDF(\"name.fname\",\"gender\")\ndf.printSchema()\n#root\n# |-- name.fname: string (nullable = true)\n# |-- gender: long (nullable = true)\n\n# Using DataFrame object (df)\ndf.select(df.gender).show()\ndf.select(df[\"gender\"]).show()\n#Accessing column name with dot (with backticks)\ndf.select(df[\"`name.fname`\"]).show()\n\n#Using SQL col() function\nfrom pyspark.sql.functions import col\ndf.select(col(\"gender\")).show()\n#Accessing column name with dot (with backticks)\ndf.select(col(\"`name.fname`\")).show()\n\n"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"727e7f5a-cea0-4bcb-a6e9-9864c9d537f1"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["import pyspark\nfrom pyspark.sql.functions import lit,col\n\ncolObj = lit(\"sparkbyexamples.com\")\n\ndata = [(\"james\",23),(\"Ann\",40)]\ndf7 = spark.createDataFrame(data).toDF(\"name.fname\",\"age\")\ndf7.printSchema()\ndf7.select(df7[\"age\"]).show()\ndf7.select(df7.age).show()\ndf7.select(df7[\"`name.fname`\"]).show()\n#Using SQL col() function\ndf7.select(col(\"age\")).show()\ndf7.select(col(\"`name.fname`\")).show()\n"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"24160419-55d0-4bf9-9b4a-1f94d766b610"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"root\n |-- name.fname: string (nullable = true)\n |-- age: long (nullable = true)\n\n+---+\n|age|\n+---+\n| 23|\n| 40|\n+---+\n\n+---+\n|age|\n+---+\n| 23|\n| 40|\n+---+\n\n+----------+\n|name.fname|\n+----------+\n|     james|\n|       Ann|\n+----------+\n\n+---+\n|age|\n+---+\n| 23|\n| 40|\n+---+\n\n+----------+\n|name.fname|\n+----------+\n|     james|\n|       Ann|\n+----------+\n\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["root\n |-- name.fname: string (nullable = true)\n |-- age: long (nullable = true)\n\n+---+\n|age|\n+---+\n| 23|\n| 40|\n+---+\n\n+---+\n|age|\n+---+\n| 23|\n| 40|\n+---+\n\n+----------+\n|name.fname|\n+----------+\n|     james|\n|       Ann|\n+----------+\n\n+---+\n|age|\n+---+\n| 23|\n| 40|\n+---+\n\n+----------+\n|name.fname|\n+----------+\n|     james|\n|       Ann|\n+----------+\n\n"]}}],"execution_count":0},{"cell_type":"code","source":["Below example demonstrates accessing struct type columns. Here I have use PySpark Row class to create a struct type. Alternatively you can also create it by using PySpark StructType & StructField classes\n\n\n#Create DataFrame with struct using Row class\nfrom pyspark.sql import Row\ndata=[Row(name=\"James\",prop=Row(hair=\"black\",eye=\"blue\")),\n      Row(name=\"Ann\",prop=Row(hair=\"grey\",eye=\"black\"))]\ndf=spark.createDataFrame(data)\ndf.printSchema()\n#root\n# |-- name: string (nullable = true)\n# |-- prop: struct (nullable = true)\n# |    |-- hair: string (nullable = true)\n# |    |-- eye: string (nullable = true)\n\n#Access struct column\ndf.select(df.prop.hair).show()\ndf.select(df[\"prop.hair\"]).show()\ndf.select(col(\"prop.hair\")).show()\n\n#Access all columns from struct\ndf.select(col(\"prop.*\")).show()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"43482c4f-3205-4739-beca-18f3ade54dfd"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["from pyspark.sql import Row\ndata1 = [Row(name = \"james\",prop = Row(hair = \"black\",eye=\"black\")),\n        Row(name = \"anna\",prop = Row(hair= 'white',eye = \"red\"))\n        ]\ndf8 = spark.createDataFrame(data = data1)\ndf8.printSchema()\n#df8.select(df8.prop.hair).show()\n#df8.select(df8.prop.eye).show()\ndf8.select(df8[\"prop.hair\"]).show()\ndf8.select(col(\"prop.hair\")).show()\ndf8.select(col(\"prop.*\")).show()\ndf8.select(col(\"prop.*\")).show()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"4be4ba05-8425-49fd-8493-7e1d866abd1c"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"root\n |-- name: string (nullable = true)\n |-- prop: struct (nullable = true)\n |    |-- hair: string (nullable = true)\n |    |-- eye: string (nullable = true)\n\n+-----+\n| hair|\n+-----+\n|black|\n|white|\n+-----+\n\n+-----+\n| hair|\n+-----+\n|black|\n|white|\n+-----+\n\n+-----+-----+\n| hair|  eye|\n+-----+-----+\n|black|black|\n|white|  red|\n+-----+-----+\n\n+-----+-----+\n| hair|  eye|\n+-----+-----+\n|black|black|\n|white|  red|\n+-----+-----+\n\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["root\n |-- name: string (nullable = true)\n |-- prop: struct (nullable = true)\n |    |-- hair: string (nullable = true)\n |    |-- eye: string (nullable = true)\n\n+-----+\n| hair|\n+-----+\n|black|\n|white|\n+-----+\n\n+-----+\n| hair|\n+-----+\n|black|\n|white|\n+-----+\n\n+-----+-----+\n| hair|  eye|\n+-----+-----+\n|black|black|\n|white|  red|\n+-----+-----+\n\n+-----+-----+\n| hair|  eye|\n+-----+-----+\n|black|black|\n|white|  red|\n+-----+-----+\n\n"]}}],"execution_count":0},{"cell_type":"code","source":["2. PySpark Column Operators\nPySpark column also provides a way to do arithmetic operations on columns using operators.\n\ndata=[(100,2,1),(200,3,4),(300,4,4)]\ndf=spark.createDataFrame(data).toDF(\"col1\",\"col2\",\"col3\")\n\n#Arthmetic operations\ndf.select(df.col1 + df.col2).show()\ndf.select(df.col1 - df.col2).show() \ndf.select(df.col1 * df.col2).show()\ndf.select(df.col1 / df.col2).show()\ndf.select(df.col1 % df.col2).show()\n\ndf.select(df.col2 > df.col3).show()\ndf.select(df.col2 < df.col3).show()\ndf.select(df.col2 == df.col3).show()\n"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"a3b3b0e0-35ae-4fdf-9727-27a9ddcb1e38"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["data2 = [(100,2,1),(200,3,4),(300,4,4)]\ndf9 = spark.createDataFrame(data = data2).toDF(\"col1\",\"col2\",\"col3\")\ndf9.printSchema()\ndf9.select(df9.col1 + df9.col2).show()\n#df9.select(col(\"col1\") + col(\"col2\")).show()\n#df9.select(df9.col1 - df9.col2).show()\n#df9.select(col(\"col1\")-col(\"col2\")).show()\ndf9.select(df9.col1 * df9.col2).show()\ndf9.select(df9.col1>df9.col2).show()\ndf9.select(col(\"col1\") == col(\"col2\")).show()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"bc22f7ec-9c5b-4da1-aefd-e1d677735e46"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"root\n |-- col1: long (nullable = true)\n |-- col2: long (nullable = true)\n |-- col3: long (nullable = true)\n\n+-------------+\n|(col1 + col2)|\n+-------------+\n|          102|\n|          203|\n|          304|\n+-------------+\n\n+-------------+\n|(col1 * col2)|\n+-------------+\n|          200|\n|          600|\n|         1200|\n+-------------+\n\n+-------------+\n|(col1 > col2)|\n+-------------+\n|         true|\n|         true|\n|         true|\n+-------------+\n\n+-------------+\n|(col1 = col2)|\n+-------------+\n|        false|\n|        false|\n|        false|\n+-------------+\n\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["root\n |-- col1: long (nullable = true)\n |-- col2: long (nullable = true)\n |-- col3: long (nullable = true)\n\n+-------------+\n|(col1 + col2)|\n+-------------+\n|          102|\n|          203|\n|          304|\n+-------------+\n\n+-------------+\n|(col1 * col2)|\n+-------------+\n|          200|\n|          600|\n|         1200|\n+-------------+\n\n+-------------+\n|(col1 > col2)|\n+-------------+\n|         true|\n|         true|\n|         true|\n+-------------+\n\n+-------------+\n|(col1 = col2)|\n+-------------+\n|        false|\n|        false|\n|        false|\n+-------------+\n\n"]}}],"execution_count":0},{"cell_type":"code","source":["4.1 alias() – Set’s name to Column\nOn below example df.fname refers to Column object and alias() is a function of the Column to give alternate name. Here, fname column has been changed to first_name & lname to last_name.\n\nOn second example I have use PySpark expr() function to concatenate columns and named column as fullName.\ndata=[(\"James\",\"Bond\",\"100\",None),\n      (\"Ann\",\"Varsa\",\"200\",'F'),\n      (\"Tom Cruise\",\"XXX\",\"400\",''),\n      (\"Tom Brand\",None,\"400\",'M')] \ncolumns=[\"fname\",\"lname\",\"id\",\"gender\"]\ndf=spark.createDataFrame(data,columns)\n#alias\nfrom pyspark.sql.functions import expr\ndf.select(df.fname.alias(\"first_name\"), \\\n          df.lname.alias(\"last_name\")\n   ).show()\n\n#Another example\ndf.select(expr(\" fname ||','|| lname\").alias(\"fullName\") \\\n   ).show()\n"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"8cb76d4c-9b27-48cb-93f5-83c3052c5127"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["from pyspark.sql.functions import expr\ndata10=[(\"James\",\"Bond\",\"100\",None),\n      (\"Ann\",\"Varsa\",\"200\",'F'),\n      (\"Tom Cruise\",\"XXX\",\"400\",''),\n      (\"Tom Brand\",None,\"400\",'M')]\ndf11 = spark.createDataFrame(data = data10).toDF(\"fname\",\"lname\",\"id\",\"gender\")\n#df11.printSchema()\n#df11.show()\ndf11.select(df11.fname.alias(\"first_name\"),\\\n           df11.lname.alias(\"last_name\")\n           ).show()\ndf11.select(expr(\"fname ||','|| lname\").alias(\"fullname\")).show()\ndf11.select(col(\"fname\").alias(\"first_name\")\n,col(\"fname\").alias(\"lastname\")).show()\n"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"68adac7c-166c-4967-a404-e6efb85b4783"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"+----------+---------+\n|first_name|last_name|\n+----------+---------+\n|     James|     Bond|\n|       Ann|    Varsa|\n|Tom Cruise|      XXX|\n| Tom Brand|     null|\n+----------+---------+\n\n+--------------+\n|      fullname|\n+--------------+\n|    James,Bond|\n|     Ann,Varsa|\n|Tom Cruise,XXX|\n|          null|\n+--------------+\n\n+----------+----------+\n|first_name|  lastname|\n+----------+----------+\n|     James|     James|\n|       Ann|       Ann|\n|Tom Cruise|Tom Cruise|\n| Tom Brand| Tom Brand|\n+----------+----------+\n\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["+----------+---------+\n|first_name|last_name|\n+----------+---------+\n|     James|     Bond|\n|       Ann|    Varsa|\n|Tom Cruise|      XXX|\n| Tom Brand|     null|\n+----------+---------+\n\n+--------------+\n|      fullname|\n+--------------+\n|    James,Bond|\n|     Ann,Varsa|\n|Tom Cruise,XXX|\n|          null|\n+--------------+\n\n+----------+----------+\n|first_name|  lastname|\n+----------+----------+\n|     James|     James|\n|       Ann|       Ann|\n|Tom Cruise|Tom Cruise|\n| Tom Brand| Tom Brand|\n+----------+----------+\n\n"]}}],"execution_count":0},{"cell_type":"code","source":[""],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"83996fde-ae15-4653-8ac7-d1e752a40b60"}},"outputs":[],"execution_count":0}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"Test_Notebook (2)","dashboards":[],"notebookMetadata":{"pythonIndentUnit":4},"language":"python","widgets":{},"notebookOrigID":2385036558321592}},"nbformat":4,"nbformat_minor":0}
